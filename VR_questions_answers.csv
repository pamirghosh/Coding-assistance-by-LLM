prompt,response
"
I want to develop a first person driving VR game. I want to programmatically move the users' view without users moving. I found PerspectiveCamera API in RealityKit. How can I add a PerspectiveCamera in RealityView to simulate driving perspective and disable Vision Pro's default perspectiveCamera?
I have tried to add a PerspectiveCamera, but it was unworkable.
RealityView { content in
    let anchorEntity = AnchorEntity(world: [0, 0, 0])
    let sphereMesh = MeshResource.generateSphere(radius: 1.5)
    let sphereMaterial = SimpleMaterial(color: .white, roughness: 0.5, isMetallic: true)
    let sphereEntity = ModelEntity(mesh: sphereMesh, materials: [sphereMaterial])
    sphereEntity.position = [0, 2, -5]
    anchorEntity.addChild(sphereEntity)

    let camera = PerspectiveCamera()
    camera.position = [1, 4, 0]
    anchorEntity.addChild(camera)
    content.add(anchorEntity)
}

","
You can use a customized PerspectiveCamera on iOS and macOS if a cameraMode property is set to .nonAR (i.e. VR mode). However, visionOS does not yet have its own "".nonAR mode"" to activate a customized non-default PerspectiveCamera (even if you run a fully immersive experience).
@available(macOS 10.15, iOS 13.0, *)
@MainActor @preconcurrency 
public class PerspectiveCamera : Entity, HasPerspectiveCamera {

    @MainActor required public init()
}

"
"
I'm working on a VR project for HTC Vive Cosmos headset and have a huge problem with UI interactions.

I was able to configute XR Rig with both controllers (left and right) which have an XR ray interactor.
Canvas has Tracked Device Graphic Raycaster script and Graphic Raycaster
EventSystem has XR UI Input Module
There is an XR interaction Manager object in the scene with XR interaction manager script on it.

The project loads and I can see the rays representing both controllers, but when I try to interact with the UI button nothing happens.
What I tried:

I of course read a documentation from Unity to XR Interaction Toolkit, especially UI setup section.
I set raycast target off for all other UI elements that do not require interaction.
I checked if the buttons have ""interactable"" on.
I checked if there were some transparent UI elements that could be overlapping my buttons.

Nothing seems to be working and I got a feeling, the problem is not in UI elements, but in configuring the toolkit itself. Have you any ideas what can I check/change to make UI interactions work?
","
The problem is solved and it was more an attention error.
My controllers have an XR Ray Interactor and it has a property Max Raycast Distance (in Inspector -> XR Ray Interactor -> Raycast Configuration -> Max Raycast Distance).
So basically my UI Elements were too far for interactor to detect them. I altered the Max Raycast Distance value and everything now works perfectly.
"
"
Index Out of Range Bug
Ignore the camera warning.
I already clicked the warnings and it brought me to this code in Unity's XRPass script:
       public Rect GetViewport(int viewIndex = 0)
        {
            return m_Views[viewIndex].viewport;
        }

I don't even know what this is and it doesn't throw an error in Visual Studio.
","
It is a bug in new versions of Unity. When you click an object that has a renderer in play mode, an error shows up. The bug has been reported and will be fixed soon. You can check here until the bug is fixed: Bug
"
"
I was facing duplicate resources error on generating singed apk with react native by using @viro-community/react-viro in Viro3DObject when using objects & mtl files from local directory inside project or url.
Error shown in image

","
I was stuck to this error for almost a week and the solution i got was a small mistake done by me.
I was getting .OBJ & .MTL files from a directory name Modals and the names of files were as under
coralchair.mtl &
coralchair.obj

And I renamed one of file from them like this
coralchair.mtl
coralchairObject.obj

then
clean build 

Now Again generate signed APK using command line or android studio.
this thing resolved the error because while generating build both files were being used as resources and having same name of files was causing conflict.
"
"
I'm looking to create an interactable VR world of my low poly scene.
How would I achieve lighting similar to this in Unity?

I have found this on the Unity store.  Any help would be appreciated!
","
Also known as volumetric lighting =) 
You can find countless assets for achieving this, a lot of which are free and of high quality. 
The asset you linked is very expensive and if it is what you want to go for then great, but the price is set according to the amount of low poly 3D environments, textures, shaders, prefabs, demo scenes etc... not for the lighting.
If you only need volumetric lighting I recommend this free and great asset that will get you set instantly :
Volumetric Lighting Asset
It has great documentation but you can always follow this tutorial if you want a little extra visual help to get started :
YouTube Tutorial
Otherwise feel free to browse the asset store!
"
"
Apple just released visionOS SDK for creating Vision Pro apps. How can I run a react-native project on Apple Vision Pro?
","
Running your app on visionOS is just a matter of adding it as a supported destination.
Select your app target in your project’s settings and navigate to the General tab. In Supported Destinations, click the Add (+) button to add a new destination and select the Apple Vision option.

After that just select Apple Vision Pro (Designed for iPad) from you run destinations list and press run

"
"
After updating my meta quest 2 I cant move my hands in my unity project
I changed nothing to the project since the last time I tested it(when I could still move around)
and after my oculus quest 2 updated I suddenly couldn't move my arms in my unity project does anyone know what the problem could be and maybe how to fix it
","
i was able to solve it by going to the project settings and going to xr plug-in management and chaging the openxr to oculus
"
"
I'm developing a Unity VR game that utilizes 360 videos, featuring distinct perspectives for a worker and a driver. The driver's videos have been edited to primarily showcase the steering wheel and the front window. While testing within Unity's Playmode, both sets of videos play smoothly without any issues, including when using a VR headset.
However, upon building the game, I encountered a problem where the worker's videos continue to play as expected, but the video playback for the driver's edited videos experiences significant lag.
To address this, I attempted to adjust the display refresh rate specifically for Oculus devices with the following code, hoping it might alleviate the lag:
Unity.XR.Oculus.Performance.TrySetDisplayRefreshRate(90f);

Unfortunately, this adjustment did not result in any noticeable improvement. I've provided a link to the recorded footage to better illustrate the issue: https://imgur.com/a/WuqpEqE
Any suggestions on optimization techniques or adjustments to Unity or Oculus settings that could improve the performance of heavily edited 360 videos would be greatly appreciated.
","
If one set of videos is lagging while the other one is not, with the same video player, your files will be different.
Inspect them with VLC, I would suspect one of them is coded that hardware decodes on the Quest while the other is not. I have often encountered H265 being delivered for example.
That or the bitrate is wildly different. VLC will help you to find out both. Adjusting the refresh rate is not the correct way to go about this.
"
"
Do you know if there is any way to integrate Anylogic with a VR device to see the simulation run in VR? What type of software/methodology has worked for you?
","
Only possible as far as you can run any Windows application in VR (i.e. in ""cinema mode""). But there is no automated way to really ""visit"" your models in VR.
However, there may be some Java packages that could help, but it would be custom coding
"
"
I am using openXR to create a unity app for Oculus Quest.
I am trying to open a VR scene after the current AR scene. The AR scene is working fine, but then I call
SceneManager.LoadScene(""Room"", LoadSceneMode.Single);

and the new scene loads (I hear the sound from the new scene), but nothing is displayed.
At the same time, if I take the helmet off and put it back on, the VR scene starts working (maybe the presence sensor triggers some sort of helmet lifecycle, and when the lifecycle resets, it starts work).
Otherwise, if I move from a VR scene to an AR scene, everything works fine. Separately scenes run fine also.
I've tried calling before loading a new scene and call initialization on new scene, but that doesn't help.
XRGeneralSettings.Instance.Manager.DeinitializeLoader()

","
On an AR scene, before loading a new scene, you must call:
XRGeneralSettings.Instance.Manager.StopSubsystems();
XRGeneralSettings.Instance.Manager.DeinitializeLoader();
SceneManager.LoadScene(""Room VR"", LoadSceneMode.Single);

And then in the new single VR scene, call Start:
XRGeneralSettings.Instance.Manager.InitializeLoaderSync();
XRGeneralSettings.Instance.Manager.StartSubsystems();

"
"
I have a VR project for which I am trying to render a canvas to be ""always on top"" of everything using a separate UI camera as noted here.
I made my UICamera object a child of the main camera - which is the centerEyeAnchor object in the OVRCameraRig.

And I set the culling mask for my UICamera to only show its layer and removed that layer from the main camera (CenterEyeAnchor).


But in fact the perspective is weird and it seems like the UICamera is offset a little bit but its transform is zero'd out in the inspector, so I don't why it's displaying so weird.  
If I set the culling mask to ""Everything"" for both cameras it's still offset a little.
","
In general you don't need the UI camera to be a child of CenterEyeAnchor. Move it out to to the top level and zero out the coordinates. The Oculus rig might be doing some magic with IPD or something else and it screws up the pixel-perfectness of UI.
"
"
I'm currently developing an app using Unity. I would use the virtual hands to interact with objects/button. I aim to implement a scene where there is an object/button and upon pressing it, another object changes color. I'm wondering how I could achieve this. Any insights or suggestions on how to go about implementing this interaction would be appreciated!
I did some with HTC Vive and Leap motion using Interaction Button (i am looking for something similar)
","
there are multiple prebuilt toolkits in unity if you want to try that related to buttons and some predefined actions. i guess if you use it and see how it works will help you.
or you can use
GameObject buttonPrecolor;
void MyAwesomeColorCreator()
{
    GameObject go = Instantiate(buttonPrecolor);
    var button = GetComponent<UnityEngine.UI.Button>();
    button.onClick.AddListener(() => FooOnClick());
}
void CuttonOnClick()
{
    Debug.Log(""color change"");
}

"
"
I'm making a vr game, some of my game objects have a world space canvas displaying the game object's info.
When I'm holding the game object upright like this, it looks fine, but if I rotate it downwards, the canvas will follow the parent.

As shown here the canvas which is a child of the game object follows the parent and ends up at the bottom. I want it to always be above the game object
Heres the hierarchy  of game objects

finally my canvas has this lookat cam script

","
I think you should rotate only ""Salt Shaker Obj"" (assuming it's GameObject with renderer) instead of entire ""Salt Shaker"". Or if it's necessary to rotate parent object then introduce another empty GameObject as parent of ""Salt Shaker"" and place canvas as ""Salt Shaker"" sibling.
"
"
I cannot seem to be able to disable the guardian in-app even though applications like first encounters do not have a guardian. Isn't it counterintuitive to not allow guardians to be turned off in passthrough mode?
","
I found the solution!
Add the following to AndroidManifest.XML
<uses-feature android:name=""com.oculus.feature.CONTEXTUAL_BOUNDARYLESS_APP"" android:required=""true"" />

"
"
I am running:

Aframe 1.3.0
iPhone 8 running IOS 15.5
MacBook Pro (Mid 2012) running macOS Catalina 10.15.7

I have an aframe webpage that loads in a simple, test environment.
I am able to view the webpage fine (including VR mode) on my MacBook, although there is some lag when trying to run it on Safari.
On my iPhone, I cannot view VR mode at all. The regular view works fine, and I am able to drag to move my camera and look around.

When I try to enter VR mode, I get prompted to place my phone into my Google Cardboard and VR does not work. It will get stuck on a white screen.


I understand that there have been issues with IOS and webXR support in the past and people seem to be encountering similar issues to mine. The fixes suggested in those links also did not work for me.
I also get the following errors in the iPhone web inspector:
TypeError: scope.dispatchEvent is not a function. (In 'scope.dispatchEvent({
    type: 'sessionstart'
})', 'scope.dispatchEvent' is undefined)
        
        
TypeError: undefined is not an object (evaluating 'camera.near')

I'd really like to make the VR work on both Android and Apple devices. Are there any work arounds to make Aframe VR work on IOS mobile devices?
","
I ended up using an older version of A-Frame.
The newest version of A-Frame doesn't run in VR mode on IOS; version 1.2.0 does. You can identify what OS the webpage is using and change the A-Frame version based on this. However, version 1.2.0 has different bugs and sometimes gives different behavior than 1.3.0. I had to make other changes on top of the A-Frame version to ensure the experience was consistent on both devices.
"
"
The initial scene works fine.  VR mode works.  But I'm not sure how to go back to normal mode.
var scene = new THREE.Scene()
var camera = new THREE.PerspectiveCamera( 75, window.innerWidth/window.innerHeight, 0.1, 1000 )
camera.position.set(70,0,70)
scene.add(camera)
var renderer = new THREE.WebGLRenderer( { alpha: true } )
renderer.setSize( window.innerWidth, window.innerHeight )
document.body.appendChild( renderer.domElement )
var controls = new THREE.OrbitControls( camera, renderer.domElement )
var effect = new THREE.StereoEffect(renderer)

var squares = []
var geometry = new THREE.BoxGeometry( .1, 3, 3 )
var radius = 50
var VR = false

for ( var lon = 1; lon < 20; lon ++ ) {
  for ( var lat = 1; lat < 20; lat ++ ) {

    var square = new THREE.Mesh( geometry, new THREE.MeshBasicMaterial( { transparent: true, opacity: 0.9, color: 0xffffff } ) )
    squares.push ( square )
    scene.add( square )

    var x = radius * Math.sin( lon ) * Math.cos( lat )
    var y = radius * Math.cos( lon )
    var z = radius * Math.sin( lon ) * Math.sin( lat )

    square.position.x = x
    square.position.y = y
    square.position.z = z

  }
}

function animate() {
  requestAnimationFrame( animate )

  if ( VR ) {
    effect.render(scene, camera)
  } else {
    renderer.render(scene, camera)
  }

}

function toggleVR() {
  if ( VR ) {
    VR = false
  } else {
    VR = true
  }
}

animate()

Here's my codepen:
https://codepen.io/megagumby/pen/RjWZPJ
Click on the button twice and you'll see what I'm talking about.  Thanks in advance.
","
The THREE.StereoEffect changes the size of the THREE.WebGLRenderer. You have to reset the size of the renderer by setSize, when you switch back.
function toggleVR() {
    if ( VR ) {
        VR = false
        renderer.setSize( window.innerWidth, window.innerHeight )
    } else {
        VR = true
    }
}

"
"
I'm getting the following errors after importing Oculus Integration from Unity Asset Store.
Using Unity 2019.4.40f1

","
2019.4.40f1 is not supported by Oculus integration

"
"
RealityKit has an option for .nonAR camera, but lacks fundamental options to build 3D nonAR apps, like camera control (.allowsCameraComtrol).
Is it viable to use RealityKit a replacement for SceneKit (which hasn't been updated for the past years and will likely be desecrated soon)?
","
.allowsCameraControl equivalent
Apple has been preparing a replacement for SceneKit, and it's definitely a Reality family – RealityKit, RealityFoundation, Reality Composer (iOS/macOS), Reality Composer Pro (rather for visionOS), Reality Converter, ARQuickLook, etc. Using RealityKit 2.0, you can create both AR and VR apps. As a tool for prototyping 3D scenes, you need to use Reality Composer. And even despite the lack of such an important feature as .allowsCameraControl, you are now able to make VR apps using RealityKit.
Workaround
Nonetheless, use my code as a starting point to create your own camera control for VR in RealityKit:
import UIKit
import RealityKit

class ViewController: UIViewController {
    @IBOutlet var arView: ARView!
    let buildings = try! Experience.loadScene()
    let camera = PerspectiveCamera()
    var current_X_Angle: Float = 0.0
    var current_Y_Angle: Float = 0.0

    override func viewDidLoad() {
        super.viewDidLoad()
        arView.environment.background = .color(.systemCyan)
        arView.scene.anchors.append(buildings)
        self.camera.position.z = 1
        buildings.children[0].addChild(self.camera)
        self.gestureRecognizer()
    }
    func gestureRecognizer() {
        for gestureRecognizer in [UIPanGestureRecognizer.self,
                                  UIPinchGestureRecognizer.self] {
            if gestureRecognizer == UIPinchGestureRecognizer.self {
                let r = UIPinchGestureRecognizer(target: self,
                               action: #selector(allowCameraControl_01))
                arView.addGestureRecognizer(r)
            }
            if gestureRecognizer == UIPanGestureRecognizer.self {
                let r = UIPanGestureRecognizer(target: self,
                               action: #selector(allowCameraControl_02))
                arView.addGestureRecognizer(r)
            }
        }
    }
    @objc func allowCameraControl_01(recognizer: UIPinchGestureRecognizer) {
        switch recognizer.state {
            case .changed, .ended:
                self.camera.position.z *= 1 / Float(recognizer.scale)
                recognizer.scale = 1.0
            default: break
        }
    }
    @objc func allowCameraControl_02(recognizer: UIPanGestureRecognizer) {
        switch recognizer.state {
            case .changed, .ended:
                let translate = recognizer.translation(in: recognizer.view)
                let angle_X = Float(translate.y / 300) * .pi / 180.0
                let angle_Y = Float(translate.x / 100) * .pi / 180.0
                self.current_X_Angle += angle_X
                self.current_Y_Angle += angle_Y                
                camera.setOrientation(Transform(pitch: current_X_Angle,
                                                  yaw: current_Y_Angle,
                                                 roll: .zero).rotation,
                                      relativeTo: buildings.anchor)
            default: break
        }
    }
}


Here you can find SwiftUI version.

visionOS Xcode Simulator
In Xcode 15's visionOS simulator, there are Pan, Orbit and Dolly UI buttons, available in lower right corner.

"
"
I created a 1:1 replica of my Room in 3D and unity. Now I want to align this virtual room with my real room via VR.
So the Table, that I see in VR matches exactly the real one.
But since I'm using the Oculus Quest, it is recalculating its Position on every start, making it impossible to align the to worlds precisely.
However, I found a script that should allow me to recenter my VR-Headset with the Mesh of my Room.
My only problem is, that I don't have a development background and the short explanations with the code snippets are not detailed enough for me..
Can somebody explain to me what to do and where to put the Code in my Scene?
Link: https://twitter.com/IRCSS/status/1231523329183559681/photo/1
","
Your question is quite broad. In general rather ask the maker of a script how it is to use ;)
But in short for using such a script: (btw in your link in the comments you see the rest o code).

Create a new script within the Assets 
ProjectView -> Right Click -> Create -> C# Script
Call it accoding to the class name AlignMesh
Double click to open it
Past in the code. 
public class AlignMesh : MonoBehaviour
{
    [Tooltip(""Drag in the OVR rig hand used to click the buttons here"")]
    public Transform HandTransform;

    [Tooltip(""Adjust the real world distance between your two target click positions here"")]
    public float ABDistance = 1.45f;

    public enum AligmentState
    {
        None, 
        PivotOneSet, 
        PivotTwoSet 
    }

    [Tooltip(""If your mesh pivot does not match the real-world point A add the offset here. The mesh will be set to point A + OriginOffset"")]
    public Vector3 OriginOffset;
    [Tooltip(""If your mesh forward axis does not match the real-world direction A->B add the offset here. The mesh will be set to A->B * RotationOffset"")]
    public Vector3 RotateOffset;

    public AligmentState alignmentState = AligmentState.None;

    private void Update()
    {
        switch (alignmentState)
        {
            case AligmentState.None:
                if (!OVR.Input.GetDown(OVRInput.Button.One)) return;
                transform.position = HandTransform.position + OriginOffset;
                alignmentState = AligmentState.PivotOneSet;
                break;

            case AligmentState.PivotOneSet:
                if (OVRInput.GetDown(OVRInput.Button.Two))
                {
                    alignmentState = AligmentState.None;
                    return;
                }

                if (!OVRInput.GetDown(OVRInput.Button.One)) return;

                var lookAtPosition = HandTransform.position + OriginOffset;
                var pivotOneToTwo = lookAtPosition - transform.position;
                var scaleFactor = pivotOneToTwo.magnitude / ABDistance;

                transform.LookAt(lookAtPosition, Vector3.up);
                transfom.rotation *= RotationOffset;
                transform.localScale *= scaleFactor;

                alignmentState = AligmentState.PivotTwoSet;
                break;

            case AligmentState.PivotTwoSet:
                if (OVRInput.GetDown(OVRInput.Button.Two))
                {
                    alignmentState = AligmentState.None;
                }
                break;
        }
    }
}

In your scene's Hierarchy select your root mesh object of the world/room you want to align
In the Inspector click Add Component and search for your created AlignMesh
Into the leftHandPosition drag and drop the according hand from the OVR rig. And in ABDistance adjust the real-world distance between your two click positions in meters.


A little explanation what this does:

The first click sets the room to that position 
So make sure that your real-world point A matches with the pivot point of the object in Unity!
The second click is used to set its rotation (so it's forward axis points to B) and scale. 
So make sure that your real-world point B is aligned to A thus that this direction matches with your meshes FORWARD axis

How does it work on a code basis:

None
Initially you are in an IDLE state None where your script only waits for the click on Button.One. 
Once you press the button the room is set to according position and you go to PivotOneSet
PivotOneSet
Now you are waiting for either a Button Two press => Cancel => back to None state
Or your press Button One again to set the second position B
Once this second position is set you adjust the rooms orientation and scale to match with the real world coordinates. Then you go to PivotTwoSet state.
PivotTwoSet
This does basically nothing but waiting for a Button Two click in order to reinitialize the setup process by going back to None state.

"
"
The setup idea:

A virtual machine with the executable virtual reality game inside runs on a server.
Someone can get access to the server from their own PC via VPN.
They can then start the virtual machine and the game and play with the hardware (headset etc.) connected to their PC.

I just need someone knowledgeable to explain to me whether that's possible or not and why. :)
Thanks in advance!
","
I can't imagine that there is software that will allow you to do such a setup with little configuration effort. You would have to take care of everything yourself and build a pipeline that reads the tracking data from your HMD, sends it to the PC, which then forwards it, and then (and this might be the hardest part) passes that data on your server to an official game (if you make the game yourself, the step would be much easier).
Apart from whether it's technically possible to make something like that work somehow, it's not something you want. It won't give you a good experience, as the latency would be way too high. The HMDs themselves are already struggling to keep the latency low enough so that users don't get motion sick.
"
"
I have built a VR project for the Oculus Rift S device with hand controllers in Unity 2020.3.43f, and now I am trying to build it.
I get the following error:
BuildFailedException: OpenXR backend for Oculus Plugin is disabled, which is required to support Unity OpenXR Plugin. Please enable OpenXR backend for Oculus Plugin through the 'Oculus -> Tools -> OpenXR' menu.
This menu item doesn't exists, this is the closest thing I could find is this:

Selecting Set OVRPlugin to OpenXR allows me to build the project, but the index finger no longer folds when pressing the index triggers for some reason.
I am using the built-in hand controllers prefabs that were imported with the Oculus unitypackage, and haven't changed anything.
Another thing I tried was to select the OpenXR in the Plug-in provider under the XR Plug-in management in the project settings.

But it disables the Oculus:

So, how do I enable that OpenXR backend?
P.S., I am using OVRPlugin version 1.78.0
P.S.2, when I debugged the code I noticed that it checks for Android platform support for some reason. Which I don't have nor need.
","
The solution was pretty simple and weird, I just had to comment/delete the line the throws the error in OVRGradleGeneration.cs
if (!useOpenXR)
{
    // throw new BuildFailedException(""OpenXR backend for Oculus Plugin is disabled, which is required to support Unity OpenXR Plugin. Please enable OpenXR backend for Oculus Plugin through the 'Oculus -> Tools -> OpenXR' menu."");
}

This solved all the issues.
"
"
This is my current script to hide a selected object with a press of a button (UI). The button has an onClick function assigned to Hide() and the SelectObject function is assigned to the object Select entered for XR Grab Interactable events:
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class HideObject : MonoBehaviour
{
    private GameObject selectedObject;

    public void Hide()
    {
        Debug.Log(""Selected object in hide: "" + selectedObject);
        if (selectedObject.activeInHierarchy)
        {
            selectedObject.SetActive(false);
        }
        else
        {
            selectedObject.SetActive(true);
        }
    }


    public void SelectObject(GameObject obj) 
    {
        selectedObject = obj;
        Debug.Log(""Selected object "" + selectedObject);
    }

}

However, the log tells me that selected object in SelectObject can be detected (the select works) but not that in Hide. I'm selecting an object using the controller grip button and hitting the button with the trigger of the controller.
An image is attached below to show hierarchy and logs:

I've tried googling and looking at the forums for help but to no avail. Was wondering if it could be the trigger buttons (to select the UI button) resulting in overriding the original selected object.
","
I managed to solve it. I had 2 instances of the script running - one attached to the Hide button and the other to the GameObject. The one attached to the Hide button caused the null reference.
To satisfy my requirements, I went ahead to create another script to reference the selected GameObject and then declare another function in the new script to hide the object on button click.
"
"
I am quite confused by the ""rotation"" and ""orientation"" for a SCNNode. In Apple's doc, they are defined quite similarly:
orientation: The node’s orientation, expressed as a quaternion. Animatable.
rotation: The node’s orientation, expressed as a rotation angle about an axis. Animatable.
And apple doc says: 
The rotation, eulerAngles, and orientation properties all affect the rotational aspect of the node’s transform property. Any change to one of these properties is reflected in the others.
So they kind of control the same thing but are using different format? That is my current understanding. But how? They are both SCNVector4 type. I understand the rotation but I am not sure how to set the orientation and how it is different.
(EDIT)
I just tried to make a default node by SCNNode() and print out its rotation and orientation:
Rotation: 
SCNVector4(x: 0.0, y: 0.0, z: 0.0, w: 0.0)
Orientation:
SCNVector4(x: 0.0, y: 0.0, z: 0.0, w: 1.0)
I am still not sure why there is 1.0. E.comm has mentioned that it keeps the definition of quaternion but that w means a rotation degree in SCNVector4 for rotation. So I am not sure why it is there since I did not rotate node in my code. 
","
There's some obvious difference between Orientation and Rotation in SceneKit framework. Orientation is expressed as a quaternion (whose result of the equation must be 1).
According to Apple documentation:

Orientation – The node’s orientation, expressed as a quaternion. Animatable.

A quaternion is a mathematical construct useful for describing rotations in three-dimensional space. Although its implementation differs from that of a 4-component vector, you specify a quaternion value using the same fields as an SCNVector4 structure.
SceneKit uses unit quaternions – those whose components satisfy the equation:
x² + y² + z² + w² == 1

for the orientation property of nodes.
var orientation: SCNQuaternion { get set }  // instance property

// There are four elements (x, y, z, w) in Structure `float4`
var orientation: SCNQuaternion = SCNQuaternion(v: float4)

typealias SCNQuaternion = SCNVector4


Rotation – The node’s orientation, expressed as a rotation angle about an axis. Animatable. The four-component rotation vector specifies the direction of the rotation axis in the first three components and the angle of rotation (in radians) in the fourth. The default rotation is the zero vector, specifying no rotation. Rotation is applied relative to the node’s pivot property.

var rotation: SCNVector4 { get set }  // instance property

var rotation: SCNVector4 = SCNVector4(x: CGFloat,
                                      y: CGFloat,
                                      z: CGFloat,
                                      w: CGFloat)

As you can see orientation instance property is expressed in SCNQuaternion type but rotation instance property is expressed in SCNVector4  type.
"
"
I'm pretty new to VR development with Unity, what I need to do is to grab a clockwork object and rotate it around its pivot by moving the right hand controller, like if I had to do that in real life
Here's the scenario:

Here's the code I'm using in Update:
public class Box1: MonoBehaviour { 
    public Transform rightHandTransf;
    Vector3 handPosition; 

   
  void Update(){ 
     Vector3 rhPos = rightHandTransf.position; 
     Quaternion rhRot = rightHandTransf.rotation; 

     // clockworkM Rotation 
     rhPos.z = clockworkM.transform.position.z - Camera.main.transform.position.z; 
     handPosition = Camera.main.WorldToScreenPoint(rhPos); 
     clockworkM.transform.rotation = Quaternion.LookRotation(Vector3.back, handPosition - clockworkM.transform.position);
  }

}
What I get is that wherever I move the hand, the clockwork rotates just a little bit, and if I rotate the headset, the clockwork rotates based on my X axis rotation with my headset
Weird behavior, I have no idea what I'm doing wrong :(
","
you probably solved it by now but in case someone else stumbles on this.
My solution is just for rotating object around Y axis but can be easily edited for any other direction:
public Transform interactorTransform;
float lastFrameAngle;
public float multiplier = 1;

private void Start() {
    XRGrabInteractable interactble = GetComponent<XRGrabInteractable>();
    interactble.selectEntered.AddListener(Selected);
    interactble.selectExited.AddListener(Deselected);
}

private void FixedUpdate() {
    if (interactorTransform != null) {
        float angle = Vector3.SignedAngle(interactorTransform.position - transform.position, interactorTransform.forward, Vector3.up);
        float delta = angle - lastFrameAngle;
        transform.Rotate(transform.up, delta*multiplier);           
        lastFrameAngle = angle;
    }
}

public void Selected(SelectEnterEventArgs arguments) {        
    interactorTransform = arguments.interactorObject.transform;
    lastFrameAngle = Vector3.SignedAngle(interactorTransform.position - transform.position, interactorTransform.forward, Vector3.up);        
}

public void Deselected(SelectExitEventArgs arguments) {        
    interactorTransform = null;        
}

"
"
I'm trying to use AR in my flutter app. I'm using the package arcore_flutter_plugin.
After adding the plugin, I'm unable to run the app and I get this error (after having disabled the null safety feature) :
e: C:\Users\LENOVO THINKPAD\OneDrive\Bureau\Hackathon\ar_test\android\src\main\kotlin\com\difrancescogianmarco\arcore_flutter_plugin\ArCoreView.kt: (245, 38): Object is not abstract and does not implement abstract member public abstract fun onActivityCreated(p0: Activity, p1: Bundle?): Unit defined in android.app.Application.ActivityLifecycleCallbacks
e: C:\Users\LENOVO THINKPAD\OneDrive\Bureau\Hackathon\ar_test\android\src\main\kotlin\com\difrancescogianmarco\arcore_flutter_plugin\ArCoreView.kt: (246, 13): 'onActivityCreated' overrides nothing

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':arcore_flutter_plugin:compileDebugKotlin'.
> Compilation error. See log for more details

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org

BUILD FAILED in 1m 57s

Can someone help me make it work ?
","
This is an error in the arcore_flutter_plugin.
Navigate into the ArCoreView.kt file:

C:\Users\LENOVO
THINKPAD\OneDrive\Bureau\Hackathon\ar_test\android\src\main
\kotlin\com\difrancescogianmarco\arcore_flutter_plugin\ArCoreView.kt

Remove '?' from the following line:
override fun onActivityCreated(activity: Activity?, savedInstanceState: Bundle?)

New-line:
override fun onActivityCreated(activity: Activity, savedInstanceState: Bundle?)

"
"
I will try to describe this situation as accurately as possible, apologies if some details are missing.
A few years ago we began experimenting with virtual reality apps for STEM education developed in Unity.  We were specifically interested in viewing molecular structures of proteins and small molecules.  Since we weren't sure of the computational load of loading full structures, we decided to first try downloading the molecules from the RCSB database into Chimera, coloring and styling them the way we wanted, then saving them as 3D objects and importing them into Unity.  This worked except it striped out all of the colors (for some reason, this didn't happen to small molecules downloaded from PubChem).  We tried a variety of methods to see if we could preserve the colors.  We saved in every common 3d file format, we tried converting between formats, we tried passing through different 3d modeling software (Blender, Rhino and MeshLab).  I seem to recall MeshLab loaded the colors correctly but we still couldn't get them correctly into Unity.
I did a deep dive in the web to see if I could find an answer and the best I was able to do was to find a website that implied that by convention, most molecular visualization software colored the molecules using vertex coloring instead of meshes.  That sounds odd to me, especially when talking about molecular surfaces, but I've never been able to find the site again to explore the issue in more depth.
It's mostly a moot point now as we switched to UnityMol once we became more comfortable with the development process, but a reviewer on one of our manuscripts is interested in the technical challenges we faced in development.  I would like to mention the color issue but preferably in a way that makes sense.  Does anyone who has worked in the molecular visualization space have any idea what I'm talking about?
","
I think the source of the confusion stems from the fact that most of the standard  Unity shaders don't represent vertex colors in any way. You would need a custom shader for that or AFAIK the Unity standard particle shader also shows vertex colors.
Vertex colors are not used 'instead' of meshes but on top of the other vertex data inside the mesh, such as: Vector3 for position, Vector2 uv (or texcoord), Color for vertex color (float4 inside a shader) and so on. For any mesh or 3D model, some of them are optional, as with vertex colors.
Many 3D models do not ""use"" vertex colors in any way so they often aren't even saved inside the model file. Still, if you want to work without textures but have some nice colors, vertex colors are perfect for that (e.g. in scientific contexts).
Let me know if this helps or if you need more information!
"
"
I need a scene in Unity that matches the real world.
This guy made all i want.
https://twitter.com/vr_oasis/status/1218493216401608704
Is there any easy solution to solve this?
Thank You for Your Kindness ^o^
","
If you want an easy solution,
Get on a phone and go on the app store or google play store and find a 3D scanning app. Warning: older phones may not have the features that make 3d scanning possible.
Take a 3d scan of the room and export it to your computer.
The 3d model might not look that good so I'd recommend getting a 3d modelling program like Blender. It will probably not look like what that guy made though.
You can try to 3d model everything from scratch like how the video probably did it but it will be timeconsuming.
If you want to replicate that video not only the 3d models:
Tracking the positions from the real world to virtual reality seems complex and hard. If you want an easy way, I'd recommend to fake it by using Unity Timeline to move the camera or somehow standing in the same place that you're scene starts when using VR.
Good Luck!
"
"
I purchased a few 3D models to use in an app I am making. The purchased files came in a .fbx format, so I have converted them to .dae. 
Unfortunately, when I put them into the art.scnassets folder in Xcode and view them, they just display as an orange outline of what the model should be. There is no texture. The same happens if I select the .dae then go to Editor, and ""Convert to SceneKit scene file format (.scn). 
I found the .tga files for the texture of the models. Each model has 3 .tga texture files (albedo, ao, and normals). Can I combine these files to make one texture for use in Xcode? 
Also, when the 3d model is selected, I cannot open the material inspector, it says it is Not Applicable. I have always used the material inspector to apply the texture file to the specific model. 
","
After converting a .dae model into .scn Xcode's native format, you need to apply all available textures to your 3D model via Properties slots in Material Inspector.

The best format for textures in Xcode could be .png, because .png files have relatively small size and can hold four channels – RGBA (.jpg holds only RGB).
You shouldn't merge Albedo, AO, and Normals because these render passes are for different slots of Material Inspector: Albedo for diffuse color, AO for Ambient Occlusion soft shadows, Normals for bump effect.
You can assign these textures programmatically:
let material = SCNMaterial()

material.diffuse.contents = UIImage(named: ""Albedo.png"")
material.ambientOcclusion.contents = UIImage(named: ""AO.png"")
material.normal.contents = UIImage(named: ""Normals.png"")

P. S.
If you can't see any parts of your 3D model in Xcode's Scene Graph, there's a polygons' normals issue. You need to reverse normals in 3D authoring software.
"
"
For context, I am using Unity to develop a VR Game. In this game, the rotation of your hands can alter the direction of flight (Similar to ""Richie's plank simulator""). I am trying to apply a force to the character rigidbody that is opposite to the orientation of the hands. How would I go about doing this?
I tried to do this by writing my own function to convert from transform.localRotation of each hand into a normalized Vector3 (that I can apply on the main rigidbody using rb.AddForce()) that represents the amount of force in each direction (-1 = backwards along the axis, 0 = nothing, 1 = forwards along the axis). I cant work out how to do this simply. I would assume unity has a built in method but i cant find anything along the lines of what im trying to do right now.
EDIT: Is it transform.eulerAngles?
","
After your clarification in the comments, I can suggest you the following approach:

Get the vector pointing out of your hands (more specifically palms from what I got from your comment) in world (player) coordinates since AddForce() applies in world coords.

For that, either you can determine it directly in world coords or get the vector in local coords and transform it into world coords. Maybe you can use the Transform.forward, .up or .right vector if one of them is pointing in that direction locally. To convert from local to world coords, use playerTransform.TransformDirection(localVector3dPointingOutOfPalm). This assumes that the player is not parented in any way and actually in world coordinates meaning on the highest level in scene hierarchy. (Please someone correct me here if I'm mistaken).

Do this for the left and right hand/palm.
If you have used the Transform.forward, .up or .right vector, you already have normalized vectors at hand. If this was not the case, normalize the resulting world vectors using Vector3.Normalize() or Vector3.normalized.
Now you have world/player vectors pointing out of each palm which are of magnitude one. Before applying a force, you can adjust the intensity by scaling (multiplying) the vectors by some factor.
What's left to do is to actually AddForce to the player:)

"
"
In Node.js I wrote:
const axios = require(""axios"");
const vrchat = require(""vrchat"");

Here's the full error message:
node:internal/modules/cjs/loader:571
      throw e;
      ^

Error [ERR_PACKAGE_PATH_NOT_EXPORTED]: Package subpath './lib/defaults' is not defined by ""exports"" in /Users/owner/vrchat_api/node_modules/axios/package.json
    at new NodeError (node:internal/errors:399:5)
    at exportsNotFound (node:internal/modules/esm/resolve:361:10)
    at packageExportsResolve (node:internal/modules/esm/resolve:697:9)
    at resolveExports (node:internal/modules/cjs/loader:565:36)
    at Module._findPath (node:internal/modules/cjs/loader:634:31)
    at Module._resolveFilename (node:internal/modules/cjs/loader:1061:27)
    at Module._load (node:internal/modules/cjs/loader:920:27)
    at Module.require (node:internal/modules/cjs/loader:1141:19)
    at require (node:internal/modules/cjs/helpers:110:18)
    at Object.<anonymous> (/Users/owner/vrchat_api/node_modules/axios-cookiejar-support/lib/index.js:8:40) {
  code: 'ERR_PACKAGE_PATH_NOT_EXPORTED'
}

Node.js v18.14.2

Curiously, the following code does not generate an error.
const axios = require(""axios"");

I have removed package-lock.json and node_modules and npm install and get the same error.
How does one resolve it?
","
This error usually occurs when there is a mismatch between the version of the axios package and the version of Node.js being used.
You can try updating your Node.js version to the latest stable version and then reinstalling the axios package.
Alternatively, you can try using an older version of the axios package that is compatible with your current version of Node.js.
To do this, you can uninstall the current version of axios using the following command:
npm uninstall axios

Then, install an older version of axios using the following command:
npm install [email protected]

Replace 0.19.2 with the version that is compatible with your current version of Node.js.
Once you have installed the compatible version of axios, try running your code again and see if the error persists.
"
"
I am currently developing a 3d VR game for Oculus Quest headset on Unity (v 2019.3.6f1)
Whenever I start the game in Unity editor (by pressing ""Play"" button) I get the following errors :
Unable to start Oculus XR Plugin.  
Failed to load display subsystem. 
Failed to load input subsystem.


XR Plugin is installed and updated to the latest version (1.2.0) : 


What could be the cause of those errors ? Thanks in advance for your answers.
","
Launching problem was resolved by removing Avatar object from my scenes.
"
"
I have a unity project with a user controller avatar, quad surface 1, another avatar, and another quad surface.  I am attempting to replicate my movements in VR onto the avatar representation of my player character relative to created surfaces. For example, if I move to the left of my created surface, then a representation of my avatar should also move to the left of its own surface. Similarly, if I point to the top left corner of my surface, then my representation should also point to the top left of its own surface.
I accomplish this by mapping my avatar's parent object (Player1) position and rotation relative to my quad surface. From there, I project the position and rotational values onto the parent object of the avatar's representation (Player2) relative to its own surface. Afterward each of the representation's parts (head, left hand, right hand) are updated to match the local position of the player-controlled avatar's parts.
In general, the movements and overall position/rotation of my avatar are mapped correctly. I begin to encounter issues, however, when I try to be more precise (e.g. touching a corner of the surface.) In this scenario, I will be touching the corner while my avatar representation will be touching near the area, but not exactly in the corner. It should be noted that both surfaces have the same scale values. Below is the code that I am using to map my movements onto the avatar.

using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class PuppetMovement : MonoBehaviour
{

    //Player Variables
    public GameObject Player1;
    public GameObject Player1Head;
    public GameObject Player1LeftHand;
    public GameObject Player1RightHand;
    
    //Representation Variables
    public GameObject Player2;
    public GameObject Player2Head;
    public GameObject Player2LeftHand;
    public GameObject Player2RightHand;
    
    //Surface Variables
    public GameObject Surface1;
    public GameObject Surface2;

    // Update is called once per frame
    void Update()
    {
        Vector3 playerRelativeToSurface = Surface1.transform.InverseTransformPoint(Player1.transform.position);
        
        Quaternion playerRotationRelativeToSurface = Player1.transform.rotation * Quaternion.Inverse(Surface1.transform.rotation);
        Player2.transform.rotation = Surface2.transform.rotation * playerRotationRelativeToSurface ;
        
        Vector3 rotatedDirection = Surface2.transform.rotation * playerRelativeToSurface;
        
        Player2.transform.position = rotatedDirection + Surface2.transform.position;

        Player2Head.transform.localPosition = Player1Head.transform.localPosition;
        Player2Head.transform.localRotation = Player1Head.transform.localRotation;
        
        Player2LeftHand.transform.localPosition = Player1LeftHand.transform.localPosition;
        Player2LeftHand.transform.localRotation = Player1LeftHand.transform.localRotation;
        
        Player2RightHand.transform.localPosition = Player1RightHand.transform.localPosition;
        Player2RightHand.transform.localRotation = Player1RightHand.transform.localRotation;
        
    }
}


This is the view from the player controlled avatar. As you can see the right controller is touching the top right corner.

This is what the representation avatar looks like. It's in the general area, but is not precisely where I am touching my surface.
I would appreciate any help in solving this issue. Thank you!
","
I can't tell exactly where your code fails except it doesn't take scaling into account.
The back conversion of the
Vector3 playerRelativeToSurface = Surface1.transform.InverseTransformPoint(Player1.transform.position);

would actually simply be
Player2.transform.position = Surface2.transform.TransformPoint(playerRelativeToSurface);


Little side note: If you directly use Transform for the fields
public Transform Player1;
public Transform Player1Head;
public Transform Player1LeftHand;
public Transform Player1RightHand;
// ...

you can get rid of the .transform everywhere which imho makes it a bit cleaner.
"
"
I was trying to figure it out if Meta Quest Pro is compatible with SteamVR. I have searched all the internet, and also tried to ask to ChatGPT, but I found no answer to this question. Anyone knows something about it?
","
I have figured it out, and Meta Quest Pro is compatible with SteamVR
"
"
I had to comment out a bunch of my code but it came down to this with 2 errors. The issue seems to be with the line that uses 'GetDevicesWithCharacteristics' because that is where the errors point too but i checked the documentation and it seems good. I also checked the line above and i'm not sure if the ', lefthandedcontrollers;' that i added after the fact to follow the documentation(which wasn't actually the problem).
I'm inexperienced so please forgive me if this is a simple mistake.

using UnityEngine.Events;
using UnityEngine.XR;

using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class controlerInputScript : MonoBehaviour
{
    GameObject gameObjectToMove;
    public int scale = 3;


    var leftHandedControllers = new List<UnityEngine.XR.InputDevice>();
    InputDeviceCharacteristics leftcontrollerdesiredCharacteristics = UnityEngine.XR.InputDeviceCharacteristics.HeldInHand | UnityEngine.XR.InputDeviceCharacteristics.Left | UnityEngine.XR.InputDeviceCharacteristics.Controller, leftHandedControllers;
    InputDevices.GetDevicesWithCharacteristics(XR.InputDeviceCharacteristics leftcontrollerdesiredCharacteristics, List<InputDevice> leftHandedControllers);


    // var rightHandedControllers = new List<UnityEngine.XR.InputDevice>();
    // var rightcontrollerdesiredCharacteristics = UnityEngine.XR.InputDeviceCharacteristics.HeldInHand | UnityEngine.XR.InputDeviceCharacteristics.Right | UnityEngine.XR.InputDeviceCharacteristics.Controller;
    // UnityEngine.XR.InputDevices.GetDevicesWithCharacteristics(rightcontrollerdesiredCharacteristics, rightHandedControllers);


    // if (leftHandedControllers > 1) || (rightHandedControllers >1){
    //     Debug.Log(""DEBUG: ERROR: More than one Left or Right handed controller detected!"")
    // }

    //assign Controlllers
    InputDevice leftController = leftHandedControllers[0];
    //InputDevice rightController = rightHandedControllers[0];

    // Start is called before the first frame update
    void Start()
    {
        
    }

    // // Update is called once per frame
    void Update()
    {
        Vector3 p = this.transform.position;
        float leftTriggerState;
        if (leftController.TryGetFeatureValue(CommonUsages.trigger, out leftTriggerState)){
            p.y = scale * leftTriggerState;
            gameObjectToMove.transform.position = p;
        }
     
    //     float rightGripState;
    //     if (rightController.TryGetFeatureValue(CommonUsages.grip, out rightGripState)){
    //         p.x = scale* rightGripState;
    //         gameObjectToMove.transform.position = p;
    //     }
    }
}

The Errors were:
Assets\ConQuestV3\controlerInputScript.cs(58,47): error CS1519: Invalid token '(' in class, struct, or interface member declaration

Assets\ConQuestV3\controlerInputScript.cs(58,156): error CS1519: Invalid token ';' in class, struct, or interface member declaration

Thanks for any help in advance!
","
I'm talking about those lines
var leftHandedControllers = new List<UnityEngine.XR.InputDevice>();
InputDeviceCharacteristics leftcontrollerdesiredCharacteristics = UnityEngine.XR.InputDeviceCharacteristics.HeldInHand | UnityEngine.XR.InputDeviceCharacteristics.Left | UnityEngine.XR.InputDeviceCharacteristics.Controller, leftHandedControllers;
InputDevices.GetDevicesWithCharacteristics(XR.InputDeviceCharacteristics leftcontrollerdesiredCharacteristics, List<InputDevice> leftHandedControllers);

in the second line, are you sure about this

, leftHandedControllers;

because this syntax is incorrect and does not have any semantics also.
In the third line, what are you trying to do in the parameters! you are redefining the previous variables in the function parameters!
I believe your code should be like:
var leftHandedControllers = new List<UnityEngine.XR.InputDevice>();
InputDeviceCharacteristics leftcontrollerdesiredCharacteristics = UnityEngine.XR.InputDeviceCharacteristics.HeldInHand | UnityEngine.XR.InputDeviceCharacteristics.Left | UnityEngine.XR.InputDeviceCharacteristics.Controller;
InputDevices.GetDevicesWithCharacteristics(leftcontrollerdesiredCharacteristics, leftHandedControllers);

"
"
I bought oculus quest 2 recently and using unity, i am trying to make games with it, but when i downloaded Oculus Integration, and walked through Getting started,

i downloaded the package
downloaded oculus developer hub (and created developer account)
paired the device and opened one of sample scenes

there was no problem, headset was working perfectly and i can interact with objects using controllers/hands,
until i try to test the game second time...
When i tried to play the game without changing anything second time, got this window

waited for few minutes then opened task manager just to see this

then i restarted the game, everything is fine... at the second play, all stuck at Application.Reload.
Made some testing with different setups :

Opened completely new scene twice (No problem)
Added the new scene an empty GameObject and added ""OVR Camera Rig"" and ""OVR Manager"" (Problem Occurs)
Reloaded same scene at the run time (Scene reloads without problem, but Problem Occurs)
Removed OVR Camera (Problem Occurs)
Removed OVR Manager Added OVR Camera back (No problem)
Played Scene with OVR Manager script First, then opened a completeley new scene (Problem Occurs)

I am using Unity 2020.3.12f1
Downloaded Oculus Integration 39.0
I'm out of ideas...




Edit : 
I found a workaround, for some reason OVR Manager Script causing this behaviour so i tried creating it on play mode and it worked!
So write a script that creates OVR Manager on start, but for some reason problem came back. Even though i created OVR Manager on play time (with addcomponent on start specifically) still editor stucks on starting the game.
However my workaround is creating OVR Manager after 1 sec of game start...
I know its a petty solution but i can't lose more time...
Feel free to Respond to this question maybe in future someone faces the same problem.
","
If you haven't solved this yet, updating to Unity 2020.3.35f solved it for me.
"
"
Recently I updated my Xcode to 11.3.1. But while working with SceneKit, I found that I can't create a particle system file.

Before



After


How can I create a particle system in a file now?
","
SceneKit Library
In Xcode 14 / 13 / 12 / 11 you have no preconfigured .scnp particle system files anymore. Instead, you can use a Particle System object coming from a Xcode Library (with the same settings in Attributes Inspector as they were in Xcode 10).

If you manually placed a Particle System from library into SceneKit's Scene graph you can then retrieve it and setup programmatically. Let's see how it looks like:
let particlesNode = sceneView.scene?.rootNode.childNode(withName: ""particles"", 
                                                     recursively: true)

particlesNode?.particleSystems?.first?.isAffectedByGravity = true
particlesNode?.particleSystems?.first?.acceleration.z = 5.0


Creating particles programmatically
Or you can easily create a Particle System from scratch using just code:
let particleSystem = SCNParticleSystem()
    
particleSystem.birthRate = 1000
particleSystem.particleSize = 1.45
particleSystem.particleLifeSpan = 2
particleSystem.particleColor = .yellow

let particlesNode = SCNNode()
particlesNode.addParticleSystem(particleSystem)

sceneView.scene!.rootNode.addChildNode(particlesNode)

Creating .scnz file containing Particle System

Select a .scn file in Project Navigator (left pane) and choose File – Export...
In drop-down menu choose Compressed Scenekit Scene Document .scnz


Or you can create .scnp file by renaming .scn – the same way @ycao proposed.
"
"







Closed. This question needs debugging details. It is not currently accepting answers.
                                
                            











 Edit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question.


Closed 3 years ago.







                        Improve this question
                    



I am implementing the exercise that comes in the documentation of virtual reality of how to implement a VR view:

https://github.com/googlevr/gvr-android-sdk/tree/master/samples/sdk-simplepanowidget

I tested it with my Nexus 5 and it works fine but with a Motorola MotoG 3, I do not see the icon to be able to see it in the glasses of virtual reality and apart it does not move when you move the mobile.
I would like to know if there is any way to make the example work on any device, and how the scenario looks for the future.
","
This is debit for gyroscope sensor. If it's not present not runs well.
"
"
I need to reset the player's position in the main menu scene to 0,0,0, whether or not they have saved their playerprefs.
This is the code I'm using in Unity to reset the player's position when they exit to the Main Menu scene. This is a VR game. It works for players who haven't saved, but doesn't reset the position of players who have saved their playerprefs, and I don't know how to make that happen.
My reset position script:

//reset position of player in main menu scene
[SerializeField] Transform playerSpawnPosition;
[SerializeField] GameObject player;
[SerializeField] private Camera playerHead;
public void ResetPosition()
{
   var rotationAngleY = playerHead.transform.rotation.eulerAngles.y 
                        - playerSpawnPosition.rotation.eulerAngles.y;
       
   player.transform.Rotate(0, -rotationAngleY, 0);
    
   var distanceDifference = playerSpawnPosition.transform.position 
                            - playerHead.transform.position;
       
   player.transform.position += distanceDifference;
}

   public void ExitToMainMenu(){
      
      if(pauseMenu == null){
         return;
      }
      pauseMenu.SetActive(false);
      if (LeftHand && RightHand)
      {
         Time.timeScale = 1f;
         LeftHand.transform.parent = leftparent;
         RightHand.transform.parent = rightParent;
      }
      
      SceneManager.LoadScene(0,LoadSceneMode.Single);
      
      ResetPosition();
      
   }


My Save Player Position script:

public void SavePlayerPosition(){
   hpc = FindObjectOfType<HVRPlayerController>();
   if(hpc == null){
      return;
   }
   PlayerPrefs.SetFloat(""PlayerXPos"",hpc.transform.position.x);
   PlayerPrefs.SetFloat(""PlayerYPos"",hpc.transform.position.y);
   PlayerPrefs.SetFloat(""PlayerZPos"",hpc.transform.position.z);
   Vector3 v = hpc.transform.rotation.eulerAngles;
   PlayerPrefs.SetFloat(""PlayerYRot"",v.y);
   
   PlayerPrefs.SetString(""SpawnId"", ""xyzSave"");
   Debug.Log(""Saved player to position "" + hpc.transform.position.x + "" ""
      + hpc.transform.position.y + "" ""+ hpc.transform.position.z + "" "");
}
public void LoadPlayerPosition(){
   hpc = FindObjectOfType<HVRPlayerController>();
   if(hpc == null){
      return;
   }
   playerX = PlayerPrefs.GetFloat(""PlayerXPos"");
   playerY = PlayerPrefs.GetFloat(""PlayerYPos"");
   playerZ = PlayerPrefs.GetFloat(""PlayerZPos"");
   playerYRot = PlayerPrefs.GetFloat(""PlayerYRot"",hpc.transform.rotation.y);
   Debug.Log(""Loaded player position "" + playerX + "" ""
      + playerY + "" ""+ playerZ);
}


","
You're likely calling LoadPlayerPosition() after every position reset. If it's called in a Start() method in a monobehaviour, it's likely going to get called after ResetPosition().
If this is the case, the 0,0,0 values for your players position might not even be set by ResetPosition, instead they're set to the default values returned by PlayerPrefs.GetFloat(), '0.0'.
To Test
Put breakpoints on ResetPosition(); and hpc = FindObjectOfType<HVRPlayerController>();. In the case where a player has set player prefs, I have a feeling your will observe that ResetPosition(); is called before hpc = FindObjectOfType<HVRPlayerController>();.
To Fix
This is a bit tricky. The solution I'd play around with is tying the two functions together in another function call. The function would have a parameter isMainMenuReset. If it's true, call ResetPosition(), else, call LoadPlayerPosition(). You'd have to replace all LoadPlayerPosition() calls in your codebase with this new function call.
"
"
In the project I have added Player Movement with Joystick for both Right Controller and Left Controller.
There are objects for which XRGrabIntractable Script attached so that they are Grabbable when Ray is Pointed and Triggered.
when I use Trigger button and Pick up one of the object it is Grabbed.
But when I use the Joystick to move the player Grabbed Object in Hand also moves and Rotates.
Is it possible to disable the movement of the Grabbed Object when we use Joystick for player movement?
Unity Version: 2021.3.1f
Device: Pico Nio 3
XR Interaction Toolkit Version: 2.2.0
Checking to see if there is any default setup to disable movement of Grabbed Object with joystick.
Thanks in advance
","
Yes you have to uncheck the Anchor control checkbox. See picture below.

"
"

I have a simple mesh cube, and I want the same image to be applied on each face of the cube :
Here is what I have so far (front face is ok but not the others)
# create mesh with texture coordinates
stage.DefinePrim('/' + assetName + '/Geom', 'Scope')
mesh = UsdGeom.Mesh.Define(stage, '/' + assetName + '/Geom/cube')
mesh.CreateSubdivisionSchemeAttr().Set(UsdGeom.Tokens.none)
mesh.CreatePointsAttr([(-x/2, -y/2, z/2), (x/2, -y/2, z/2), (-x/2, y/2, z/2), (x/2, y/2, z/2), (-x/2, y/2, -z/2), (x/2, y/2, -z/2), (-x/2, -y/2, -z/2), (x/2, -y/2, -z/2)])
mesh.CreateExtentAttr(UsdGeom.PointBased(mesh).ComputeExtent(mesh.GetPointsAttr().Get()))
mesh.CreateNormalsAttr([(0,0,1), (0,1,0), (0,0,-1), (0,-1,0), (1,0,0), (-1,0,0)])
mesh.SetNormalsInterpolation(UsdGeom.Tokens.uniform)

mesh.CreateFaceVertexCountsAttr([4, 4, 4, 4, 4, 4])
mesh.CreateFaceVertexIndicesAttr([0,1,3,2, 2,3,5,4, 4,5,7,6, 6,7,1,0, 1,7,5,3, 6,0,2,4]) # per-face vertex indices
    
texCoords = mesh.CreatePrimvar('st', Sdf.ValueTypeNames.TexCoord2fArray, UsdGeom.Tokens.faceVarying) # a 'faceVarying' mesh attribute is stored per-face per-vertex
texCoords.Set([(0, 0),(1, 0), (1,1), (0, 1), (0, 0), (1, 0), (1, 1), (0, 1), (0, 0), (1, 0), (1, 1), (0, 1), (0, 0), (1, 0)])

texCoords.SetIndices(Vt.IntArray([0,1,2,3, 3,2,4,5, 5,4,6,7, 7,6,8,9, 1,10,11,2, 12,0,3,13]))

Front face has the texture as intended, but others are weird. I don't understand how texture Coordinates are defined.
Thanks for helping.
","
USDA Python scripting
I have the following solution, but it has incorrect mapping (you should reconstruct this cube primitive from scratch). However, this example clearly shows you how the texture is mapped into each individual face of the USDA model.

#usda 1.0
(
    defaultPrim = ""Cube""
    upAxis = ""Y""
)

def Xform ""Cube"" (kind = ""component"")
{
    def Mesh ""polyCube""
    {    
        float3[] extent = [(-0.5, -0.5, -0.5), (0.5, 0.5, 0.5)]    
        int[] faceVertexCounts = [4, 4, 4, 4, 4, 4]    
        int[] faceVertexIndices = [ 0, 1, 3, 2,  2, 3, 5, 4,  4, 5, 7, 6, 
                                    6, 7, 1, 0,  1, 7, 5, 3,  6, 0, 2, 4 ]

        point3f[] points = [ (-0.5,-0.5, 0.5), (0.5,-0.5, 0.5), 
                             (-0.5, 0.5, 0.5), (0.5, 0.5, 0.5), 
                             (-0.5, 0.5,-0.5), (0.5, 0.5,-0.5), 
                             (-0.5,-0.5,-0.5), (0.5,-0.5,-0.5) ]

        texCoord2f[] primvars:st = [ (0, 0), (0, 1), (1, 0), (1, 1), (0, 0), 
                                     (0, 1), (1, 0), (1, 1), (0, 0), (0, 1), 
                                     (1, 0), (1, 1), (0, 0), (0, 1) ] (    
            interpolation = ""faceVarying""
        )

        int[] primvars:st:indices = [ 0, 1, 2, 3,   3, 2, 4, 5,    5, 4, 6, 7, 
                                      7, 6, 8, 9,   1, 10, 11, 2,  12, 0, 3, 13 ]

        color3f[] primvars:displayColor = [(0.75, 0.75, 0.75)]
        rel material:binding = </Texturing/Material>
    } 
}

Here's a shader:
def Xform ""Texturing"" 
{
    def Material ""Material""
    {
        token outputs:surface.connect = </Texturing/Material/Surface.outputs:surface>

        def Shader ""Surface""
        {
            uniform token info:id = ""UsdUVTexture""
            asset inputs:file = @image.jpg@

            token inputs:wrapS = ""repeat""
            token inputs:wrapT = ""repeat""
            token outputs:rgb

            color3f inputs:diffuseColor.connect = </Texturing/Material/Surface.outputs:rgb>    
            token outputs:surface
        }
    }
}

"
"







Closed. This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet Stack Overflow guidelines. It is not currently accepting answers.
                                
                            











 We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.


Closed 1 year ago.







                        Improve this question
                    



I am new to unity and testing VR development with Oculus quest 2 using XR Plugin management. I want to make a simple program where if user looks at left or right side I want to console it. It would be great if anyone can guide me or refer me to some good tutorial or docs.
","
Grab a reference to the camera, it will be always oriented the way the user is looking, convert it to Euler, and its Y will tell you the angle on the vertical axis. More important issue is - you need to establish what do you mean by left and right - in a way the user is always looking forward, left and right are relative against something. It would probably be best to have a reference transform on the scene that establishes a 'forward' direction, you could then 'flatten' both forward vectors (zero out y component of the transform.forward), and use Vector3.SignedAngle(Vector3 from, Vector3 to, Vector3.up); to get a value in degrees, you can use directly - negative angle whill be left while positive will be right (possibly the other way around, depending on parameter order).
"
"
I have a Unity3D VR application (platform agnostic) that needs to navigate from the app to settings and back.
What we are doing is trying to install an APK from within our VR application. In this process, the android OS must confirm with the user if that's ok before proceeding.
We would like to hook into this permissions process to be able to call it at a different stage in the application, and if possible listen to when the user has confirmed their choice, and then react in the app accordingly.
So far, I can use this code to force my app to check the permissions for installing from unknown sources, and then send you to the correct settings. Note this code is far from cleaned up, but you get the general idea:
var unityClass = new AndroidJavaClass(""com.unity3d.player.UnityPlayer"");
var currentActivity = unityClass.GetStatic<AndroidJavaObject>(""currentActivity"");
var packageManager = currentActivity.Call<AndroidJavaObject>(""getPackageManager"");
bool launchIntent = packageManager.Call<bool>(""canRequestPackageInstalls"");

if (!launchIntent)
{
    try
    {
        using AndroidJavaClass unityClass = new AndroidJavaClass(""com.unity3d.player.UnityPlayer"");
        using AndroidJavaObject currentActivity = unityClass.GetStatic<AndroidJavaObject>(""currentActivity"");
        string packageName = currentActivity.Call<string>(""getPackageName"");
        using AndroidJavaClass uriClass = new AndroidJavaClass(""android.net.Uri"");
        using AndroidJavaObject uriObject =
                uriClass.CallStatic<AndroidJavaObject>(""fromParts"", ""package"", packageName, null);
        using AndroidJavaObject intentObj = new AndroidJavaObject(""android.content.Intent"",
                ""android.settings.MANAGE_UNKNOWN_APP_SOURCES"", uriObject);
            
        currentActivity.Call(""startActivity"", intentObj);
    }
    catch (Exception ex)
    {
        Debug.LogException(ex);
    }
}

What I would like to do is

Install our VR app
on First run it will check these permissions
if they are not granted, inform the user, then navigate the user to the correct settings window to 'enable' it.
In our VR app, listen for this change and act accordingly.
OR
What intent flags should I have to ensure the back to application button/back arrow is shown in the settings window? On a different device the same code shows an arrow box at the bottom and I can click it and it takes me back to the application and continues from where I left off. How do I make sure that shows for every device?

EDIT 4/11/22: If I can ensure that button comes up then I can just check back in the app again if the settings have been enabled and everything is good.
","
For now, the code I posted runs differently on 2 diff VR headsets. One will send us back to the app, the other does not have a back button. For now we have a solution for both issues.
"
"
I am accessing VR HMD using XR-Plugin management system on Unity 2022 and using Tracked Pose Driver to update camera rotation with HMD rotation. I don't want to update camera position with respect to HMD so Tracking Type is set to Rotation Only mode. However I need to get HMD local position while doing that (so that camera view is not updated with the position) in order to do some stuff if there is too much head movement, acceleration etc.
How do I get correct HMD local position without updating the camera view?
note: When I try xrCamera.transform.localPosition while tracking type is Rotation Only, I keep getting (0,0,0) as a result, which is of course useless.
","
In a custom script, you can add an InputActionProperty, and process its value in Update. If it's not enabled somewhere else, you need to do it yourself. Something like that:
[SerializeField] private InputActionProperty _hmdPosition;

private void Start()
{
    _hmdPosition.action.Enable();
}

private void Update()
{
    Vector3 pos = _hmdPosition.action.ReadValue<Vector3>();
    // do stuff
}

In the inspector you need to set it by reference or set directly in the InputActionProperty (ActionType: Value, ControlType: Vector3 and bind centerEyePosition [XR HMD]).
"
"
I have a problem with my VR project in Unity, where the Raycast and the tooltips from my UI Canvas appear blurry and shaky, and the overall project looks more blurry than in the Oculus Menu outside my application for example.
Does anyone know how to solve this?
Here is a picture that show my problem
","
This looks like a result of TimeWarp, a mechanism oculus is using to interpolate intermediate frames to achieve target frame rate in cases when the application is not pushing out enough frames per second
tl;dr this is a performance problem
"
"
absolute VR dev newbie here.
As stated in the title, how can I ""build and run"" VR application developed in Unity on HP Omnicept Reverb G2 headset? In other words, how can I turn my Unity project into an .exe or apk (not sure what is the correct file format)? Then how can I load this .exe file into my HP headset and run it?
I have experience developing VR applications for Quest2, and I remembered building my Unity project into an apk, and then load it into Quest2 through Sidequest. However, I have no idea how I may do the same thing but with HP headset...Please help!!! Thanks
","
In Unity, you can choose to build for Universal Windows Platform, please refer to Set up a new OpenXR project with MRTK - Mixed Reality | Microsoft Learn. Then refer Using Visual Studio to deploy and debug - Mixed Reality | Microsoft Learn to deploy the application to the PC via Visual Studio, and run the application via Windows Mixed Reality on the PC. This document may be useful to you - Unity development for VR - Mixed Reality | Microsoft Learn.
"
"
I am trying to tag two PUN instantiated game objects with ""Player1"" and ""Player2"" tags by looking at their PhotonView ViewIDs through an RPC call. I am able to successfully tag the player 1 game object with the player 1 tag, however, no matter what I try, I am unable to set the player2 tag to the player2 object. The code is networked and running on two Oculus Quest headsets. I can start the application on one Quest and it will assign the Player1 tag properly. However, when I start the application on the second Quest, it spawns a player gameobject, but does not tag the object with the Player2 tag even though the player 2 object's PhotonView matches the ""2001"" value. Below is the code that I am using to spawn in an XROrigin and a networked representation for each player.
using System;
using UnityEngine;
using Photon.Pun;
using Photon.Realtime;
using UnityEngine.XR.Interaction.Toolkit;


public class NetworkPlayerSpawner : MonoBehaviourPunCallbacks
{
    public GameObject XROriginPrefab;
    
    [HideInInspector]
    public GameObject spawnedPlayerPrefab;

    private PhotonView pv;
   
    private void Start()
    {
        pv = GetComponent<PhotonView>();
    }

    private void Update()
    {
       // Debug.Log(PhotonNetwork.CurrentRoom.PlayerCount);
    }

    public override void OnJoinedRoom()
    {
        base.OnJoinedRoom();
        var playerCount = PhotonNetwork.CurrentRoom.PlayerCount;
        Debug.Log(""The player count is: "" + playerCount);
        var teleportAreas = GameObject.FindGameObjectsWithTag(""Floor"");


        //playerCount = 2;
        if (playerCount == 1)
        {
            XROriginPrefab = Instantiate(XROriginPrefab, new Vector3(0, 2.36199999f, 3.78999996f),
                new Quaternion(0, 0, 0, 1));
            
            spawnedPlayerPrefab = PhotonNetwork.Instantiate(""Network Player"", transform.position, transform.rotation);
            //spawnedPlayerPrefab.tag = ""Player1"";
            foreach (GameObject go in teleportAreas)
            {
                go.AddComponent<TeleportationArea>();
            }
        }

        else
        {
           XROriginPrefab = Instantiate(XROriginPrefab, new Vector3(-10.3859997f,1.60699999f,10.6400003f),
                new Quaternion(0,0,0,1));
           
            spawnedPlayerPrefab = PhotonNetwork.Instantiate(""Network Player"", transform.position, transform.rotation);
            //spawnedPlayerPrefab.tag = ""Player2"";
            
            
            //If teleport breaks again, I uncommented this line, so it should be commented out again. Should allow for teleport in User 2's room.
            foreach (GameObject go in teleportAreas)
            {
                go.AddComponent<TeleportationArea>();
            }
        }
        rpcCallTagAssign();
    }

    public override void OnPlayerEnteredRoom(Player newPlayer)
    {
       base.OnPlayerEnteredRoom(newPlayer); 
       Debug.Log(""Remote Player Joined!"");
       rpcCallTagAssign();
    }


    public override void OnLeftRoom()
    {
        base.OnLeftRoom();
        PhotonNetwork.Destroy(spawnedPlayerPrefab);
    }

    [PunRPC]
    private void tagAssign()
    {
        if (spawnedPlayerPrefab.GetComponent<PhotonView>().ViewID==1001)
        {
            spawnedPlayerPrefab.tag = ""Player1"";
        }
        if (spawnedPlayerPrefab.GetComponent<PhotonView>().ViewID==2001)
        {
            spawnedPlayerPrefab.tag = ""Player2"";
        }
    }

    private void rpcCallTagAssign()
    {
        pv.RPC(""tagAssign"", RpcTarget.AllViaServer);
    }
}

I am new to networking with Photon, so any help with resolving this issue would be greatly appreciated. Thank you!
","
The code needs to run on each player (including the copies).  The current code can only change the object you have a reference for (spawnedPlayerPrefab).  The easiest way is to add an RPC function on the player.  That RPC would get called for each instance of that player across the connected clients.
Script On Player.
[PunRPC]
private void AssignTag()
{
    if (photonView.ViewID == 1001)
    {
        gameObject.tag = ""Player1"";
    }
    else if (photonView.ViewID == 2001)
    {
        gameObject.tag = ""Player2"";
    }
}

In NetworkPlayerSpawner
spawnedPlayerPrefab = PhotonNetwork.Instantiate(...);
spawnedPlayerPrefab.GetComponent<PhotonView>().RPC(""AssignTag"", RpcTarget.AllBufferedViaServer);

The RPC is buffered so future clients entering after you will set your player copy to the correct tag as well (or vice versa).
"
"
I was creating a virtual room with aframe and to implement the mirror I used  Diarmid's code: https://cdn.jsdelivr.net/gh/diarmidmackenzie/aframe-multi-camera@latest/src/mirror.min.js and also tried this user's code too https://github.com/juunini/aframe-mirror/blob/main/aframe-mirror.js. They worked normally but when I put renderer=""colorManagement: true"" the reflection of my room seen in the mirror had different contrast. I will leave the link to the glitch site below.
https://glitch.com/edit/#!/fast-able-paneer?path=index.html%3A22%3A14
","
I reached out to Diarmid and he resolved the bug. The reason behind the bug was the latest update in the THREE.js library. Here is the link to the branch with the updates which are to be merged to the main branch as of now: https://github.com/diarmidmackenzie/aframe-multi-camera/blob/updated-three-reflector/src/mirror.js
link to the main branch: https://github.com/diarmidmackenzie/aframe-multi-camera/blob/main/src/mirror.js
"
"
currently it looks to me like Chrome 83, does not support any VR experience although HTC Vive is connected (SteamVR 1.9.16 installed). SteamVR, when started manually, also finds the headset, controllers and base station.
https://threejs.org/examples/?q=webxr#webxr_vr_ballshooter
Any VR Example states ""VR not supported""
digging arround a bit, the navigator describes the ""immersive-vr"" session not to be available. Promise response of code below is: false
navigator.xr.isSessionSupported('immersive-vr').then((issupported) => {console.log(issupported)})

The previous flags of Chrome (83.0.4103.61) linked to WebXR are not available any more.
Any ideas what I am doing wrong?
EDIT:
navigator.xr.requestSession( 'immersive-vr', { optionalFeatures: [ 'local-floor', 'bounded-floor' ] } ).then( (session) => console.log(session) );

Gives:
Uncaught (in promise) DOMException: The specified session configuration is not supported.
","
Issue will be solved by SteamVR supporting OpenVR as they are going to...
https://steamcommunity.com/games/250820/announcements/detail/2396425843528787270
Until then I would recommend to use:
""C:\Program Files (x86)\Google\Chrome\Application\chrome.exe"" --enable-features=openvr --disable-features=XRSandbox --force-webxr-runtime=openvr --enable-experimental-web-platform-features https://threejs.org/

"
"
I've looked everywhere for this but I'm coming up blank. How do you replicate what Chris Lattner was demonstrating with Playgrounds and SceneKit at WWDC? I want to have a SceneKit scene, animating, in Playgrounds.
I tried cutting and pasting the setup code from the SceneKit project template, thinking it would magically start rendering, but it does not.
I tried watching the keynote and pausing and zooming on on Lattner's screen looking for hints at the source code, but he appeared to be importing all his code from elsewhere in his project, so it gave me no clues. There does not seem to be anything in the documentation, or I'm missing it.
","
Since Swift doesn't have source compatibility between versions, the code in this answer might not work in either future or previous versions of Swift. Currently is has been updated to work in Xcode 7.0 Playgrounds with Swift 2.0.

The XCPlayground framework is what you need, and it is documented here.
Here is a very simple scene to get you started with Scene Kit in Swift:
import SceneKit
import QuartzCore   // for the basic animation
import XCPlayground // for the live preview
import PlaygroundSupport

// create a scene view with an empty scene
var sceneView = SCNView(frame: CGRect(x: 0, y: 0, width: 300, height: 300))
var scene = SCNScene()
sceneView.scene = scene

// start a live preview of that view
PlaygroundPage.current.liveView = sceneView

// default lighting
sceneView.autoenablesDefaultLighting = true

// a camera
var cameraNode = SCNNode()
cameraNode.camera = SCNCamera()
cameraNode.position = SCNVector3(x: 0, y: 0, z: 3)
scene.rootNode.addChildNode(cameraNode)

// a geometry object
var torus = SCNTorus(ringRadius: 1, pipeRadius: 0.35)
var torusNode = SCNNode(geometry: torus)
scene.rootNode.addChildNode(torusNode)

// configure the geometry object
torus.firstMaterial?.diffuse.contents  = NSColor.red   // (or UIColor on iOS)
torus.firstMaterial?.specular.contents = NSColor.white // (or UIColor on iOS)

// set a rotation axis (no angle) to be able to
// use a nicer keypath below and avoid needing
// to wrap it in an NSValue
torusNode.rotation = SCNVector4(x: 1.0, y: 1.0, z: 0.0, w: 0.0)

// animate the rotation of the torus
var spin = CABasicAnimation(keyPath: ""rotation.w"") // only animate the angle
spin.toValue = 2.0*Double.pi
spin.duration = 3
spin.repeatCount = HUGE // for infinity
torusNode.addAnimation(spin, forKey: ""spin around"")

When I run it, it looks like this:


Note that to run Scene Kit in an iOS playground, you need to check the ""Run in Full Simulator"" checkbox.

You find the Playground Setting in the Utilities Pane (⌥⌘0 to hide or show)
"
"
I'm trying to render a frame, with realistic depth of field effect. I've already tried the depth of field properties in the camera node, but it doesn't produce usable results. 
Is there a switch to max-out rendering quality of the depth of field effect? Performance is not a factor, I just need to render a frame, and user can wait for it.
","
SceneKit isn't able to do (out of the box) heavy, high quality post processing or still image rendering computation of this type. Theoretically you could probably build a setup that uses its rendering approaches to do both. But it's not a high quality renderer. If the user can wait, and you really want to focus on quality of imagery, Unreal Engine has the capacity to do this sort of thing, built in, and far higher quality post processing, effects, lights, materials, particles and rendering.
"
"
So me and some people are developing a VR game and we need to have it when the VR hand Presses the button Some objects are hidden so when the trigger on the collider is entered
So I have the function
public void update()
{
//Called once per frame
}
Private void Ontriggerenter(collider, other)
{
    if ( other.tag == ""Hand"")
{
    Copper.SetActive(false);
}

Where copper is a public  game object
I still have the start and update functions not sure if that's the problem but the button clicks but the function is never triggered dose someone maybe know the problem
","
the ""T"" and ""E"" of OnTriggerEnter should be capitalized and OnTriggerEnter takes a variableof type Collider. I am not sure if you have configured the colliders correctly. Here is how the code should look like
Private void OnTriggerEnter(collider other)
{
    if ( other.tag == ""Hand"")
    {
       Copper.SetActive(false);
    }
}

You can check out this article on Unity Collision basics to get some basic idea.
"
"
Is it possible to control regular html objects outside of a-scene using A-Frame entities? For example, I would like to toggle a modal object when selecting a plane, sphere, etc. within an embedded scene. I know about the UI Modal that can be displayed within the scene, but the ability to operate between the scene and ""exterior"" elements would be very powerful. I'm sure this is possible, but I do not have the VR developer skills yet to figure this one out! Thanks in advance for your help!
","
On desktop, this is a really cool idea. I just saw Ueno use this type of interaction technique on https://interview.ueno.co.
As Diego and Steve pointed out, it's not too difficult to interact with HTML from A-Frame.
I've created a small example to demonstrate:
https://glitch.com/edit/#!/a-frame-to-html-modal
For the component:
<script>
AFRAME.registerComponent('a-frame-to-html', {
  init: function () {
    let box = document.querySelector('#box')
    let modal = document.querySelector('.modal')

    box.addEventListener => {
      modal.classList.add
    })
  }
});
</script>

Then the markup:
<body>
  <div class=""modal"">
    <!-- Modal content can go here ... -->
  </div>
  <a-scene a-frame-to-html>
    <a-entity camera=""userHeight: 1.6"" look-controls cursor=""rayOrigin: mouse""></a-entity>
    <a-box id=""box"" position=""-1 0.5 -3"" rotation=""0 45 0"" color=""#4CC3D9"" shadow></a-box>
    <a-sphere position=""0 1.25 -5"" radius=""1.25"" color=""#EF2D5E"" shadow></a-sphere>
    <a-cylinder position=""1 0.75 -3"" radius=""0.5"" height=""1.5"" color=""#FFC65D"" shadow></a-cylinder>
    <a-plane position=""0 0 -4"" rotation=""-90 0 0"" width=""4"" height=""4"" color=""#7BC8A4"" shadow></a-plane>
    <a-sky color=""#ECECEC""></a-sky>
  </a-scene>
</body>

"
"
I am wondering how many pixels is 1 unit in Unity3D for Oculus rift. For example, how a cube of 1 by 1 by 1 units could be given its dimensions in pixels.
","
There's not a 1:1 correlation here. It depends on many factors, such as the distance to the object, the angle you're viewing the object, the field of view of your camera, and the pixel resolution of your headset.
This is sort of like asking how many feet an object should be in a movie so that it takes up 6 feet of a movie theater screen. It'll depend on the kind of lens the movie is shot with, how far away the movie camera is, how big the movie theater screen is, etc.
However, at runtime, you can get the current pixel position on the screen of a position in the 3D world using Camera.WorldToScreenPoint. You could then do this for multiple points (say, at each end) of an object of interest to determine how large it is currently appearing on the screen.
"
"
I installed Unity 2022.1.11f1 in a folder on my D drive since my C drive is nearly full (I think this may be part of the problem). I made a new project with the VR Core template. When I try to install XR Interaction toolkit I get these error messages:
    [Package Manager Window] Cannot perform upm operation: ENOENT: no such file or directory, mkdir 'D:\Stuff-thats-on-my-desktop\Random Coding Stuff\Unity\Games\Vr Newest\Library\PackageCache\.tmp-13992-4IcEoKfKjCQJ' [NotFound].
UnityEditor.EditorApplication:Internal_CallUpdateFunctions ()

    [Package Manager Window] Error adding package: com.unity.xr.interaction.toolkit.
UnityEditor.EditorApplication:Internal_CallUpdateFunctions ()

I also get similar error messages when I open the project. I've found nothing on the internet related to this and have no clue how to solve it so I really appreciate any help, thanks
","
Got it solved. Completely uninstalled unity and reinstalled it entirely on my D drive under /Unity with the subfolders Projects, Editors, and Downloads for each respective setting in Unity Hub, everything worked perfectly after that.
"
"
I'm using ExoPlayer to create an VR video
I have a 360 degree video and I did this
<com.google.android.exoplayer2.ui.PlayerView
    android:id=""@+id/video_view_player""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    app:fastforward_increment=""15000""
    app:surface_type=""spherical_gl_surface_view""
    app:rewind_increment=""15000""
    app:controller_layout_id=""@layout/content_player_controller""
    android:background=""@color/black""
    app:shutter_background_color=""@color/black""
    app:show_buffering=""always""
    app:show_timeout=""3000"" />

and in PlayerActivity I setDefaultStereoMode
player?.prepare(mediaSource)
    (player_view.videoSurfaceView as SphericalGLSurfaceView?)!!.setDefaultStereoMode(
        C.STEREO_MODE_MONO
    )

and it shows like this

what I need is something like this

","
Its not clear from your question ion you have done this also, but you do need to set the StereoMode for the player too:

public static @interface C.StereoMode


The stereo mode for 360/3D/VR videos. One of Format.NO_VALUE, C.STEREO_MODE_MONO, C.STEREO_MODE_TOP_BOTTOM, C.STEREO_MODE_LEFT_RIGHT or C.STEREO_MODE_STEREO_MESH.

(https://exoplayer.dev/doc/reference/com/google/android/exoplayer2/C.StereoMode.html)
You would have something like this in your code:
player.prepare(videoSource)
        (playerView.videoSurfaceView as SphericalGLSurfaceView?)!!.setDefaultStereoMode(
            C.STEREO_MODE_LEFT_RIGHT
        )

"
"
I'm not looking for a spoon fed solution but perhaps some insight because I have tried many different scenarios in attempt to get good performance.
I have a scenario where I would like to apply damage over time to X amount of enemies (Zombies), from a collider (Projectile).
Current Problematic Setup
Currently, I have a script attached to my Projectile which will StartCoroutine onTriggerEnter that will decrement the Zombies life over time. This works wonderfully, until you want to damage more than 15 or so Zombies at time.
I presume this is from some of the magic going on in all of the StartCoroutines. Here is my couroutine thats called on OnTriggerEnter.
IEnumerator DoDOT(float damageDuration, int damageCount, float damageAmount)
    {
        damagingOverTime = true;
        int currentCount = 0;
        while (currentCount < damageCount)
        {
            zombieHealth -= damageAmount;
            yield return new WaitForSeconds(damageDuration);
            currentCount++;
        }
        damagingOverTime = false;
    }

Possible Solution
Now I've done a simple test (with double the amount of zombies) where I've hard coded the Damage Over Time mechanic onto the Zombies Update function. Of course when I start the scene, every single Zombie begins to take damage over time, but there are no performance issues.
I'm thinking I could have the update in my Zombie game object ""watching"" for a Damage Over Time flag, then my Projectile can activate that flag OnTriggerEnter.  The Zombie Update will than begin to apply Damage over time. Could those with more experience let me know if this is the correct approach, and if not, what is?
Performance goal 90 FPS not 60FPS. My current process of starting co-routines for the DOT would actually work fine in theory in a 60FPS environment because you have a lot more room for processing before it could cause a frame drop. But when you aim for 90FPS that same coroutine can not handle it because we no longer have that extra processing room. I'm trying to figure out why my Possible Solution provides more efficient results in a 90FPS environment, or what are possible solutions for more efficient results in a 90FPS environment.
PS - It's not really another zombie shooter, those terms are just examples :D
","
Some debugging is needed here.
You can do something like count the numbers of times this function is called just to make sure it's not called too many times.  
Another thing you can try to do is move this to the Update method of the projectile itself and keep a list of all enemies that were hit by this projectile (simply add them to the list in   OnTriggerEnter).
So instead of having many Coroutines running in the background everything is handled on a simple loop that loops through all the enemies that were hit by it.
P.s writing this on mobile sorry for any mistakes
"
"
This is the blue screen
I'm trying to load 3d models into my scene, and even though other models are loading fine, there seems to be a problem with adding any new ones.
The console shows this error
This is the code that I'm working with:
<html>
  <head>
    <script src=""https://aframe.io/releases/1.3.0/aframe.min.js""></script>
    <script src=""https://unpkg.com/[email protected]/dist/aframe-environment-component.min.js""></script>
  </head>
  <body>
    <a-scene>
      <a-assets>
        <audio id=""background"" src=""asset/audio/background.mp3"" preload=""auto""></audio>
        <a-asset-item
          id=""tree""
          src=""https://vazxmixjsiawhamofees.supabase.co/storage/v1/object/public/models/tree-lime/model.gltf""
        ></a-asset-item>
        <a-asset-item
          id=""tree1""
          src=""https://vazxmixjsiawhamofees.supabase.co/storage/v1/object/public/models/tree-lime/model.gltf""
        ></a-asset-item>
        <a-asset-item
          id=""tree2""
          src=""https://vazxmixjsiawhamofees.supabase.co/storage/v1/object/public/models/tree-lime/model.gltf""
        ></a-asset-item >
        <a-asset-item id=""tree3""
        src=""https://vazxmixjsiawhamofees.supabase.co/storage/v1/object/public/models/tree-beech/model.gltf"">
        <a-asset-item>
        <a-asset-item id=""avatar""
        src=""./asset/avatars/96df486e-fbad-4c3d-8aa4-57f6cf51769f.glb"">
        <a-asset-item>

        </a-asset-item>
        <a-asset-item id=""building1"" src=""./asset/building1/scene.gltf""></a-asset-item>
        <a-asset-item id=""building3"" src=""./asset/building3/scene.gltf""></a-asset-item>
        <a-asset-item id=""scifi"" src=""./asset/scifi/scene.gltf""></a-asset-item>
        
      </a-assets>
      <a-entity environment=""preset:tron ""></a-entity>
      <a-entity position=""3.5 0 -25"" gltf-model=""#tree""></a-entity>
      <a-entity position=""-25 0 -25"" gltf-model=""#tree1""></a-entity>
      <a-entity position=""-25 0 -10"" gltf-model=""#tree2""></a-entity>
      <a-entity position=""19 0.6 -11"" gltf-model=""#building1""></a-entity>
        <a-entity position=""-15 0 -10"" gltf-model=""#building3""></a-entity>
        <a-entity position=""-5 0 -10"" gltf-model=""#scifi""></a-entity>
        <a-entity position=""20 0 11"" gltf-model=""#tree3""></a-entity>
      <a-entity position=""10 20 -50"" gltf-model=""#avatar""></a-entity>
      
    </a-scene>
  </body>
</html>

Here, the assets marked with id - tree3 and avatar are the only ones not rendering, and the page gets stuck on the blue screen. I tried commenting these out and adding other models, but they do not seem to work as well. The page loads only when I remove these 2 models and don't add anything new. Trying other solutions, I tried changing directories & using links instead of local references, but nothing seems to work.
Also, the audio randomly plays some times and the other times it doesn't. I'm unable to understand this error as well. I tried replacing the :
<audio id=""background"" src=""asset/audio/background.mp3"" preload=""auto""></audio>
with
<a-sound src=“url(asset/audio/background.mp3)”><a-sound> as suggested in another solution, but this doesn't work as well.
Any help would be really appreciated!
","
The assets (tree3 and avatar) not loading, I believe are caused by the 2 small errors in the a-asset-item open/close logic. I marked your original code and added some notes to explain further below, but also included the adjusted code that you should be able to use as well.
The other issue regarding your audio being intermittent. I believe has to do with using an mp3 file which are usually a bit large and not super ideal for the web that can hiccup on loading. We can try to optimize that mp3 to keep the file size small and/or convert it to a web optimized format like .ogg to help. Another important thing to note for audio in a-frame is that it is spatial, so if you move away from it then it can get quieter. If you want audio to act more like normal background music then you can just attach that audio entity to the user/player. More info on a-frame sound https://aframe.io/docs/1.3.0/components/sound.html
Original Code w/ notes :
<html>
  <head>
    <script src=""https://aframe.io/releases/1.3.0/aframe.min.js""></script>
    <script src=""https://unpkg.com/[email protected]/dist/aframe-environment-component.min.js""></script>
  </head>
  <body>
    <a-scene>
      <a-assets>
        <audio id=""background"" src=""asset/audio/background.mp3"" preload=""auto""></audio><!-- try optimizing the mp3 to a smaller file size and/or a web optimized format lik .ogg as well as let's tweak the src to ./asset just in case too. -->
        <a-asset-item
          id=""tree""
          src=""https://vazxmixjsiawhamofees.supabase.co/storage/v1/object/public/models/tree-lime/model.gltf""
        ></a-asset-item>
        <a-asset-item
          id=""tree1""
          src=""https://vazxmixjsiawhamofees.supabase.co/storage/v1/object/public/models/tree-lime/model.gltf""
        ></a-asset-item>
        <a-asset-item
          id=""tree2""
          src=""https://vazxmixjsiawhamofees.supabase.co/storage/v1/object/public/models/tree-lime/model.gltf""
        ></a-asset-item >
        <a-asset-item id=""tree3""
        src=""https://vazxmixjsiawhamofees.supabase.co/storage/v1/object/public/models/tree-beech/model.gltf"">
        <a-asset-item><!-- should be an </a-asset-item> close here instead -->
        <a-asset-item id=""avatar""
        src=""./asset/avatars/96df486e-fbad-4c3d-8aa4-57f6cf51769f.glb"">
        <a-asset-item><!-- this should be removed to allow the following  </a-asset-item> to close out this asset item -->

        </a-asset-item>
        <a-asset-item id=""building1"" src=""./asset/building1/scene.gltf""></a-asset-item>
        <a-asset-item id=""building3"" src=""./asset/building3/scene.gltf""></a-asset-item>
        <a-asset-item id=""scifi"" src=""./asset/scifi/scene.gltf""></a-asset-item>

      </a-assets>
      <a-entity environment=""preset:tron ""></a-entity>
      <a-entity position=""3.5 0 -25"" gltf-model=""#tree""></a-entity>
      <a-entity position=""-25 0 -25"" gltf-model=""#tree1""></a-entity>
      <a-entity position=""-25 0 -10"" gltf-model=""#tree2""></a-entity>
      <a-entity position=""19 0.6 -11"" gltf-model=""#building1""></a-entity>
        <a-entity position=""-15 0 -10"" gltf-model=""#building3""></a-entity>
        <a-entity position=""-5 0 -10"" gltf-model=""#scifi""></a-entity>
        <a-entity position=""20 0 11"" gltf-model=""#tree3""></a-entity>
      <a-entity position=""10 20 -50"" gltf-model=""#avatar""></a-entity>

    </a-scene>
  </body>
</html>

Fixed Code Example :
<html>
<head>
    <script src=""https://aframe.io/releases/1.3.0/aframe.min.js""></script>
    <script src=""https://unpkg.com/[email protected]/dist/aframe-environment-component.min.js""></script>
</head>
<body>
<a-scene>

<a-assets>
    <audio id=""background"" src=""./asset/audio/background.mp3"" preload=""auto""></audio>

    <a-asset-item
    id=""tree"" src=""https://vazxmixjsiawhamofees.supabase.co/storage/v1/object/public/models/tree-lime/model.gltf""></a-asset-item>

    <a-asset-item id=""tree1"" src=""https://vazxmixjsiawhamofees.supabase.co/storage/v1/object/public/models/tree-lime/model.gltf""></a-asset-item>

    <a-asset-item
    id=""tree2"" src=""https://vazxmixjsiawhamofees.supabase.co/storage/v1/object/public/models/tree-lime/model.gltf""></a-asset-item>

    <a-asset-item id=""tree3""
    src=""https://vazxmixjsiawhamofees.supabase.co/storage/v1/object/public/models/tree-beech/model.gltf""></a-asset-item>

    <a-asset-item id=""avatar""
    src=""./asset/avatars/96df486e-fbad-4c3d-8aa4-57f6cf51769f.glb""></a-asset-item>

    <a-asset-item id=""building1"" src=""./asset/building1/scene.gltf""></a-asset-item>

    <a-asset-item id=""building3"" src=""./asset/building3/scene.gltf""></a-asset-item>

    <a-asset-item id=""scifi"" src=""./asset/scifi/scene.gltf""></a-asset-item>
</a-assets>

<a-entity environment=""preset:tron""></a-entity>
<a-entity position=""3.5 0 -25"" gltf-model=""#tree""></a-entity>
<a-entity position=""-25 0 -25"" gltf-model=""#tree1""></a-entity>
<a-entity position=""-25 0 -10"" gltf-model=""#tree2""></a-entity>
<a-entity position=""19 0.6 -11"" gltf-model=""#building1""></a-entity>
<a-entity position=""-15 0 -10"" gltf-model=""#building3""></a-entity>
<a-entity position=""-5 0 -10"" gltf-model=""#scifi""></a-entity>
<a-entity position=""20 0 11"" gltf-model=""#tree3""></a-entity>
<a-entity position=""10 20 -50"" gltf-model=""#avatar""></a-entity>

</a-scene>
</body>
</html>

"
"
Started using unity 3 days ago, trying to make a VR game for fun.
At this point in my project, I have a kunai that you can throw, I want it to do damage to a cube I created as a my target. I want the cube to be destroyed once the Kunai hits it.
I followed a tutorial and have these two scripts.
Kunai Script:
{
private void OnCollisionEnter(Collision collision)
{
    if(collision.gameObject.TryGetComponet<Enemy>(out Enemy enemyComponet))
    {
        enemyComponet.TakeDamage(3);
    }
    Destroy(gameObject);
}

}
Box/Enemy Script:
{
[SerializeField] float health, maxHealth = 3f;

private void Start()
{
    health = maxHealth;

}

public void TakeDamage(float damageAmount)
{
    health -= damageAmount;

    if(health <= 0)
    {
        Destroy(gameObject);
    }
}

}
I am getting a CS1061 Error, Kinda confused on what its trying to say to me though.
","
TryGetComponent is mispelled in the Kunai script (the second ""n"" is missing). Also, I guess ""EnemyComponet"" is mispelled too ;)
"
"
I have seen some examples demo of ARKit where material A is blocking material B, kind of creating occlusion effect, or black hole, or masking. But all of them seems to be using Unity engine (I might be wrong). I wanted to do this using what Apple provide already. Or maybe using Metal Shader.
Wondering if anyone knows a trick that allows this for ARKit or even in SceneKit term, when 2 objects are overlapping, but wanting one object to occlude the other object, leaving the other object empty. It cannot be CSG right, because that would be too expensive?
","
To generate a hold-out mask, also known as Occlusion Material, use.colorBufferWriteMask, instance property that writes depth data when rendering the material.
sphere.geometry?.firstMaterial?.colorBufferWriteMask = []

Then assign an appropriate object's rendering order:
sphere.renderingOrder = -100  

And, at last, allow SceneKit to read from / write to depth buffer when rendering the material:
sphere.geometry?.firstMaterial?.writesToDepthBuffer = true
sphere.geometry?.firstMaterial?.readsFromDepthBuffer = true

sphere.geometry?.firstMaterial?.isDoubleSided = true


"
"
Is it possible for a webpage to listen for button presses of a Oculus controller simply using some generic javascript code like document.addEventListener?
In VR browsers by default the primary thumbstick scrolls the page up and down. The idea is to re-map it to trigger different actions.
From my research it looks like I need to use an A-frame but I'm looking for a generic solution that works across different websites with just Vanilla Javascript, not inside an immersive context.


","
In short, no.
It's not possible to interface with an Oculus controller outside of an immersive WebXR context.
The Inputs and Sources section of the WebXR API states:

While the Gamepad record is defined by the Gamepad API specification, it's not actually managed by the Gamepad API

So the Gamepad API will not work, and we're forced to use the WebXR API. Futhermore, we are forced to use one of the immersive modes for our webxr session. The WebXR Controller Sample states (confirmed this, see comments in rules below):

WebXR gamepads are not available during inline sessions.

That being said, you can still ...
Do it with minimal (vanilla) Javascript.
Depending on your definition of vanilla, this will work (Codesandbox):
const gl2 = document
  .querySelector('canvas.c3d')
  .getContext('webgl2');
const session = await navigator.xr.requestSession('immersive-vr')
session.updateRenderState({ baseLayer: new XRWebGLLayer(session, gl2) });

const hookGamepadControls = () => {
  const gamepad = session?.inputSources?.[0]?.gamepad; //replace with code to choose appropriate input source/controller
  if (gamepad) {
    setInterval(scroll(gamepad), 30);
  }
};
session.requestAnimationFrame(hookGamepadControls);  //We can request a single frame and run a timer to poll controller, no need to request additional frames

Minimum rules to follow to get Oculus or any VR/AR/XR controllers/gamepads to show up:

Main rules:

We must not be in an inline context, only immersive-vr or immersive-ar (otherwise source.gamepad will be null)
We must be in a frame context (otherwise source.gamepad will be null)


Collary rules:

We must connect the render state to the GL context (otherwise we won't receive animation frames)
We must have a canvas element (otherwise we won't be able to get a GL context)



"
"
Prior to today, everything was working fine and I was able to deploy from Unity to specifically an APK. I wasn't able to operate in ""Play"" mode so I tried updating the Oculus Platform SDK, which proceeded to hang in ""Installing..."" forever, leading to my current predicament.
What happens:

I hit ""download"" on anything inside the Downloads section of the Oculus Developer Hub

It hangs saying ""Installing..."" for hours and does nothing.


Also, when I try to change my ADB Path in Oculus Developer Hub, it ""fails"" and asks me to try again, but the new value is present every time.
I've tried installing/uninstalling both ODH and Oculus software. I've tried changing the ADB path. I've tried installing and adding JRE and NDK to the path. I'm running out of options here. Does anyone have any advice? This is blocking me from doing development work in Unity.
","
To avoid issues, ODH (Oculus Developer Hub) must reference the same adb.exe as the one referenced by your Unity Editor.
If, for instance, you are using the Unity Editor 2021.3.1f1 with the standard settings (you didn't changed the default installation path and also installed the Android Build Support module and the corresponding sub-modules), then Unity will use the adb.exe found in:
 C:\Program Files\Unity\Hub\Editor\2021.3.1f1\Editor\Data\PlaybackEngines\AndroidPlayer\SDK\platform-tools\adb.exe

ODH has a buggy behavior when trying to change the ADB path (ODH > Settings > General > ADB Path > Edit): when you select the desired adb path from the Detected ADB Clients drop down and click Restart ODH, you get the following error message:

""Unable to change ADB path and restart ODH. Please try again.""


Just ignore the error message. Dont' click Cancel, just close ODH and start it again. Then navigate again to the menu where you changed the ADB path, i.e. ODH > Settings > General > ADB Path > Edit. You should now have the path you chose before manually restarting ODH.
"
"
I have two points (let's call them pointA and pointB) of type SCNVector3. I want to draw a line between them. Seems like it should be easy, but can't find a way to do it.
I see two options, both have issues:

Use a SCNCylinder with a small radius, with length |pointA-pointB| and then position it/rotate it.

Use a custom SCNGeometry but not sure how; would have to define two triangles to form a very thin rectangle perhaps?


It seems like there should be an easier way of doing this, but I can't seem to find one.
Edit: Using the triangle method gives me this for drawing a line between (0,0,0) and (10,10,10):
CGFloat delta = 0.1;
SCNVector3 positions[] = {  SCNVector3Make(0,0,0),
    SCNVector3Make(10, 10, 10),
    SCNVector3Make(0+delta, 0+delta, 0+delta),
    SCNVector3Make(10+delta, 10+delta, 10+delta)};
int indicies[] = {
    0,2,1,
    1,2,3
};

SCNGeometrySource *vertexSource = [SCNGeometrySource geometrySourceWithVertices:positions count:4];
NSData *indexData = [NSData dataWithBytes:indicies length:sizeof(indicies)];
SCNGeometryElement *element = [SCNGeometryElement geometryElementWithData:indexData primitiveType:SCNGeometryPrimitiveTypeTriangles primitiveCount:2 bytesPerIndex:sizeof(int)];
SCNGeometry *line = [SCNGeometry geometryWithSources:@[vertexSource] elements:@[element]];

SCNNode *lineNode = [SCNNode nodeWithGeometry:line];
[root addChildNode:lineNode];

But there are problems: due to the normals, you can only see this line from one side! It's invisible from the other side. Also, if ""delta"" is too small you can't see the line at all. As it is, it's technically a rectangle, rather than the line I was going for, which might result in small graphical glitches if I want to draw multiple joined up lines.
","
There are lots of ways to do this.
As noted, your custom geometry approach has some disadvantages. You should be able to correct the problem of it being invisible from one side by giving its material the doubleSided property. You still may have issues with it being two-dimensional, though.
You could also modify your custom geometry to include more triangles, so you get a tube shape with three or more sides instead of a flat rectangle. Or just have two points in your geometry source, and use the SCNGeometryPrimitiveTypeLine geometry element type to have Scene Kit draw a line segment between them. (Though you won't get as much flexibility in rendering styles with line drawing as with shaded polygons.)
You can also use the SCNCylinder approach you mentioned (or any of the other built-in primitive shapes). Remember that geometries are defined in their own local (aka Model) coordinate space, which Scene Kit interprets relative to the coordinate space defined by a node. In other words, you can define a cylinder (or box or capsule or plane or whatever) that's 1.0 units wide in all dimensions, then use the rotation/scale/position or transform of the SCNNode containing that geometry to make it long, thin, and stretching between the two points you want. (Also note that since your line is going to be pretty thin, you can reduce the segmentCounts of whichever built-in geometry you're using, because that much detail won't be visible.)
Yet another option is the SCNShape class that lets you create an extruded 3D object from a 2D Bézier path. Working out the right transform to get a plane connecting two arbitrary points sounds like some fun math, but once you do it you could easily connect your points with any shape of line you choose.
"
"
I'm trying to create a system in VR in which an object rotation follows the rotation of the player's hand. Unfortunately, when the hand rotates at a high angle, the rotated object flips in a strange way as if it had a problem with the rotation change from 360 degrees to 0. I tried many ways to solve this problem, but each one that worked did not allow me to change ""sensitivity"" on the basis of:

1 - target is rotating 1: 1 ratio with hand
2 - target is rotating twice as much as hand

etc.
// In FixedUpdate
Quaternion deltaRotation = Quaternion.Inverse(target.rotation) * hand.rotation;
deltaRotation.ToAngleAxis(out float angle, out Vector3 axis);
target.rotation *= Quaternion.AngleAxis(angle * sensitivity, axis);

Any help would be appreciated!
","
So the problem is in
Quaternion deltaRotation = Quaternion.Inverse(target.rotation) * hand.rotation;

if the sensitivity is != 1 these objects rotations get desynchronized so the deltaRotation will not give you the value you expect. You probably wanted to check how much hand rotated since last frame, like that:
Quaternion _lastFrameRotation;

void Awake()
{
    _lastFrameRotation = transform.rotation;
}

private void FixedUpdate()
{
    Quaternion deltaRotation = Quaternion.Inverse(_lastFrameRotation) * transform.rotation;
    deltaRotation.ToAngleAxis( out float angle, out Vector3 axis );
    target.rotation *= Quaternion.AngleAxis( angle * sensitivity, axis );

    _lastFrameRotation = transform.rotation;
}

"
"
I am working on a VR speedrun game and I need a timer. The timer doesn't need to be showed in the screen for the player, just on the map I made. It needs to start when the player (VR) passes a specific point and end when it reaches a different point. If anyone has an idea of how to make this work I would really appreciate it.
","
On the start line you could have an empty gameobject with a trigger collider on it, and in the OnTriggerEnter event you could start a Coroutine that keeps track of the time, and on the finish line you'd have another trigger collider that sets a flag and stops the timer.
Something along the lines of this should work:
using UnityEngine;
using System;

public class Player : MonoBehaviour {

    private bool _isTimerStarted = false;
    private float _timeElapsed = 0;
    
    private void OnTriggerEnter(Collider other) {
        if (other.gameObject.name.Equals(""Start Line"")) {
            _isTimerStarted = true;
            StartCoroutine(StartTimer());
        } else if (other.gameObject.name.Equals(""Finish Line"") {
            _isTimerStarted = false;
        }
    }

    IEnumerator StartTimer() {
        while (_isTimerStarted) {
            _elapsedTime += Time.deltaTime;
            yield return null;
        }
        yield break;
    }
}

For this to work just make sure your player has a RigidBody attached or else no collision will be detected :)
"
"
I am building an AR App.
Here is my code
Content View:
@State var timeAccumulate = 0
let styleCount = 2
let timer = Timer.publish(every: 10, on: .main, in: .common).autoconnect()
var body: some View {
let vc = ARViewContainer(timeAccumulate: timeAccumulate)
   .edgesIgnoringSafeArea(.all)
   .onAppear(perform: { getData() })
   .onReceive(timer) { _ in
        timeAccumulate = (timeAccumulate + 1) % styleCount
   }
   return vc
}

ARViewContainer:
struct ARViewContainer: UIViewRepresentable {
    var timeAccumulate: Int
    let boxAnchor = try! Experience.loadBox()
    let documents = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first!
    
    func makeUIView(context: Context) -> ARView {
        let arView = ARView(frame: .zero)
        arView.frame = CGRect(x: 0, y: 0, width: UIScreen.main.bounds.size.width, height: UIScreen.main.bounds.size.height)
        
        arView.autoresizingMask = [.flexibleWidth, .flexibleHeight]
        
        for i in 1...(imageOrder.count+1-1) {
            let entity = boxAnchor.findEntity(named: ""box\(i)"")!
       
            let modelEntity = entity.children[0]
            modelEntity.generateCollisionShapes(recursive: true)
            arView.installGestures([.all], for: modelEntity as! Entity & HasCollision)
        }
        
        arView.scene.anchors.append(boxAnchor)
        
        return arView
    }
 
    func updateUIView(_ uiView: ARView, context: Context) {
        print(timeAccumulate)
        for i in 1...(imageOrder.count+1-1) {
            let boxThis = boxAnchor.findEntity(named: ""box\(i)"")!
            var modelEntity: ModelEntity!

            if timeAccumulate == 0 {
                modelEntity = try! ModelEntity.loadModel(named: ""Box"", in: Bundle.main)
                boxThis.children[0] = modelEntity
            } else {
                modelEntity = try! ModelEntity.loadModel(named: ""Chess1"", in: Bundle.main)
                boxThis.children[0] = modelEntity
            }
            do {
                var material = SimpleMaterial()
                
                let location = documents.appendingPathComponent(""\(i).jpg"")
                let tre = try TextureResource.load(contentsOf: location, withName: ""\(i).jpg"")
                material.baseColor = try MaterialColorParameter.texture(tre)
                
                material.roughness = .float(0.0)
                
                (modelEntity as Entity & HasModel).model?.materials = [material]
            }catch{
                //print(""no image"", count)
            }
        }
    }
}

I want 25 boxes to transform into another model every ten seconds, the box, then the chess, then the box, then the chess
I am sure that parameter timeAccumulate is working, it is 0, then 1, then 0, then 1.
But the view does not change. It's always been a box.
What am I missing?
Thank you.
","
Fix the following errors and it will do the trick:

Use $ for timeAccumulate property wrapper in ContentView struct to get a binding struct.
ARViewContainer(timeAccumulate: $timeAccumulate)


Then use @Binding attribute for timeAccumulate property in ARViewContainer struct.
@Binding var timeAccumulate: Int


baseColor is still working in iOS 15 but it'll be irrelevant on iOS 16. So, use color instead:
var material = SimpleMaterial()

material.color = .init(tint: .white,
                    texture: .init(try! .load(named: ""texture.png"")))



"
"
I've been working in a project some weeks now in a Mixed Reality project using Unity and MRTK and I've found a strange behaviour when adding screen capture to the application. I'm using Unity 2019.4.26f1, and I haven't found any issues so far but after adding this line of code
ScreenCapture.CaptureScreenshot(""test.png"");

into a script, I've realised that when I press Play and go into Game mode, everything goes correctly (the game starts, the screenshot is taken). BUT when I stop it and try to go into Play mode again, Unity freezes. If I don't press Play again, Unity still works until it tries to compile scripts or some heavy task like that, when it freezes and can only be closed with the task manager.
If I remove that line, Unity goes back to normal.
","
This issue cannot be reproduced in Unity 2020.3 LTS. Does this happen when the remoting play or in Editor? According to Choosing a Unity version and XR plugin - Mixed Reality | Microsoft Docs, if you target to HoloLens 2, please upgrade to the recommended 2020.3 LTS version that is stated in the document.
In addition, ScreenCapture is a Unity built-in class and if there are any usage questions, you may find support from Unity forum.
"
"
I’m trying to build the UE4 version from Oculus from the GitHub repository. But during the build I keep getting some errors. I’ll link the Output log right here:
https://drive.google.com/file/d/1iLFmV5mZI3r8tJjAbxVqXnlWFcI6tNbj/view?usp=sharing (38.7 KB)
Before everything comes crashing I get this:
12>C:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\winrt\wrl/event.h(211): error C4668: '_NOEXCEPT_TYPES_SUPPORTED' is not defined as a preprocessor macro, replacing with '0' for '#if/#elif'
12>C:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\winrt\wrl/event.h(211): error C4668: '__cpp_noexcept_function_type' is not defined as a preprocessor macro, replacing with '0' for '#if/#elif'
12>C:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\winrt\wrl/event.h(371): error C4668: '_NOEXCEPT_TYPES_SUPPORTED' is not defined as a preprocessor macro, replacing with '0' for '#if/#elif'
12>C:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\winrt\wrl/event.h(371): error C4668: '__cpp_noexcept_function_type' is not defined as a preprocessor macro, replacing with '0' for '#if/#elif'
12>C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Microsoft\VC\v170\Microsoft.MakeFile.Targets(44,5): error MSB3073: The command ""..\..\Build\BatchFiles\Build.bat -Target=""UE4Editor Win64 Development"" -Target=""ShaderCompileWorker Win64 Development -Quiet"" -WaitMutex -FromMsBuild"" exited with code 6.
12>Done building project ""UE4.vcxproj"" -- FAILED.

After these lines, it starts to skip a lot.
","
Two things:

Make sure you have Win10 SDK version 10.0.18362 installed as UE will try to use that
I found this post and made this change and I was able to compile:
https://chowdera.com/2022/03/202203261010082767.html

"
"
I have a question if you can answer me it will delight me. How do you enable the touch screen keyboard? Is it inside the XR Interaction Toolkit?
I have no clue or whatsoever to get that touch keyboard out and use it in a input field in Unity.
","
Usually the touchscreen keyboard opens up on its own when you click on an InputField.
However if it doesn't, you should be able to open it manually like this in C#
public void OpenKeyboard()
{
     TouchScreenKeyboard.Open("""");
}

You can learn more about the touchscreen keyboard and its properties
right here.
Hope this helped you :)
"
"
I am trying to figure out a way to take a sphere in my scene and set the color of the sphere to a variable. How can I achieve this? I would also like it to update so for example, if a button is pressed and the variable changes to a different color then the sphere's color will change too.
Here is my markup for the sphere:
<a-sphere id=""eleId"" class=""head"" scale=""0.3 0.3 0.3"" position=""0 1.5 -1""></a-sphere>

What I would like is to set the color of that sphere to a variable called color. How can I achieve this?
","
you can try something like this
<body>
    <button style=""position:fixed;z-index:999"" onclick=""colorChange()"">Color change</button>
    <a-scene>
    <a-sphere id=""eleId"" class=""head"" color=""#ff1122"" scale=""0.3 0.3 0.3"" position=""0 1.5 -1""></a-sphere>
</a-scene>
</body>
<script>
    function colorChange(){
  var letters = '0123456789ABCDEF';
  var color = '#';
  for (var i = 0; i < 6; i++) {
    color += letters[Math.floor(Math.random() * 16)];
  }
  document.querySelector(""a-sphere"").setAttribute(""color"",color)
}
</script>

once you click on the change button it randomly generates you a color and sets the color attribute with the randomly generated value
"
"
I'm trying to create something in A-frame (aframe.io) where when a button onclick event happens, a cube will change colors. Here is my current code:
 <script src=""https://aframe.io/releases/1.2.0/aframe.min.js""></script>
<button style=""z-index: 999; position: fixed"">RESIZE</button>
<script>
  AFRAME.registerComponent(""foo"", {
    init: function() {
      const btn = document.querySelector(""button"");
      const sphere = document.querySelector(""a-sphere"");
      btn.addEventListener(""click"", e => {
         sphere.setAttribute(""color"",""red"");
      })
    }
  })
</script>
<a-scene foo>
  <a-sphere position=""0 1.25 -5"" radius=""1.25"" color=""#EF2D5E""></a-sphere>
</a-scene>

The code here works perfectly but its not using an onclick event. How can I change the code above so when there is an onclick function on the button and when that function occurs the color changes? Just clarifying, I know the code above is working perfectly but it's using an event listener to tell when the button is being clicked, I need to change that to an onclick event.
","
If you wish to use normal JS (onclick) instead of all register components then you can try something like this
<script src=""https://aframe.io/releases/1.2.0/aframe.min.js""></script>
    <button style=""z-index: 999; position: fixed"" onclick=""s()"">RESIZE</button>
    <a-scene foo>
        <a-sphere position=""0 1.25 -5"" radius=""1.25"" color=""#EF2D5E""></a-sphere>
    </a-scene>
<script>
    function s(){
        const sphere = document.querySelector(""a-sphere"");
        sphere.setAttribute(""color"",""red"");
    }
</script> 

you can call a function s and that function will set attribute for color
"
"
I want to make a mix of virtual reality and augmented reality.
The goal is I have a stereo camera (for each eyes).
I tried to put two ARSCNView in a viewCotnroller but it seems ARKit enable only one ARWorldTrackingSessionConfiguration at the same time. How can I do that? 
I researched to copy the graphic representation of a view to past this to an other view but impossible to find. Please help me to find the solution.
I found this link, maybe can it illumine us:
ARKit with multiple users
Here's a sample of my issue: 
https://www.youtube.com/watch?v=d6LOqNnYm5s
PS: before unlike my post, comment why! 
","
The following code is basically what Hal said. I previously wrote a few lines on github that might be able to help you get started. (Simple code, no barrel distortion, no adjustment for the narrow FOV - yet).
Essentially, we connect the same scene to the second ARSCNView (so both ARSCNViews are seeing the same scene). No need to get ARWorldTrackingSessionConfiguration working with 2 ARSCNViews. Then, we offset its pointOfView so it's positioned as the 2nd eye.
https://github.com/hanleyweng/iOS-Stereoscopic-ARKit-Template
"
"
We were able to have custom raycasting using bitmasks:
let hitTest = sceneView.hitTest(location, options: [categoryBitMask: bitmask])

But hitTest is deprecated now and I can't figure out how to set bitmask for raycast query:
let query = sceneView.raycastQuery(from: location, allowing: .estimatedPlane, alignment: .horizontal)

","
SceneKit
In SceneKit you can use bitmasks in context of  [SCNHitTestResult]. hitTest(_:options:) instance method is not deprecated yet and it works in iOS 15.4.
let sceneView = ARSCNView(frame: .zero)

enum HitTestType: Int {
    case object_A = 0b00000001
    case object_B = 0b00000010
}

let point: CGPoint = gesture.location(in: self.sceneView)

let bitMask = HitTestType.object_A.rawValue | HitTestType.object_B.rawValue

let results = sceneView.hitTest(point, options: [.categoryBitMask: bitMask])

P.S.
Only hitTest(_:types:) is deprecated at the moment.

RealityKit
In RealityKit you can use bitmasks in CollisionCastHit's context:
let arView = ARView(frame: .zero)

let point: CGPoint = gesture.location(in: self.arView)
    
let (origin, direction) = arView.ray(through: point)!
    
let raycasts: [CollisionCastHit] = arView.scene.raycast(origin: origin, 
                                                     direction: direction, 
                                                        length: 50, 
                                                         query: .any, 
                                                          mask: .default, 
                                                    relativeTo: nil)

...or this way:
let raycasts: [CollisionCastHit]  = arView.hitTest(point, 
                                                   query: .any, 
                                                    mask: .default)

"
"
I was wondering about how much .scn data I should have in my SCNKit application? Are there any limitations except the general iOS app size limitations (found in this post)?
I'd expect that it should be a reasonable size compared to the device RAM. So to be safe for the app not crashing on a specific device it should avoid getting to close to the limits mentioned in this post e.g if the RAM is 1GB I should just stay away from anything around 400MB to be safe?
Further what size of .scn file can I have loaded at one time? So e.g. if there are 6 scenes of 40MB each, it might be not a good idea to have all of them loaded into memory at one time (240MB) but okay to swap them around when needed? But this would still be a big amount of data when having the over-the-air limit in mind.
I lack experience in terms what can I expect from an application using SCNKit scenes and couldn't find very much to hold on.
","
Seems that it's incorrectly to talk about the Apple's game engines (like SceneKit or RealityKit) in such a context. As you know, game engines render 3D content in real time at 60 fps. If there is a drop frame, the app displays the content intermittently. As soon as you cross the ""red line"" you'll get notorious drop frame.
What can make you cross this ""red line""? It's a good question. Usually, scenes containing more than 100K polygons in total (especially in iOS), heavy-contact physics, emission of a large number of particles, Hi-res 4K textures, realtime shadows and large number of PBR materials can make your app to guaranteedly start skipping frames.
How to deal with it? There is no miraculous remedy. As soon as you notice that your app ""stutters"", first of all, you need to control a number of polygons (keep it under 100K) and textures' size (keep them under 2K). Physics and particles are more difficult to control, but they also need to be controlled. For example, a single particle emitter will work nicely even if it emits about 20K particles per second (with a moderate lifespan), not millions of them.

This table helps you understand what certain entities affect (macOS version).




SceneKit entity
What does it affect?




Hi-rez textures (4K or 8K)
Consume considerably more RAM


Higher number of polygons
Increases CPU processing and consumes more RAM


Higher number of particles
Increases GPU / CPU processing, consumes more RAM


Higher number of PBR materials
Increases GPU processing and consumes more RAM


Hi-rez poly-models with forward shadows
Considerably increase GPU processing



 "
"
I want to present some scene with PBR. I created metalness and roughness textures and want to apply it to the mesh. When I try to do it in Xcode - everything is fine. I can add this model to the scene and then add this textures to the model.

But I want to do it programmatically. Here is my code:
NSData *vertices = [[NSData alloc] initWithBytes:modelPly->vertices length:modelPly->vertexCount * 3 * sizeof(float)];
SCNGeometrySource *vertexSource = [SCNGeometrySource geometrySourceWithData:vertices
                                                                   semantic:SCNGeometrySourceSemanticVertex
                                                                vectorCount:modelPly->vertexCount
                                                            floatComponents:YES
                                                        componentsPerVector:3
                                                          bytesPerComponent:sizeof(float)
                                                                 dataOffset:0
                                                                 dataStride:sizeof(float) * 3];

// Normal source
NSData *normals = [[NSData alloc] initWithBytes:modelPly->normals length:modelPly->vertexCount * 3 * sizeof(float)];
SCNGeometrySource *normalSource = [SCNGeometrySource geometrySourceWithData:normals
                                                                   semantic:SCNGeometrySourceSemanticNormal
                                                                vectorCount:modelPly->vertexCount
                                                            floatComponents:YES
                                                        componentsPerVector:3
                                                          bytesPerComponent:sizeof(float)
                                                                 dataOffset:0
                                                                 dataStride:sizeof(float) * 3];


// Texture coordinates source
NSData *facesTextures = [[NSData alloc] initWithBytes:modelPly->texCoord length:modelPly->vertexCount * 2 * sizeof(float)];
SCNGeometrySource *texcoordSource = [SCNGeometrySource geometrySourceWithData:facesTextures
                                                                     semantic:SCNGeometrySourceSemanticTexcoord
                                                                  vectorCount:modelPly->vertexCount
                                                              floatComponents:YES
                                                          componentsPerVector:2
                                                            bytesPerComponent:sizeof(float)
                                                                   dataOffset:0
                                                                   dataStride:sizeof(float) * 2];


NSData *indicesData = [NSData dataWithBytes:modelPly->indices length:modelPly->indexCount * sizeof(uint) * 3];
SCNGeometryElement *elements = [SCNGeometryElement geometryElementWithData:indicesData
                                                            primitiveType:SCNGeometryPrimitiveTypeTriangles
                                                           primitiveCount:modelPly->indexCount
                                                            bytesPerIndex:sizeof(uint)];

SCNGeometry *geometry = [SCNGeometry geometryWithSources:@[vertexSource, normalSource, texcoordSource] elements:@[elements]];
MDLMesh *mesh = [MDLMesh meshWithSCNGeometry:geometry];

Then I'm trying to create material:
MDLPhysicallyPlausibleScatteringFunction *scatFunction = [MDLPhysicallyPlausibleScatteringFunction new];
MDLMaterial *mdlMaterial = [[MDLMaterial alloc] initWithName:@""material"" scatteringFunction:scatFunction];
[mdlMaterial removeAllProperties];
MDLMaterialProperty *bcProperty = [[MDLMaterialProperty alloc] initWithName:@""BaseColor"" semantic:MDLMaterialSemanticBaseColor URL:plyItem.textureFileURL];
MDLMaterialProperty *metalnessProperty = [[MDLMaterialProperty alloc] initWithName:@""metallic"" semantic:MDLMaterialSemanticMetallic URL:[[NSBundle mainBundle] URLForResource:@""metalness"" withExtension:@""jpg""]];
MDLMaterialProperty *roughnessProperty = [[MDLMaterialProperty alloc] initWithName:@""roughness"" semantic:MDLMaterialSemanticRoughness URL:[[NSBundle mainBundle] URLForResource:@""roughness"" withExtension:@""jpg""]];

[mdlMaterial setProperty:bcProperty];
[mdlMaterial setProperty:metalnessProperty];
[mdlMaterial setProperty:roughnessProperty];

and apply it to my geometry:
SCNNode *node = [SCNNode nodeWithMDLObject:mesh];
SCNMaterial *material = [SCNMaterial materialWithMDLMaterial:mdlMaterial];
material.lightingModelName = SCNLightingModelPhysicallyBased;
node.geometry.firstMaterial = material;

If I check in debugger mdlMaterial contains all information about roughness and metalness, but material doesn't. What's wrong?
","
Managed to do it via SCNGeometry only.
SCNNode *node = [SCNNode nodeWithGeometry:geometry];
node.geometry.firstMaterial.diffuse.contents = plyItem.textureFileURL;
node.geometry.firstMaterial.metalness.contents = [[[NSBundle mainBundle] URLForResource:@""metalness"" withExtension:@""jpg""] path];
node.geometry.firstMaterial.roughness.contents = [[[NSBundle mainBundle] URLForResource:@""roughness"" withExtension:@""jpg""] path];
node.geometry.firstMaterial.lightingModelName = SCNLightingModelPhysicallyBased;

"
"
I'm creating multiple nodes automatically and I want to arrange them around me, because at the moment I'm just increasing of 0.1 the current X location. 
capsuleNode.geometry?.firstMaterial?.diffuse.contents = imageView
capsuleNode.position = SCNVector3(self.counterX, self.counterY, self.counterZ)
capsuleNode.name = topic.name
self.sceneLocationView.scene.rootNode.addChildNode(capsuleNode)
self.counterX += 0.1

So the question is, how can I have all of them around me instead of just in one line? 
Did someone of you have some math function for this? Thank you! 
","
Use this code (macOS version) to test it:
import SceneKit

class GameViewController: NSViewController {

    override func viewDidLoad() {
        super.viewDidLoad()
        let scene = SCNScene()
        let scnView = self.view as! SCNView
        scnView.scene = scene
        scnView.allowsCameraControl = true
        scnView.backgroundColor = NSColor.black

        for i in 1...12 {  // HERE ARE 12 SPHERES

            let sphereNode = SCNNode(geometry: SCNSphere(radius: 1))
            sphereNode.position = SCNVector3(0, 0, 0)

            // ROTATE ABOUT THIS OFFSET PIVOT POINT
            sphereNode.simdPivot.columns.3.x = 5
            sphereNode.geometry?.firstMaterial?.diffuse.contents = NSColor(calibratedHue: CGFloat(i)/12, 
                                                                              saturation: 1, 
                                                                              brightness: 1,                
                                                                                   alpha: 1)

            // ROTATE ABOUT Y AXIS (STEP is 30 DEGREES EXPRESSED IN RADIANS)
            sphereNode.rotation = SCNVector4(0, 1, 0, (-CGFloat.pi * CGFloat(i))/6)
            scene.rootNode.addChildNode(sphereNode)
        }
    }
}




P.S. Here's a code for creating 90 spheres:

for i in 1...90 {

    let sphereNode = SCNNode(geometry: SCNSphere(radius: 0.1))
    sphereNode.position = SCNVector3(0, 0, 0)
    sphereNode.simdPivot.columns.3.x = 5
    sphereNode.geometry?.firstMaterial?.diffuse.contents = NSColor(calibratedHue: CGFloat(i)/90, saturation: 1, brightness: 1, alpha: 1)
    sphereNode.rotation = SCNVector4(0, 1, 0, (-CGFloat.pi * (CGFloat(i))/6)/7.5)
    scene.rootNode.addChildNode(sphereNode)
}


"
"
I am pretty new to Swift, and SceneKit, and my current problem is that the custom shape I am trying to create is not showing up, even though the primitive shapes in the framework show up fine. 
I have been following the tutorial from https://www.raywenderlich.com/1261-scene-kit-tutorial-with-swift-part-1-getting-started. 
I have also checked out the answer on SO: SceneKit – Custom geometry does not show up. I have looked at other answers on here but none work for me. 
Here is my code: 
import UIKit
import SceneKit
import QuartzCore

class GameViewController: UIViewController {
    var scnView: SCNView!
    var scnScene: SCNScene!
    var cameraNode: SCNNode!

    override func viewDidLoad() {
        super.viewDidLoad()
        setupView()
        setupScene()
        setupCamera()
        let lightNode0 = SCNNode()
        lightNode0.light = SCNLight()
        lightNode0.light!.type = .omni
        lightNode0.position = SCNVector3(x: 0, y: 10, z: 10)
        scnScene.rootNode.addChildNode(lightNode0)

        let lightNode1 = SCNNode()
        lightNode1.light = SCNLight()
        lightNode1.light!.type = .omni
        lightNode1.position = SCNVector3(5, -10, 0)
        scnScene.rootNode.addChildNode(lightNode1)
        spawnShape()
    }

    func shouldAutorotate() -> Bool {
        return true
    }

    func prefersStatusBarHidden() -> Bool {
        return true
    }
    func setupView() {
        scnView = self.view as! SCNView
        // 1
        scnView.showsStatistics = true
        // 2
        scnView.allowsCameraControl = true
        // 3
        scnView.autoenablesDefaultLighting = true
    }
    func setupScene() {
        scnScene = SCNScene()
        scnView.scene = scnScene
    }

    func setupCamera() {
        // 1
        cameraNode = SCNNode()
        // 2
        cameraNode.camera = SCNCamera()
        // 3
        cameraNode.position = SCNVector3(x: 0, y: 0, z: 15)
        // 4
        scnScene.rootNode.addChildNode(cameraNode)
    }

And here is the function that produce the custom shape:
    func spawnShape() {
        // 1
        var geometry:SCNGeometry
        let positions = [
            SCNVector3(-2, 1.5, 0), //0
            SCNVector3(-2, 1.5, 0), //1
            SCNVector3(2, -1.5, 0), //2
            SCNVector3(2, 1.5, 0), //3
            SCNVector3(-2, 1.5, 0.4), //4
            SCNVector3(2, 1.5, 0.4) //5
        ]
        let source = SCNGeometrySource(vertices: positions)
        let indices:[CInt] = [
            0, 2, 1,
            0, 3, 2,
            0, 4, 5,
            0, 5 ,3,
            4, 1, 2,
            4, 2, 5
            ]
        let element = SCNGeometryElement(indices: indices, primitiveType:.triangles)

        // 4
        geometry = SCNGeometry(sources: [source], elements: [element])
        let geometryNode = SCNNode(geometry: geometry)

        // 5
        scnScene.rootNode.addChildNode(geometryNode)
    }
}

","
Try my light-weighting code (macOS version for quick testing). 
It's working:
import SceneKit

class GameViewController: NSViewController {

    override func viewDidLoad() {
        super.viewDidLoad()
        let scene = SCNScene()
        let cameraNode = SCNNode()
        cameraNode.camera = SCNCamera()
        scene.rootNode.addChildNode(cameraNode)
        cameraNode.position = SCNVector3(x: 0, y: 0, z: 3)

        let geometry: SCNGeometry?
        let positions = [
            SCNVector3(0, 1, 0),
            SCNVector3(-0.5, 0, 0.5),
            SCNVector3(0.5, 0, 0.5),
            SCNVector3(0.5, 0, -0.5),
            SCNVector3(-0.5, 0, -0.5),
            SCNVector3(0, -1, 0),
        ]
        let source = SCNGeometrySource(vertices: positions)
        let indices: [UInt32] = [
            0, 1, 2,
            2, 3, 0,
            3, 4, 0,
            4, 1, 0,
            1, 5, 2,
            2, 5, 3,
            3, 5, 4,
            4, 5, 1
        ]
        let element = SCNGeometryElement(indices: indices, primitiveType:.triangles)
        geometry = SCNGeometry(sources: [source], elements: [element])
        geometry!.firstMaterial?.diffuse.contents = NSColor.red
        let geometryNode = SCNNode(geometry: geometry)
        scene.rootNode.addChildNode(geometryNode)

        let scnView = self.view as! SCNView
        scnView.scene = scene
        scnView.allowsCameraControl = true
        scnView.autoenablesDefaultLighting = true
        scnView.backgroundColor = NSColor.black
    }
}

"
"
I am working on a scene with a character and a platform with a stairs. Character must move up and down of the stairs and jump from the platform:

I move character by virtual d-pad. I am looking for a correct way to move a character. Now I have two ways, each has its advantages and disadvantages:
First way: ray test. This way works perfect: I can move character up and down of the stairs, but if I have a lot of objects near the pad, and I want to jump I need to do a lot of ray tests.
Second way: move physics body of the character. Using this way I couldn't understand how to move character down of the stairs. Each time it looks like a jump over the stairs. 
So what is the correct way to move a character over terrain with different height?
","
To move a character over a terrain with different heights you should use collisions. To find out more about it, watch a part on collision meshes in Enhancements to SceneKit at WWDC 2015 (time 17:30) and also look at a code in Fox 2 game's sample.
Here's a code's snippet how to use collisions:
private func loadCharacter() {
    let scene = SCNScene( named: ""Art.scnassets/character/max.scn"")!
    model = scene.rootNode.childNode( withName: ""Max_rootNode"", recursively: true)
    model.simdPosition = Character.modelOffset

    characterNode = SCNNode()
    characterNode.name = ""character""
    characterNode.simdPosition = Character.initialPosition

    characterOrientation = SCNNode()
    characterNode.addChildNode(characterOrientation)
    characterOrientation.addChildNode(model)

    let collider = model.childNode(withName: ""collider"", recursively: true)!
    collider.physicsBody?.collisionBitMask = Int(([ .enemy, .trigger, .collectable ] as Bitmask).rawValue)

    let (min, max) = model.boundingBox
    let collisionCapsuleRadius = CGFloat(max.x - min.x) * CGFloat(0.4)
    let collisionCapsuleHeight = CGFloat(max.y - min.y)

    let collisionGeometry = SCNCapsule(capRadius: collisionCapsuleRadius, height: collisionCapsuleHeight)
    characterCollisionShape = SCNPhysicsShape(geometry: collisionGeometry, options:[.collisionMargin: Character.collisionMargin])
    collisionShapeOffsetFromModel = float3(0, Float(collisionCapsuleHeight) * 0.51, 0.0)
}

"
"
I'm trying to set up a vignette effect within a SCNView. I did this following this tutorial using more or less the same code as I'm unexperienced with the range of the values. But when I apply that to my SCNViews camera object, nothing happens.
The docs about vignette read that it is necessary to set wantsHDR = true so I did that without any noticeable difference.
// scene setup (light, models, etc.)
...
sceneView.backgroundColor = .gray
sceneView.allowsCameraControl = true
let camera = sceneView.scene?.rootNode.camera
camera?.wantsHDR = true
camera?.vignettingPower = 0.6

camera?.bloomIntensity = 1.4
camera?.bloomBlurRadius = 1.0
    
camera?.fStop = 20.0
camera?.fStop = 5.0
camera?.focusDistance = 1.0

I've only changed the parameters which were marked as deprecated but that wasn't the issue.
I've instanciated the SCNView with the Storyboard and I'm accessing it by having an Outlet in my ViewController and I can use lot of functions with success.
Further I experienced problems setting up MSAA4x with sceneView.antialiasingMode = .multisampling4X. No difference in the outcome. And some more methods/parameters with skybox/environment lighting not doing anything (see this post).
No errors shown in the console.
","
If you added a new camera to the scene, it would render a vignette.
let cameraNode = SCNNode()
cameraNode.camera = SCNCamera()
cameraNode.position.z = 10
cameraNode.camera?.wantsHDR = true
cameraNode.camera?.vignettingPower = 1
cameraNode.camera?.vignettingIntensity = 1
sceneView.scene?.rootNode.addChildNode(cameraNode)


Your approach doesn't work because you're accessing the default camera.
cameraNode.camera = sceneView.scene?.rootNode.camera

"
"
I'm adding objects to a scene for each entry in a database. I had a cube appearing in the scene for entry, but when I tried to add raycasting to click on objects it doesnt work, the objects dont appear and the console reads ""Expression unavailable"".. I got parts of the code from the three.js website for Raycasting so not sure what I'm doing wrong. 
Here is the JS code: 
var renderer, scene, container, camera;
var geometry, material;
var controls, group;

var raycaster = new THREE.Raycaster();
var mouse = new THREE.Vector2();


init()

function onMouseMove( event ) {

    // calculate mouse position in normalized device coordinates
    // (-1 to +1) for both components

    mouse.x = ( event.clientX / window.innerWidth ) * 2 - 1;
    mouse.y = - ( event.clientY / window.innerHeight ) * 2 + 1;

}

function init() {
    // init renderer
    renderer = new THREE.WebGLRenderer( { antialias: true } );
    renderer.setPixelRatio( window.devicePixelRatio );
    renderer.setSize( window.innerWidth, window.innerHeight );
    // document.body.appendChild( renderer.domElement );

    container = document.getElementById('container');
    container.appendChild( renderer.domElement );

    // init scene
    scene = new THREE.Scene();
    scene.background = new THREE.Color( 0xffffff );

    group = new THREE.Group();
    scene.add( group )

    //fetch data from database and add object for each entry
    getData()
    async function getData() {
        var response = await fetch('/api/indexvr');
        var data = await response.json();
        console.log(data) 

        for (var i=0; i<data.length; i++) {
            cube = new THREE.Mesh( geometry, material );
            cube.position.x = i;
            scene.add(cube);
            //group.add(data)
        }
    }

    // init camera
    camera = new THREE.PerspectiveCamera( 50, window.innerWidth / window.innerHeight, 1, 1000 );
    camera.position.set( 15, 15, 15 ); //camera.position.set( 5, 0, 10 );
    camera.lookAt( scene.position );
    // controls = new OrbitControls( camera, renderer.domElement );
    // controls.enableRotate = true;
}

function render() {

    // update the picking ray with the camera and mouse position
    raycaster.setFromCamera( mouse, camera );

    // calculate objects intersecting the picking ray
    var intersects = raycaster.intersectObjects( scene.children );

    for ( var i = 0; i < intersects.length; i++ ) {

        intersects[ i ].object.material.color.set( 0xff0000 );

    }

    renderer.render( scene, camera );

}

window.addEventListener( 'mousemove', onMouseMove, false );

window.requestAnimationFrame(render);

The HTML just has a div called ""container"" and this tag: 
<script src=""https://cdnjs.cloudflare.com/ajax/libs/three.js/108/three.js""></script>

It doesn't produce any error it only says this in the console:
 
So it is fetching the data but can't render the scene


var renderer, scene, container, camera;
var geometry, material;
var controls, group;

var raycaster = new THREE.Raycaster();
var mouse = new THREE.Vector2();


init()

function onMouseMove(event) {

  // calculate mouse position in normalized device coordinates
  // (-1 to +1) for both components

  mouse.x = (event.clientX / window.innerWidth) * 2 - 1;
  mouse.y = -(event.clientY / window.innerHeight) * 2 + 1;

}

function init() {
  // init renderer
  renderer = new THREE.WebGLRenderer({
    antialias: true
  });
  renderer.setPixelRatio(window.devicePixelRatio);
  renderer.setSize(window.innerWidth, window.innerHeight);
  // document.body.appendChild( renderer.domElement );

  container = document.getElementById('container');
  container.appendChild(renderer.domElement);

  // init scene
  scene = new THREE.Scene();
  scene.background = new THREE.Color(0xffffff);


  group = new THREE.Group();
  scene.add(group)

  //fetch data from database and add object for each entry
  getData()
  async function getData() {
    /**
     * @author TheJim01
     * Replacing DB call with fake data to make it work here.
     * Nancy: Please feel free to add appropriate data.
     */
    // var response = await fetch('/api/indexvr');
    // var data = await response.json();
    var data = [{}, {}, {}, {}, {}]
    console.log(data)

    for (var i = 0; i < data.length; i++) {
      cube = new THREE.Mesh(geometry, material);
      cube.position.x = i;
      scene.add(cube);
      //group.add(data)
    }
  }

  // init camera
  camera = new THREE.PerspectiveCamera(50, window.innerWidth / window.innerHeight, 1, 1000);
  camera.position.set(15, 15, 15); //camera.position.set( 5, 0, 10 );
  camera.lookAt(scene.position);
  // controls = new OrbitControls( camera, renderer.domElement );
  // controls.enableRotate = true;
}

function render() {

  // update the picking ray with the camera and mouse position
  raycaster.setFromCamera(mouse, camera);

  // calculate objects intersecting the picking ray
  var intersects = raycaster.intersectObjects(scene.children);

  for (var i = 0; i < intersects.length; i++) {

    intersects[i].object.material.color.set(0xff0000);

  }

  renderer.render(scene, camera);

}

window.addEventListener('mousemove', onMouseMove, false);

window.requestAnimationFrame(render);
<script src=""//threejs.org/build/three.js""></script>
<script src=""https://cdnjs.cloudflare.com/ajax/libs/three.js/108/three.js""></script>
<script src=""https://unpkg.com/spritejs/dist/spritejs.min.js""></script>

<div id=""container""></div>



","
There are several things wrong that I can see. Whether that's because you omitted sections of your code, I can't say.
First, the code you provided does not define geometry, nor a material. You implied you're drawing a cube for each DB result, so I'll make an assumption and use BoxBufferGeometry. You also don't have any lights defined, so I'll just use MeshBasicMaterial which doesn't need lights.
With those out of the way, it looks like you're halfway to setting up a render loop using window.requestAnimationFrame, but you still only call render once, even though your DB fetching is asynchronous. In other words, the render might occur before you even get a response from your DB, so you would see nothing. I've added some boilerplate code to set up a render loop, similar to how three.js does in its examples.
Interestingly enough, that was all it took. The raycaster started working, and I was able to console log the results out. I did get some false-positives when the scene first started rendering, but that's because there hadn't been any mouse input yet, so it was raycasting from the middle of the screen (where the first cube exists).
Normally, you wouldn't want to raycast for every frame, but I understand the VR situation might be different (darn fidgety humans).
Finally, one last change I made was to give each cube its own material (well, clones of the original). This was necessary to ensure you could raycast against each one individually.


// Need to create geometry and material
var geometry = new THREE.BoxBufferGeometry(0.5, 0.5, 0.5);
var material = new THREE.MeshBasicMaterial({
  color: ""green""
});

var renderer, scene, container, camera;
var controls, group;

var raycaster = new THREE.Raycaster();
var mouse = new THREE.Vector2();

init()

function onMouseMove(event) {

  // calculate mouse position in normalized device coordinates
  // (-1 to +1) for both components

  mouse.x = (event.clientX / window.innerWidth) * 2 - 1;
  mouse.y = -(event.clientY / window.innerHeight) * 2 + 1;

}

function init() {
  // init renderer
  renderer = new THREE.WebGLRenderer({
    antialias: true
  });
  renderer.setPixelRatio(window.devicePixelRatio);
  renderer.setSize(window.innerWidth, window.innerHeight);
  // document.body.appendChild( renderer.domElement );

  container = document.getElementById('container');
  container.appendChild(renderer.domElement);

  // init scene
  scene = new THREE.Scene();
  scene.background = new THREE.Color(0xffffff);


  group = new THREE.Group();
  scene.add(group)

  //fetch data from database and add object for each entry
  getData()
  async function getData() {
    /**
     * @author TheJim01
     * Replacing DB call with fake data to make it work here.
     * Nancy: Please feel free to add appropriate data.
     */
    // var response = await fetch('/api/indexvr');
    // var data = await response.json();
    var data = [{}, {}, {}, {}, {}]
    //console.log(data)

    for (var i = 0; i < data.length; i++) {
      cube = new THREE.Mesh(geometry, material.clone());
      cube.position.x = i;
      scene.add(cube);
      //group.add(data)
    }
  }

  // init camera
  camera = new THREE.PerspectiveCamera(50, window.innerWidth / window.innerHeight, 1, 1000);
  camera.position.set(15, 15, 15); //camera.position.set( 5, 0, 10 );
  camera.lookAt(scene.position);
  // controls = new OrbitControls( camera, renderer.domElement );
  // controls.enableRotate = true;
}

function render() {

  // update the picking ray with the camera and mouse position
  raycaster.setFromCamera(mouse, camera);

  // calculate objects intersecting the picking ray
  var intersects = raycaster.intersectObjects(scene.children);
  if (intersects.length > 0) {
    console.log(intersects);
  }

  for (var i = 0; i < intersects.length; i++) {

  intersects[i].object.material.color.set(0xff0000);
  

  }

  renderer.render(scene, camera);

}

window.addEventListener('mousemove', onMouseMove, false);

// Here's the bbasic render loop implementation
function animate() {
  requestAnimationFrame(animate);
  render();
}
animate();
html,
body {
  width: 100%;
  height: 100%;
  margin: 0;
  padding: 0;
  overflow: hidden;
}
<script src=""https://cdnjs.cloudflare.com/ajax/libs/three.js/108/three.min.js""></script>

<div id=""container""></div>



"
"
I build my first VR app using unity. I am getting the following error when trying to build and run on a connected Oculus Quest 2 device!

Android device is not responding!  Make sure USB debugging has been
enabled and that the device has authorized this computer. Check your
device, in most cases there should be a small icon in the status bar
telling you if the USB connection is up.

I have tried many different ways to get this working, I am not able to figure this out, is there anything else I can try? I have listed down things I tried and config I have!

I have developer mode ON.
When I connect USB to Macbook Pro I do get an option to Allow in oculus quest with the following message and I select ""Allow"":


Allow Access to data, the connected device will be able to access
files on this headset. [Deny] [Allow]


I see my Oculus quest device in the ""Run Device"" dropdown in build settings and have it selected.
I also installed Android File Transfer and I do see all folders and files on the quest 2 device.
I tried a factory reset and restarting the quest 2 device and my MacBook pro multiple time.
App I have built runs fine when I click the play button and also it works when I build for the macOS and run on Macbook pro.

Screen shot for the settings and error File > Build Settings

Thank you!
","
It is working after I installed the android platform tools.

Install android-platform-tools

brew install android-platform-tools


Reconnect the USB to Oculus Quest 2 device, this time in oculus I got 2 confirmation boxes! One of them has my laptop address and also the checkbox to remember. I selected Allow.

After that File > Build and Run is now working all fine!


It required to have other updates to the settings but after I installed adb I did get the 2 confirmations in the quest when I connected and now the build and run functionality works all fine!
"
"
I added appropriate CDN's for aframe ,aframe extras, recast plugin
<script src=""https://aframe.io/releases/1.1.0/aframe.min.js""></script>
<script src=""https://recast-api.donmccurdy.com/aframe-inspector-plugin-recast.js""></script>
<script src=""https://cdn.jsdelivr.net/gh/donmccurdy/[email protected]/dist/aframe-extras.min.js"">/script>

,added my nav mesh entity inside <a-scene>
 <a-entity id=""wallmesh"" position=""0 0.769 0"" gltf-model=""navmesh/nav3.glb"" visible=""false""></a-entity>

and for player or camera I tried this,
 <a-entity id=""player-rig"" >
        <a-entity
          id=""player""
          networked=""template:#avatar-template;attachTemplateToLocal:false;""
          movement-controls=""constrainToNavMesh: true""
          camera=""active:true;""
          position=""-27 2.5 24""
          
          look-controls
          spawn-in-circle=""radius:2""
          rotate-with-camera
        >
        <a-entity id=""player-body""  networked=""template:#body; attachTemplateToLoacal:false;""         position=""-.17 -.88 0""></a-entity>
      </a-entity>
      </a-entity>

But my camera didnt move itself and spawned in different position than expected. Please  help me out if you find wrong anything in this approach...
","
The fact that your camera is in the wrong place and not moving suggests you might be hittting this issue, whereby A-Frame fails to notice that you have a user-defined camera, and adds a default camera, which takes over.
https://github.com/aframevr/aframe/issues/3921
In the console, enter
document.querySelector('[camera]')

If it returns 2 elements you almost certainly have the issue.
Simplest fix is to ensure you declare your user-defined camera before any <template> tags in your a-scene (you are using Networked A-Frame, so I guess you have some of these?)
Alternatively, something like this code in one of your components would probably also fix the issue.
    const sceneEl = document.querySelector('a-scene')
    
    if (sceneEl.camera.el.id !== 'player') {    
        console.log(""Deleting unwanted A-Frame default camera"")
        const wrongCameraEntity = this.el.sceneEl.camera.el;
        const cameraEntity = document.getElementById('player');
        cameraEntity.setAttribute('camera', 'active', true);
        wrongCameraEntity.parentEl.removeChild(wrongCameraEntity);
    }

"
"
Long things short:
I want make my iOS application Mac friendly. In my case I want (I don‘t know how to do) to use my ARSCNViews as an view that presents me everything in VR.
I don‘t want to have any special room. I only want to show SCNNodes.
Short:
I have got an ARSCNView so I see things in AR,
*but I want to see the things(SCNNodes) in VR, so that the users don’t have to continue using camera.

I want to see make an VRView from my AR(SCN)View.

I couldn‘t find any information how to do that.
I tried hard for 4 days.
Does somebody know how to do that?
Thanks
PS:
I already tried:

create a new camera


Use some not helpful code from Stackoverflow and  Dev Documentation
Like Stereo


Tried to create to ARSCNViews to give one as the input for the other on (I gave the first non camera input


(Something really stupid) I wanted to use an SCN

","
The simplest solution for nonAR (a.k.a. VR) app is to use SCNView.
"
"
My blit render feature works on the PC outside VR and in multipass rendering. But when rendering the shader in single pass instanced VR the left eye is grey and the right eye is black.

Here is my shader that is being used in the render feature. To keep the shader simple I've removed some code from below and only showing the relevant areas:
   struct Attributes
   {
     float4 vertex : POSITION;
     float2 uv : TEXCOORD0;

     UNITY_VERTEX_INPUT_INSTANCE_ID
   };

   struct Varyings
   {
     float2 uv : TEXCOORD0;
     float4 vertex : SV_POSITION;

     UNITY_VERTEX_INPUT_INSTANCE_ID
     UNITY_VERTEX_OUTPUT_STEREO
   };

   Varyings vert(Attributes i)
   {
     UNITY_SETUP_INSTANCE_ID(i);
     
     Varyings o = (Varyings)0;
     UNITY_TRANSFER_INSTANCE_ID(i, o);
     UNITY_INITIALIZE_VERTEX_OUTPUT_STEREO(o);
     o.vertex = TransformObjectToHClip(i.vertex.xyz);
     o.uv = UnityStereoTransformScreenSpaceTex(i.uv);
     
     return o;
   }

   half4 frag(Varyings i) : SV_Target0
   {
     UNITY_SETUP_INSTANCE_ID(i);
     UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(i);

     if (SLICE_ARRAY_INDEX == 0)
     {
       return half4(1,0,0,1);
     }
     else
     {
       return half4(0,0,1,1);
     }

   }

Here is the Render Feature code:
     public class RaymarchRenderFeature : ScriptableRendererFeature
 {
   [SerializeField] private RenderPassEvent passEvent = RenderPassEvent.AfterRenderingSkybox;
 
   private RaymarchRenderPass _renderPass;
 
   public override void Create()
   {
     _renderPass = new RaymarchRenderPass(name)
     {
       renderPassEvent = passEvent
     };
   }
 
   public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData)
   {
     renderer.EnqueuePass(_renderPass);
   }
 }
 
 public class RaymarchRenderPass : ScriptableRenderPass
 {
   private readonly string _profilerTag;
 
   private RenderTargetIdentifier _destination;
 
   public RaymarchRenderPass(string profilerTag)
   {
     _profilerTag = profilerTag;
   }
 
   public override void Configure(CommandBuffer cmd, RenderTextureDescriptor cameraTextureDescriptor)
   {
     RenderTextureDescriptor descriptor = cameraTextureDescriptor;
     // descriptor.enableRandomWrite = true;
     
     cmd.GetTemporaryRT(Shader.PropertyToID(""_Destination""), descriptor);
     _destination = new RenderTargetIdentifier(""_Destination"");
   }
 
   public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData)
   {
     var camera = renderingData.cameraData.camera;
     var cameraColourTexture = renderingData.cameraData.renderer.cameraColorTarget;
 
 #if UNITY_EDITOR
     if (camera.cameraType is not (CameraType.SceneView or CameraType.Game))
     {
       return;
     }
 #endif
 
     if (!Raymarch.ShouldRender())
     {
       return;
     }
 
     CommandBuffer cmd = CommandBufferPool.Get(_profilerTag);
 
     Raymarch.Material.SetMatrix(Shader.PropertyToID(""_CamToWorldMatrix""), camera.cameraToWorldMatrix);
     Raymarch.UploadShaderDataInvoke();
 
     cmd.Blit(cameraColourTexture, _destination, Raymarch.Material);
     cmd.Blit(_destination, cameraColourTexture);
 
     context.ExecuteCommandBuffer(cmd);
     // cmd.Clear();
     CommandBufferPool.Release(cmd);
 
     context.Submit();
   }
 
   public override void OnCameraCleanup(CommandBuffer cmd)
   {
     cmd.ReleaseTemporaryRT(Shader.PropertyToID(""_Destination""));
   }
 }

Any help would be much appreciated! If you'd like to check out the rest of the code its available on GitHub: Unity-Raymarching
","
Turns out this is a known issue, check out the issue tracker page here.
It's fixed at the time of writing but not published to the community just yet, hopefully, this will happen sometime in the near future. For the time being there is a workaround using DrawMesh instead of a Blit call. Check out this repo on how to implement it:
https://github.com/0lento/URP_RendererFeature_SPI_Example
"
"
thanks kindly for your time and attention. I recognise this is a long shot but I'm hoping someone might be generous enough to relay some advice or guidance. I am in the beginning phases of researching how I might build an app for a mobile device, using javascript and related tools, libraries and packages. The concept for the app is that it will access the device camera, recognise faces, and overlay animated AR assets onto the device display. However, I want users to couple their phone with a wearable headset, and use the app through a split screen VR style display. I assume I'll need some sort of VR wrapper for the core AR application. At the moment, I am focussed primarily on the graphical display and UI aspects, so that I can build a proof-of-concept to test whether the idea is even viable. I recognise I may be misguided to attempt this in javascript rather than native mobile languages - if this is the case, I would welcome any opinions on the matter. I'm asking about javascript because that's what I know, basically.
Thus far, I've been reading about various libraries such as WebXR, Three.js and others. I assume I'll need to use React Native, though it's not easy to get a clear sense of whether I should even be trying to achieve what I want using javascript. I have no code to show as yet.
Additionally, I recognise there are similar questions already posted to the forum - for example, this one: VR+AR on mobile phone.
I haven't found any recent threads that address this specific set of requirements so I do apologise if I've missed something. If there is info on the forum, grateful if someone could point me to the relevant thread. At the very least, thanks for reading. Cheers, all.
","
I found Snap's Lens Studio extremely intuitive and powerful. It provides templates for feature recognition, tracking, and physics. It also provides advanced controls for custom creative. I would also expect it to receive future feature development support. It can be monetized.
...or do you want to expose yourself to more computer vision terminology and patterns? Try searching CodePen or CodeSandbox for features such as: MediaPipe, OpenCV, face detection webcam. But the overhead of a VR/AR app is probably best described in an O'Reilly book or John Carmack keynote speech. Topics like pupil distance, foot tracking, and predictive tracking.
"
"
All the examples and tutorials on the threejs site and elsewhere offer this line: 
document.body.appendChild( WEBVR.createButton( renderer ) );

Which adds a nice little button to enter VR mode. But there doesn't seem to be any way to detect whether or not the user has pressed it and entered VR mode or not. And there doesn't seem to be any way to enter VR mode besides using that button. Have I missed something?
","
You can use the following line of code to check if the XR device has an active session.
renderer.xr.isPresenting()

"
"
I'm working on a VR game in Unreal Engine 4 using Blueprints.
I want to calculate the (yaw) angle the user needs to turn his/her gun (whose direction is determined via the position of the motion controllers) in order to be pointing towards the target.
I figure this might be the way to do it:

Subtract the location of the target from the location of the gun
Get the yaw component of that as a vector pointing from the gun as origin
Subtract the current yaw of the gun direction from that yaw component to get the yaw angle the user needs to turn to get to the target

Except I'm not quite sure how to execute that. I've been experimenting (as seen in the screenshot below), but not doing the correct operations. Any thoughts?
Thanks!

","
A more elegant and robust solution is to use the gun actor's world transform to calculate the relative rotation to the object:

Get the gun's world transform. The rotation should point in the forward vector direction. You can make a transform with its location and forward vector, but likely the component transform will work.
Use the operation InverseTransformLocation on this transform, with the target's location as other parameter. This creates a vector that is the target's location in the gun's space
Get the rotation of this vector with the RotationFromXVector operation. 

This rotator contains the correct yaw, but also pitch. And it will also work when your objects are rotated in space arbitrarily, or your objects become children of even more actors.
"
"
I am creating a VR application with mrtk 2.1 and want to have the User on the same height when playing seated or standing.
When Playing seated the camera is too low.
I could adjust the height of the MixedRealityPlayspace which contains the MainCamera, but that resets after using the teleport function.
Additionally learned from this GitHub Post, that there is an Offset between the MainCamera and MixedRealityPlayspace which is not accessible via mrtk so calculating the right height would be difficult.
This just does not seem like the right approach.
How can I make sure that the User is always on the right head height in game even when playing seated?
","
OK i found a solution that works for me.
I use the following code to move the mixedRealityPlayspace up or down to achieve the wanted player height.(if you want to use this make sure to tag your mixedRealityPlayspace GameObject ""Player"")
var mixedRealityPlayspace = GameObject.FindGameObjectsWithTag(""Player"")[0].transform;
var camHeight = mainCamera.transform.position.y;
var adjustement =wantedPlayerHeight-camHeight ;
mixedRealityPlayspace.position= new Vector3(mixedRealityPlayspace.position.x,mixedRealityPlayspace.position.y+adjustement, mixedRealityPlayspace.position.z);

I call this on ones on the start of my Scene and in the OnTeleportCompleted Event described in this Dokumentation for the MRTK Teleport system
"
"
There is sample code to play video in Decentraland. This link
However, I want to play video from a playlist in YouTube channel. Is there any sample code you can reefer me?
","
Unfortunately you cannot directly access any Youtube videos due to a CORS policy set on their side. This is not an issue with Decentraland itself, but rather with Youtube and other content providers that have a similar CORS policy.
Youtube and Twitch (for example) have their own javascript players that not only display ads but also gather information about the viewer. Seeing as this is the basis of their income, they do not want people bypassing this and just viewing the video for ""free"".
You may notice that youtube videos work in CryptoVoxel for some reason. This is due to some trickery that the game is doing, where it is loading in the html/js and placing the video on-top of the game itself. (At least that is what I've heard)
In the meantime I strongly suggest you look into hosting your videos on another platform like Vimeo. You can directly access the video files if you pay for their service. Alternatively, you can host your videos on an Antmedia server through a cloud hosting service like Digital Ocean. Both of these are great options.
"
"
How can I set the display to stereoscopic programmatically in Unity for an app deployed to an Android device?  
I want a UI menu where the user can toggle between ""VR mode"" and normal mode.  I do not want VR mode by default as it should be an option at run-time.  I know there is a setting for ""Virtual Reality Supported"" in the build settings, but again, I do not want this enabled by default.
","
Include using UnityEngine.XR; at the top.
Call XRSettings.LoadDeviceByName("""") with empty string followed by XRSettings.enabled = false; to disable VR in the start function to disable VR.
When you want to enable it later on, call XRSettings.LoadDeviceByName(""daydream"") with the VR name followed by XRSettings.enabled = true;. 
You should wait for a frame between each function call. That requires this to be done a corutine function.
Also, On some VR devices, you must go to Edit->Project Settings->Player and make sure that Virtual Reality Supported check-box is checked(true) before this will work. Then you can disable it in the Start function and enable it whenever you want.
EDIT:
This is known to work on some VR devices and not all VR devices. Although, it should work on Daydream VR.  Complete code sample:
IEnumerator LoadDevice(string newDevice, bool enable)
{
    XRSettings.LoadDeviceByName(newDevice);
    yield return null;
    XRSettings.enabled = enable;
}

void EnableVR()
{
    StartCoroutine(LoadDevice(""daydream"", true));
}

void DisableVR()
{
    StartCoroutine(LoadDevice("""", false));
}

Call EnableVR() to enable vr and DisableVR() to disable it. If you are using anything other than daydream, pass the name of that VR device to the LoadDevice function in the EnableVR() function. 
"
"
I am trying to get basic grabbing with OVRGrabber working for an Oculus Quest game in Unity.
I can use the custom hands scene and everything works fine. However, when I try and implement the same in my own scene, I am unable to grab an OVRGrabble GameObject.
Here's the hierarchy for my OVRCameraRig. I am using the OVRControllerPrefab as child objects to LeftHandAnchor and RightHandAnchor:

For each of the OVRControllerPrefab GameObjects, I have a Rigidbody and Sphere Collider (set to isTrigger) along with the OVRGrabber script. For the OVRGrabber script, I have the sphere collider set to be the grab collider

Finally, for the grabble game object, I have a RigidBody, Box Collider (with is trigger not set) and OVR Grabble

For the life of me, I can't figure out what I'm doing wrong. Any help will be greatly appreciated.
","
It is because the GripTransform for your OVRGrabber is set to null. Usually your hand GameObject set should look like this. If you do not have a gripTrans gameObject, create a gameObject at the center of the hand and attach this to ""GripTransform"" for that hand's OVRGrabber.


"
"
I’m making a horror vr game in unity with the ovr player controller and I’m trying to figure out how to make an animation I made in blender play behind me when ever I walk into a certain box trigger.
","
You could use something along the lines:
[SerializeField] private GameObject player;
[SerializeField] private Animation animation;

private void OnTriggerEnter(Collision other)
{
    if (other.gameObject == player)
        animation.Play();
}

The player and animation fields need to be initialized from the inspector.
"
"
I'm starting to learn aframe and want to use it with an Oculus Quest 2. I have the hands displayed and I'm trying to move around the VR world but when I move the hands don't move ... how to make the hands follow the camera?
thanks
This is my code
<html>
  <head>
    <script src=""https://aframe.io/releases/1.2.0/aframe.min.js""></script>
        <script src=""//cdn.rawgit.com/donmccurdy/aframe-extras/v6.1.0/dist/aframe-extras.min.js""></script>

    <script>


    </script>
  </head>
  <body>
    <a-scene>
        <a-entity id=""rig"" movement-controls position=""0 0 0"">
            <a-entity camera position=""0 0 0"" look-controls=""pointerLockEnabled: true""></a-entity>
        </a-entity>

        <a-entity id=""leftHand"" hand-controls=""hand: left; handModelStyle: lowPoly; color: #ffcccc""></a-entity>
        <a-entity id=""rightHand"" hand-controls=""hand: right; handModelStyle: lowPoly; color: #ffcccc""></a-entity>
                                


      <a-box position=""-1 0.5 -3"" rotation=""0 45 0"" color=""#4CC3D9""></a-box>
      <a-sphere position=""0 1.25 -5"" radius=""1.25"" color=""#EF2D5E""></a-sphere>
      <a-cylinder position=""1 0.75 -3"" radius=""0.5"" height=""1.5"" color=""#FFC65D""></a-cylinder>
      <a-plane position=""0 0 -4"" rotation=""-90 0 0"" width=""4"" height=""4"" color=""#7BC8A4""></a-plane>
      <a-sky color=""#ECECEC""></a-sky>
    </a-scene>
  </body>
</html>

","
I finally found ...
At the beginning I put the controllers in the camera ... but I had to put them in #rig
<a-entity id=""rig"" movement-controls position=""0 0 0"">
   <a-entity camera position=""0 0 0"" look-controls=""pointerLockEnabled: true""></a-entity>

    <a-entity id=""leftHand"" hand-controls=""hand: left; handModelStyle: lowPoly;""></a-entity>
    <a-entity id=""rightHand"" hand-controls=""hand: right; handModelStyle: lowPoly;""></a-entity>
</a-entity>

"
"
So basically I ""made"" a script that controls the hand movement, and has proper collisions too. Most of it works, however, the rotation seems to have a limited speed, even if its speed is set to high values.
The speed of the rotation is slow, no matter what I change the multiplier to.
Here is the code:
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class Hand: MonoBehaviour
{
    // Physics Movement
    [SerializeField] private GameObject followObject;
    [SerializeField] private float followSpeed = 30f;
    [SerializeField] private float rotateSpeed = 100f;
    [SerializeField] private float teleportDistance = 0.75f;
    [SerializeField] private Vector3 positionOffset;
    [SerializeField] private Vector3 rotationOffset;
    private Transform _followTarget;
    private Rigidbody _body;
    private bool _inGrab = false;

    void Start()
    {
        // Physics Movement
        _followTarget = followObject.transform;
        _body = GetComponent<Rigidbody>();
        _body.collisionDetectionMode = CollisionDetectionMode.Continuous;
        _body.interpolation = RigidbodyInterpolation.Interpolate;
        _body.mass = 20f;

        // Teleport Hands
        _body.position = _followTarget.position;
        _body.rotation = _followTarget.rotation;
    }

    void Update()
    {
        PhysicsMove();
    }

    private void PhysicsMove()
    {
        // Position
        var positionWithOffset = _followTarget.position + positionOffset;
        var distance = Vector3.Distance(positionWithOffset, transform.position);
        _body.velocity = (positionWithOffset - transform.position).normalized * (followSpeed * distance);

        // Rotation
        var rotationWithOffset = _followTarget.rotation * Quaternion.Euler(rotationOffset);
        var q = rotationWithOffset * Quaternion.Inverse(_body.rotation);
        q.ToAngleAxis(out float angle, out Vector3 axis);
        _body.angularVelocity = axis * (angle * Mathf.Deg2Rad * rotateSpeed);

        // Teleport back when too far
        if(distance > teleportDistance)
        {
            _body.position = _followTarget.position;
            _body.rotation = _followTarget.rotation;
        }
    }
}

The script is attached to a hand model with a rigidbody, and the follow-target is the XR Controller. Everything seems to be working, except the speed of the rotation.
Rarely worked with quaternions, so it might just be some basic issue.
","
You need to change the maxAngularVelocity of the rigidbody. The default is only 7 radians per second, which is less than 2 full rotations per second.
Setting it to something higher, such as 20, will allow your Rigidbody to turn more quickly when needed. Unfortunately, this isn't accessible through the inspector, but it is  through scripting:
void Start()
{
    // Physics Movement
    _followTarget = followObject.transform;
    _body = GetComponent<Rigidbody>();
    _body.collisionDetectionMode = CollisionDetectionMode.Continuous;
    _body.interpolation = RigidbodyInterpolation.Interpolate;
    _body.mass = 20f;
    _body.maxAngularVelocity = 20f;

    // Teleport Hands
    _body.position = _followTarget.position;
    _body.rotation = _followTarget.rotation;
}

"
"
https://aframe.io/aframe/examples/boilerplate/360-video/
I am new to Aframe js. So, I'm trying to mimic this page. I have copied whole code but still its is not working.
<html>
  <head>
      <script src=""https://aframe.io/releases/1.2.0/aframe.min.js""></script>
    </head>
    <body>
        <a-scene inspector="""" keyboard-shortcuts="""" screenshot="""" vr-mode-ui="""" device-orientation-permission-ui="""">
            <a-assets>
                
                <video id=""video"" autoplay="""" loop="""" crossorigin=""anonymous"" playsinline="""" webkit-playsinline="""" src=""https://bitmovin.com/player-content/playhouse-vr/progressive.mp4"">
                </video>
            </a-assets>
            <a-videosphere rotation=""0 -90 0"" src=""#video"" play-on-click="""" material="""" geometry="""" scale="""">
            </a-videosphere>
            <a-camera camera="""" position="""" rotation="""" look-controls="""" wasd-controls="""">
                <a-entity position=""0 0 -1.5"" text=""align:center;
                                                    width:6;
                                                    wrapCount:100;
                                                    color: white;
                                                    value: Click or tap to start video"" hide-on-play=""#video"">
                </a-entity>
            </a-camera>
            </a-scene>
        
        
        <div id=""highlighter--hover-tools"" style=""display: none;"">
            <div id=""highlighter--hover-tools--container"">
                <div class=""highlighter--icon highlighter--icon-copy"" title=""Copy""></div>
                <div class=""highlighter--separator""></div>
                <div class=""highlighter--icon highlighter--icon-change-color"" title=""Change Color""></div>
                <div class=""highlighter--separator""></div>
                <div class=""highlighter--icon highlighter--icon-delete"" title=""Delete""></div>
            </div>
        </div></body>
</html>


Edit:

I am receiving an error in my console log
","
You would want to read the Aframe documentation before starting to work with it:
Use a Local Server
For the options below, we should develop projects using a local server so that files are properly served. Options of local servers include:

Downloading the Mongoose application and opening it from the same directory as your HTML file.
Running python -m SimpleHTTPServer (or python -m http.server for Python 3) in a terminal in the same directory as your HTML file.
Running npm install -g live-server && live-server in a terminal in the same directory as your HTML file.
Once we are running our server, we can open our project in the browser using the local URL and port which the server is running on (e.g., http://localhost:8000). Try not to open the project using the file:// protocol which does not provide a domain; absolute and relative URLs may not work.

"
"
I'm trying to animate the position of a gltf model using A-Frame by clicking on a button. The animation works without the 'startEvents' property but stops working as soon as the property is added. The animation does work on an aframe primitive. 
Here's an example of the problem: https://jsfiddle.net/ofp46ge0/1/
HTML:
<a-entity id=""#whale"" gltf-model=""#whale"" position=""0 1.5 -10"" rotation=""5 -50 0"" scale=""5 5 5"" 
animation=""property: position; from: 0 1.5 -10; to: 0 0 0; startEvents: position; dur: 2000; delay: 2000;"">
</a-entity>

JS:
document.querySelector("".enter"").addEventListener('click',function () {
  document.querySelector(""#whale"").emit(""position"");
});

What am I missing here?
","
You have an <a-asset-item id=""whale""> and your event is going there. Rename it to id=""whaleModel"".
"
"
I'm trying to find a way to select objects in VR.
When a certain object is clicked upon a certain code will run telling what to do - such as quit the app.
Right now, the code runs to see whether an object has the layer 'Interactable', but how do I set my code up to check for more layers?
In the code, I've already tried setting up a check for more masks than just the 'Interactable' mask. 
private void ProcessTouchpadDown()
    {
        if (!m_CurrentObject)
            return;

        Interactable interactable = m_CurrentObject.GetComponent<Interactable>();
        interactable.Pressed();
    }
}

Upon the code above, the code below will run.
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class Interactable : MonoBehaviour
{
    public void Pressed()
    {
        //This code will run when an object with the 'interactable' layer is clicked. 
    }
}

I expect that when I add another layer mask I can just add it to the ProcessTouchDown and make a copy of the interactable.pressed(); to apply different functions when an object is pressed.
","
Layer masks in Unity are bit flags. You can use this to your advantage and iterate over the layers you want to check by bit shifting them.
    LayerMask layer = LayerMask.GetMask(""Layer A"", ""Layer B"", ""Layer C"");
    Interactable interactable = currentObject.GetComponent<Interactable>();

    //Checks if interactable's layer is contained within the layermask
    if (layer == (layer | (1 << interactable.gameObject.layer))) 
        interactable.Pressed();

"
"
Im trying to isntantiate a prefab with photon bit i have this issue

Thats because the prefab im trying to instantiate needs some dependencies from de scene like the camera


Any ideas how can i do this?? :/
Thanks a lot!
","
Just try to assign the object in the script. If you need the camera use
followObject = GameObject.FindWithTag(""MainCamera"");

or
followObject = Camera.main;

in the Start() method.
"
"
First, here is my setup:
Rig:

Windows 10
AMD Ryzen ThreadRipper 3970x 32-core Processor (64 CPUs)
Nvidia GeForce RTX 3080 Ti
125 GB Ram
New installation of Premiere pro 2021 with updated importer files as requested here and downloaded here

When I import a video file that has already been processed in insta360 stitcher, and that look like this:

I start getting this error while premiere is trying to ""Generate peak file"", I don't even need to drag the file into the timeline before it starts complaining. I have been dealing with this problem for over a day and am at my wits end, here is what I have tried and where I get it from:

Placing files in the C drive instead of D drive suggested here
Disable Cuda and used ""Software only"" as the Playback and Rendering setting suggested here and here
Delete render files - this wasnt possible because it is greyed out but suggested here

Additionally I changed my graphics card Drivers to Studio Driver instead of Game-ready driver, this did not work either

The next thing I can try is to reroll back to pre 14.4 9 but I would really rather not have to do this.
==Edit==
I installed v 14.3 and got the same error so I installed 14.2 where i also got the same issue and then moved onto 14.1 where I am still getting the same issue. So this downgrade also is not working:

I am really at a loss as to what to try next, any suggestions would be most welcome.
Thank you.
","
I finally got this fixed, it turns out it was the plugin that came with 3.0.0 version of insta360 stitcher + updates.
Insta360 sent me their updated Stitcher 3.1.2 which comes with a brand new plugin but is not yet listed on their website. They have given me permission to post the file here for anyone who wishes to download it.
https://static.insta360.com/Insta360Stitcher/Insta360Stitcher_Winx64_3.1.2_build_202012301001.exe

"
"
I have recently started learning VR on udacity, and when I use unity, no scripts show up in the inspector. When I use 5.6.1p2, I get this message.

Here is the console tab

I recently installed a beta version of unity, as referred to by a mentor, and there is no longer an error message, but the scripts simply do not appear
","
Problem resolved:
After putting course assets into a new folder, and extracting them, all dependencies were accessible within unity. Library was rebuilt with all required dependencies, and project is completed and working.
To open the project, I opened unity first, and used the open project button to select the folder including the assets, library, and temp folders. When I opened through the assets folder, the library wasn't complete, therefor not all components and dependencies were accessible.
"
"
I'm trying to convert a 180° fisheye video to a normal/regular video using the v360 filter of FFmpeg.
This is the command I tried:
ffmpeg -i in.mp4 -vf ""v360=input=fisheye:output=flat:iv_fov=180:v_fov=90"" out.mp4
But the output says clearly No such filter: 'v360', although v360 is a common filter listed in docs and other filters I used before worked just fine. I tried updating/reinstalling and looking for solutions, not fixing it.
Why is the filter missing? How can I debug this? Should I doe the task using another program entirely?
Command output:
ffmpeg version 4.2.4-1ubuntu0.1 Copyright (c) 2000-2020 the FFmpeg developers
  built with gcc 9 (Ubuntu 9.3.0-10ubuntu2)
  configuration: --prefix=/usr --extra-version=1ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared
  libavutil      56. 31.100 / 56. 31.100
  libavcodec     58. 54.100 / 58. 54.100
  libavformat    58. 29.100 / 58. 29.100
  libavdevice    58.  8.100 / 58.  8.100
  libavfilter     7. 57.100 /  7. 57.100
  libavresample   4.  0.  0 /  4.  0.  0
  libswscale      5.  5.100 /  5.  5.100
  libswresample   3.  5.100 /  3.  5.100
  libpostproc    55.  5.100 / 55.  5.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'in.mp4':
  Metadata:
    major_brand     : mp42
    minor_version   : 0
    compatible_brands: mp42mp41
    creation_time   : 2021-09-11T14:18:33.000000Z
  Duration: 00:02:48.02, start: 0.000000, bitrate: 26056 kb/s
    Stream #0:0(eng): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 2160x1080 [SAR 1:1 DAR 2:1], 25924 kb/s, 50 fps, 50 tbr, 50k tbn, 100 tbc (default)
    Metadata:
      creation_time   : 2021-09-11T14:18:33.000000Z
      handler_name    : Mainconcept MP4 Video Media Handler
      encoder         : AVC Coding
    Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 125 kb/s (default)
    Metadata:
      creation_time   : 2021-09-11T14:18:33.000000Z
      handler_name    : Mainconcept MP4 Sound Media Handler
Stream mapping:
  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))
  Stream #0:1 -> #0:1 (aac (native) -> aac (native))
Press [q] to stop, [?] for help
[AVFilterGraph @ 0x55ee57567340] No such filter: 'v360'
Error reinitializing filters!
Failed to inject frame into filter network: Invalid argument
Error while processing the decoded data for stream #0:0
Conversion failed!

","
Your ffmpeg is too old
You need at least version 4.3 to use the v360 filter (see Changelog). For Linux you can download a new version or compile it yourself.
The online documentation is synced with the latest code (git master branch). So releases may not have features mentioned in the online documentation. Refer to your locally installed documentation if you are stuck using an old release: man ffmpeg-filters
"
"
On A-Frame an entity is relative to its parent. If you move the parent entity, the child moves as well. That's great. But I think it would be great if there was a way to specify an entity to be relative to another entity, that is not necessarily its parent. Even to be relative to the screen. Here's a code snippet to show what I mean:
<a-entity id=""entity1"" position=""0 0 0"">
    <a-entity id=""entity2"" relative = """"></a-entity> <!-- """" or ""#entity1"" would make it relative to ""entity1"". This is the way it already works, so the relative component would have """" as the default value -->
    <a-entity id=""entity3"" relative = ""#entity4""></a-entity> <!-- This makes ""entity3"" relative to ""entity4"", not ""entity1"" -->
    <a-entity id=""entity5"" relative = ""#entity1 #entity4"" relativeWeight = ""0.5 0.5""></a-entity> <!-- This makes ""entity5"" half influenced by ""entity1"" and half by ""entity4"". But it could have other values like ""0.8 0.2"" or ""1 0""(80% affected by entity1 and 20% by entity 4, then 100% affected by entity1 and 0% by entity4) -->
</a-entity>

<a-entity id=""entity4"" position=""0 1 0"">
</a-entity>

<a-entity id=""entity6"" relative=""Screen""> <!-- I was thinking of some special values, like ""Screen"", ""TopScreen"", ""BottomScreen"", to make elements be relative to the center of the screen, top, bottom, etc.  -->
</a-entity>

<a-entity id=""entity7"" relative=""#entity5""> <!-- This entity is relative to entity5, which in turn is relative to both entity1 and entity4. In practice, this makes entity7 be indirectly affected by entity1 and entity4 as well because of entity5. -->
</a-entity>

You would then be able to animate the relativeWeight component to smoothly transition an entity's relativity to another entity.
Unity has this: https://docs.unity3d.com/Manual/Constraints.html
ZapWorks Studio has this as well: https://docs.zap.works/studio/scripting/reference/node/functions/relativeto-setter/ and https://docs.zap.works/studio/scripting/reference/node/functions/relativetoprop-setter/ .
It's an extremely useful feature I think.
An example usage of this, is having an UI that is relative to the tracking in an augmented reality experience, but if the user moves away from the tracking, the UI goes to the screen.
","
You can implement this function using this A-Frame component.
https://github.com/diarmidmackenzie/screen-display#blend-transforms
Instead of this syntax:
<a-entity id=""entity5"" relative = ""#entity1 #entity4"" relativeWeight = ""0.5 0.5"">
</a-entity>

...the component uses the following syntax:
<a-entity id=""entity5"" blend-transforms=""objectA:#entity1;
                                         objectB:#entity4;
                                         percentage: 50"">
</a-entity>

Basically this does exactly what is being asked for here.
For AR, when you want to move objects to/from the screen, this can be used in combination with the screen-display component (in the same repo) which provides a simple syntax for positioning objects on screen.
"
"
I am able to get the position of the base stations but this only updates once, while controllers and HDM are constantly updated. Is there a way to force the refresh so that I can get the position of the base stations (trackingreference) in real-time? thanks!
","
Technically there is, I'm talking driver side right now, but it kinda does nothing, tracking references are normal tracked objects after all so drivers can update their poses like normal with a vr::VRServerDriverHost()->TrackedDevicePoseUpdated() call and on init through the pose returned by GetPose()
Now, that should work, but it doesnt, even more most of the time custom tracking refences don't show up in SteamVR. what it does under the hood? no idea
Also commercial headsets might update their tracking reference's position only once on startup which seems to be the case
"
"
Is it possible to get real-time data about the HMD tracking quality via OpenVR? In SteamVR you can see a warning when your headset if not being tracked correctly. Is it possible to get that info via OpenVR API only?
","
OpenVR is the public part of SteamVR, you cant have one without the other, especially if you're an application
"
"
I want to have VR videos on my server and be able to play them on most devices through a website.
I have tried everything that comes up in the first feed on a google search but have not found want I’m been looking for.
The SW can be licensed, but I must be able to play videos from my server. There are several hosting services that does VR through , but that is not what I need.
A simple js include and a reference to the video url should be all that is needed….
Any tips on where to look?
","
I ended up using delight VR web player. Simple integration and a free tier that fit my needs: https://delight-vr.com/
"
"
I want to have VR videos on my server and be able to play them on most devices through a website.
I have tried everything that comes up in the first feed on a google search but have not found want I’m been looking for.
The SW can be licensed, but I must be able to play videos from my server. There are several hosting services that does VR through , but that is not what I need.
A simple js include and a reference to the video url should be all that is needed….
Any tips on where to look?
","
I ended up using delight VR web player. Simple integration and a free tier that fit my needs: https://delight-vr.com/
"
"
I'm having trouble using the new action based input system in Unity OpenXR.
With the old (device based) input system it was possible to retrieve an input device object from outside the XR Rig using the InputDevices.GetDeviceAtXRNode(<node>) function.
For example: This is what I would do in the old system to retrieve position data of the right hand controller:
InputDevices.GetDeviceAtXRNode(XRNode.RightHand).TryGetFeatureValue(CommonUsages.devicePosition, out Vector3 position);
InputDevices.GetDeviceAtXRNode(XRNode.RightHand).TryGetFeatureValue(CommonUsages.deviceRotation, out Quaternion rotation);

Unfortunatly I can't find a way to do the same thing with the new action based input system. All the documentation that I could find on this topic refers to the old way of doing it. It appears that this method does not work anymore.
So, is there a way to retrieve an input device from outside the XR Rig using the new action based input system?
In case it helps: My Unity version is 2020.3.4f1 and I'm using the OpenXR plugin version 1.0.3.
Any help is greatly appreciated.
","
ActionBasedController[] controllerArray = ActionBasedController.FindObjectsOfType<ActionBasedController>();
ActionBasedController controller = controllerArray[0];

By calling this method you get all ActionBasedControllers returned as an array. The controller can be identified by their name.
controller.name.Equals(""Left Controller"")

The trigger value can be read by following command:
//Position
controller.positionAction.action.ReadValue<float>();
//Trigger
controller.activateAction.action.ReadValue<float>();
//Grip
controller.selectAction.action.ReadValue<float>();

"
"
Hi i'm looking to develop my first mobile vr game and I've been following a few tutorials to get started with this in Unity.
https://developers.google.com/cardboard/develop/unity/quickstart
All of these explain how you need to install a specific package via the package manager:
https://github.com/googlevr/cardboard-xr-plugin.git
When I add this via the package manager using the Git URL option I just get the error

[Package Manager Window] Error adding package:
https://github.com/googlevr/cardboard-xr-plugin.git.
UnityEditor.EditorApplication:Internal_CallUpdateFunctions () (at
/Users/bokken/buildslave/unity/build/Editor/Mono/EditorApplication.cs:327)

Is there something I'm missing? is there some kind of setting or something? All the tutorials literally just add the URl and it works but mine just wont seem to.
I'm using the latest version of unity and running it on a macbook pro if that has any relevance.
Thanks
","
Make sure you have Git installed on your PC which is a requirement to be able to install git packages by git URL in the Package Manager –
"
"
I am wondering how it would be possible in A-frame (aframe.io) to set the color of an object to a variable. For example, if a variable called x has a value of #8F3A84 then the color of the shape will be set to that color. If the variable has a value of #738F3A then the shape will also set to that color. How can I achieve something like this?
Here is the code for my shape:       <a-box position=""0 20 0"" static-body width=""20"" depth=""20"" height=""0.1""></a-box>
","
You can use a function instead of a variable. Like the following:
const changeColor = (eleId, colorValue) => {
  const box = document.getElementById(eleId)
  box.setAttribute('color', colorValue)
}

Edit
Full Implementation
index.html
<HTML>
<body>
  <a-scene stats="""" inspector="""" keyboard-shortcuts="""" screenshot="""" vr-mode-ui="""" device-orientation-permission-ui="""">
    <a-box id='test' position=""0 5 0"" static-body="""" width=""5"" depth=""5"" height=""5"" material="""" geometry=""""></a-box>
  </a-scene>
</body>
<script>
window.changeColor = (eleId, colorValue) => {
  const box = document.getElementById(eleId)
  box.setAttribute('color', colorValue)
}

changeColor('test', 'red')
</script>
</html>

Call the changeColor function from anywhere
"
"
I am making a video game in Aframe and I'm wondering how can I set my jump height to a variable. Here is the code I'm currently using
<a-entity id=""rig""
            position=""17.5 50.1 0"" 
            movement-controls=""speed: 0.2;""
            kinematic-body=""enableJumps: true;""
            jump-ability=""distance: 1.8;""
         
                networked=""template:#avatar-template;attachTemplateToLocal:false;""
                spawn-in-circle=""radius:3""
            tracker>
    <a-entity camera=""far: 1000;""
              wasd-controls
              look-controls=""pointerLockEnabled: true;""
              position=""0 1.6 0"">
        
    </a-entity>
  </a-entity>

What I would like to happen is to set the jump-ability to equal a variable. For example: jump-ability=""distance: VARIABLE;"" Does anybody know how I can achieve this?
","
Without any framework (like angular, or react) you can't use js variables within html elements.
The if you want some external js script to modify the property, you should do this with setAttribute():


// either use any logic in a js script
const btn = document.querySelector(""button"");
const sphere = document.querySelector(""a-sphere"");
btn.addEventListener(""click"", e => {
  // set the radius to a random number
  sphere.setAttribute(""radius"", Math.random() * 1 + 0.5);
})
<script src=""https://aframe.io/releases/1.2.0/aframe.min.js""></script>
<button style=""z-index: 999; position: fixed"">RESIZE</button>
<script>
  // or preferably within a custom component
  AFRAME.registerComponent(""foo"", {
    init: function() {
      const btn = document.querySelector(""button"");
      const sphere = document.querySelector(""a-sphere"");
      btn.addEventListener(""click"", e => {
        // no need to duplicate the script logic
        // sphere.setAttribute(""radius"", Math.random() * 1 + 0.5);
      })
    }
  })
</script>
<a-scene foo>
  <a-sphere position=""0 1.25 -5"" radius=""1.25"" color=""#EF2D5E""></a-sphere>
</a-scene>



Which changes the radius value each time the button is clicked.
"
"
So, I´m developing a VR game with base building functionalities and am using the position where the player hand Raycast hit as the position a preview GameObject has to go. But obviously when I do that, the gameObject clips through the Hit mesh. I am trying to find a way to avoid that.
I´ve came up with a solution but don´t get it to work in any way. Maybe it´s my idea, maybe my programming knowledge, no idea, but well. This would be it.

So Preview GameObject Transform is set to the HitPoint transform and moved away from point of collision until not colliding anymore. I guess? I only come up with performance heavy solutions and they don´t even work anyways. Does someone know a good solution?
Thanks in advance!
","
This sounds like you are looking for Physics.ComputePenetration

Compute the minimal translation required to separate the given colliders apart at specified poses.

So you will need a Collider on your placed object (the hit one already has one anyway) and then do e.g.
// Wherever you get this from
Collider objectToPlace;

if(Physics.Raycast(yourRay, out var hit))
{
    objectToPlace.trabsform.position = hit.point;

    if(Physics.ComputePenetration(objectToPlace, objectToPlace.transform.position, objectToPlace.transform.rotation, hit.collider, hit.transform.position, hit.transform.rotation, out var direction, out var distance))
    {
        objectToPlace.position += direction * distance;
    }
}

"
"
Since a decent frame rate is so important in VR apps, I was just wondering if you can predict frames dropping? If so, before this issue actually occurs, can you deactivate some scripts or other features except the camera transform's updating and rendering of the environment? So, if performance drops (i.e. frames drop) no nausea will be experienced.
","
Predicting the future is not very likely, so you're going to have to adapt on the fly when you see performance drop. Either that or you could imagine creating a test environment a user could run where you try and figure out the capabilities of the user's hardware setup and tweak settings accordingly for future actual app runs. (i.e. ""the test environment ran below the desired 120fps at medium settings, so default to low from now on"")
I don't know what platform you are on exactly, but just in case you're on the Oculus ecosystem you might be able to get some help however.
By default on Oculus devices you're supported by what they refer to as ""Asynchronous TimeWarp"". This in essence decouples the headset's transform update and rendering from the framerate of your application. If no up-to-date frame is available, the latest frame will be transformed based on the latest head tracking information, reducing how noticeable such hiccups are. You will still want to avoid this having to kick in as much as possible though.
Additionally, Oculus supports ""Fixed Foveated Rendering"" on their mobile platforms where depending on your GPU utilization the device can render at lower resolutions at the edges of your view. In practice I've found this to be surprisingly effective, even though (as the name implies) it's fixed at the center of the view and does not include any eye tracking. But as with the previous method, not needing it is always better.
I'm unfortunately less familiar with options on other devices, but I'm sure others will pitch in if those exist.
"
"
So, I´m developing a VR game with base building functionalities and am using the position where the player hand Raycast hit as the position a preview GameObject has to go. But obviously when I do that, the gameObject clips through the Hit mesh. I am trying to find a way to avoid that.
I´ve came up with a solution but don´t get it to work in any way. Maybe it´s my idea, maybe my programming knowledge, no idea, but well. This would be it.

So Preview GameObject Transform is set to the HitPoint transform and moved away from point of collision until not colliding anymore. I guess? I only come up with performance heavy solutions and they don´t even work anyways. Does someone know a good solution?
Thanks in advance!
","
This sounds like you are looking for Physics.ComputePenetration

Compute the minimal translation required to separate the given colliders apart at specified poses.

So you will need a Collider on your placed object (the hit one already has one anyway) and then do e.g.
// Wherever you get this from
Collider objectToPlace;

if(Physics.Raycast(yourRay, out var hit))
{
    objectToPlace.trabsform.position = hit.point;

    if(Physics.ComputePenetration(objectToPlace, objectToPlace.transform.position, objectToPlace.transform.rotation, hit.collider, hit.transform.position, hit.transform.rotation, out var direction, out var distance))
    {
        objectToPlace.position += direction * distance;
    }
}

"
"
Since a decent frame rate is so important in VR apps, I was just wondering if you can predict frames dropping? If so, before this issue actually occurs, can you deactivate some scripts or other features except the camera transform's updating and rendering of the environment? So, if performance drops (i.e. frames drop) no nausea will be experienced.
","
Predicting the future is not very likely, so you're going to have to adapt on the fly when you see performance drop. Either that or you could imagine creating a test environment a user could run where you try and figure out the capabilities of the user's hardware setup and tweak settings accordingly for future actual app runs. (i.e. ""the test environment ran below the desired 120fps at medium settings, so default to low from now on"")
I don't know what platform you are on exactly, but just in case you're on the Oculus ecosystem you might be able to get some help however.
By default on Oculus devices you're supported by what they refer to as ""Asynchronous TimeWarp"". This in essence decouples the headset's transform update and rendering from the framerate of your application. If no up-to-date frame is available, the latest frame will be transformed based on the latest head tracking information, reducing how noticeable such hiccups are. You will still want to avoid this having to kick in as much as possible though.
Additionally, Oculus supports ""Fixed Foveated Rendering"" on their mobile platforms where depending on your GPU utilization the device can render at lower resolutions at the edges of your view. In practice I've found this to be surprisingly effective, even though (as the name implies) it's fixed at the center of the view and does not include any eye tracking. But as with the previous method, not needing it is always better.
I'm unfortunately less familiar with options on other devices, but I'm sure others will pitch in if those exist.
"
"
We are currently trying to create a virtual object ( A plane ) and overlay it on the controller of a Magic Leap Device. However, we are having some difficulties doing this.
When I set the width or any dimension of an entity to say 1 meter. The resulting virtual image is not 1 meter and actually smaller.
Also, It seems like the image does on stay at the same position relative to the controller when we move it around. There seems to be a offset that magic leap adds.
  <a-entity id=""controller"" magicleap-controls=""hand: right"">
        <a-plane id=""board"" width=""0.45"" height=""0.1"" position=""0 0 0.0"" rotation=""-45 0 0"" color=""#b3b3b3"" material=""side: double; transparent: true; opacity: 0.5"">
        </a-plane>
</a-entity>

This is the code I am using. I am essentially trying to create a virtual board (which will map to the physical board -- but the sizes are not lining up)
What could be the issue?? Any leads would be appreciated
","
I think it may be caused by the parent node's attributes. In A-Frame every entity inherits position, scale, and rotation from its parent element. The child element inherits that and marks it in its local space as [0 0 0] (or [1 1 1] for scale) and that can be whatever else in world space. Any child transformation will be applied on top of that.
quick edit - not only the direct parent will have an effect but every single entity that is wrapping the final element.
source - relative-positioning
Look at the example below
there are 2 spheres that are identical but one is wrapped in another entity with changed attributes. Look what happens - it gets distorted and displaced as the transformation is applied to its local space.


<!DOCTYPE html>
<html>
    <head>
        <script src=""https://aframe.io/releases/1.0.0/aframe.min.js""></script>
        <style>
        
        </style>
    </head>
    <body>
        <a-scene>
          <a-entity position=""-6 0 -5"" scale=""2 2 1"">
            <a-sphere scale=""1 2 3"" position=""3 1 -5"" color=""blue""> </a-sphere>
          </a-entity>
          <a-sphere scale=""1 2 3"" position=""3 1 -5"" color=""red""> </a-sphere>
        </a-scene>
    </body>
</html>



"
"
I have been making a script in Unity that measures how far a player has moved in the real world using XRNodes like this for example with the right hand:
InputTracking.GetLocalPosition(XRNode.RightHand) 
at the start of the movement and then comparing it to the end position
Now I would like to get the distance moved, even if the player moved around in a circle.
Is the a method to do this with XRNodes? Measuring total distance moved during play?
","
Yes, well, you could just simply sum it up every frame like
// Stores the overall moved distance
private float totalMovedDistance;

// flag to start and stop tracking
// Could also use a Coroutine if that fits you better
private bool track;

// Store position of last frame
private Vector3 lastPos;

public void BeginTrack()
{
    // reset total value
    totalMovedDistance = 0;
    
    // store first position
    lastPos = InputTracking.GetLocalPosition(XRNode.RightHand);

    // start tracking
    track = true;
}

public void EndTrack()
{
    // stop tracking
    track = false;

    // whatever you want to do with the total distance now
    Debug.Log($""Total moved distance in local space: {totalMovedDistance}"", this);
}

private void Update()
{
    // If not tracking do nothing
    if(!track) return;

    // get current controller position
    var currentPos = InputTracking.GetLocalPosition(XRNode.RightHand);

    // Get distance moved since last frame
    var thisFrameDistance = Vector3.Distance(currentPos, lastPos);

    // sum it up to the total value
    totalMovedDistance += thisFrameDistance;

    // update the last position
    lastPos = currentPos;
}

"
"
I had my first contacts with Three.js recently and I need some help to do some tests.
At the moment I am trying to create an AR application, where an object is inserted into the real environment using the cell phone’s camera. Initially the object is presented on the screen in blue color, however in the top I want to put three buttons with three different colors, for example blue, red and green, and when touching one of the buttons the object just change its colors.
I created the object and its materials on Blender. One of the faces of the object has a material with a UV texture and the rest of the object has a second material, without textures, just a material with the initial node (created in the Blender cycles). So I’ve created 6 materials, 3 for the textures, one each, and 3 for the colors, one each as well. On Blender I exported this object as .glb with materials and textures.
Now I’m importing this object on Three.js, I already understood how to add this object to the scene and render it so that it appears on the phone’s screen. Inclusive the first materials that came with the object worked normally (coming from .glb), I didn’t create anything about the materials in Three.js, everything is shown in the scene so far comes from the .glb that I exported in blender and just loaded on three.js.
Now I need to create the second part of the application, that is: when I touch the button of one color, the object must change color, as well as the “texture”. And it is here that I need help.
How do I do this?
Do I need three different objects? Or just one object with all 6 materials? How can I do this features on three.js? I need help on Blender and on Three.js.
Can someone give a direction please? Or a link to a video or a good conversation about this theme.
Thanks in advance.
","
Essentially what you need to do is change the material of the relevant meshes within your .glb file.
When you load the .glb file with the GLTFLoader you give it an 'onLoad' function parameter where you have the first access to the contents of the file (the file itself is it's own mini scene) - This is the point where you can traverse through the scene, locate the mesh and save it as a variable so that you can later manipulate it.
Now, assuming there are more than a couple of meshes in the file, how can you know which one is correct? The best way would be by the name. Since you created the object from Blender, you can give a unique name to each mesh to later identify in three.js.
So now within the onLoad function you can insert some logic to confirm that the correct mesh is the one you are storing as a variable, consider the following code:
let meshFromGLTF;
const meshName = ""MESH_0"" // assuming this is the name you gave it on blender

...
loader = new GLTFLoader();
loader.load( ""path/to/gltf"", ( gltf ) => {
    const scene = gltf.scene;
    scene.traverse( ( child ) => {
        if ( child.isMesh && child.name === meshName ) { // to confirm it is a mesh
            meshFromGLTF = child;
            // this may also be a good point to store the original material as a variable
        }
    }
})

And now since you stored the mesh as a variable, it will be easy to change it's material and texture with a button. Simply supply an onClick function when the button is clicked and within that function you can do the following:
const texture = textureLoader.load( 'path/to/texture.png' );
const materialBlue = new MeshBasicMaterial ( { map: texture, color: 'blue' } );

function onClickBlue () {
   meshFromGLTF.material = materialBlue;
}

In terms of creating materials for the mesh, it is quite a broad topic, you can check out the documentation and see what kind of materials you may need, textures you apply on the material itself on parameters that include the word 'map'.
I created a small example here that does this approach.
"
"
I was wondering how it would be possible in A-frame (aframe.io) turn a box into a link. I have the link structure and the box structure but I want the box to turn into a link. Any ideas on how I could accomplish this?
","
a-frames link component uses window.location to change websites, you can do the same within a custom component:


<script src=""https://aframe.io/releases/1.2.0/aframe.min.js""></script>
<script>
  // component declaration
  AFRAME.registerComponent(""mylink"", {
    // define a url in the schema
    schema: {
      href: {}
    },
    init: function() {
      // when clicked - change the location:
      this.el.addEventListener(""click"", (e) => {
        window.location = this.data.href;
      })
    }
  })
</script>

<a-scene cursor=""rayOrigin: mouse"">
  <a-box position=""0 1 -2"" color=""blue"" mylink=""href: https://aframe.io/;""></a-box>
</a-scene>




You could also grab the anchor (a) element and do `anchorElement.click();` instead of changing the `window.location` if you prefer.
    "
"
I tried to create a basic virtual reality IDE based on Karaf.
I have 3 maven-projects:

VRServer who connect to a native-component for rendering.
VR Component Toolkit for VR-HMI-Widgets (Having a VRFrame who extends from VRComponent) as a Multi-Module-Maven-Project.
The VR IDE project read/write files and execute lowlevel commands.


I installed all of them into Karaf:
karaf@root()> bundle:list
START LEVEL 100 , List Threshold: 50
 ID | State  | Lvl | Version            | Name
----+--------+-----+--------------------+---------------------------------------------------------------------------
 31 | Active |  80 | 4.3.0              | Apache Karaf :: OSGi Services :: Event
 54 | Active |  80 | 1.3.2              | Apache Aries SPI Fly Dynamic Weaving Bundle
 55 | Active |  80 | 9.0.0              | org.objectweb.asm
 56 | Active |  80 | 9.0.0              | org.objectweb.asm.commons
 57 | Active |  80 | 9.0.0              | org.objectweb.asm.tree
 58 | Active |  80 | 9.0.0              | org.objectweb.asm.tree.analysis
 59 | Active |  80 | 9.0.0              | org.objectweb.asm.util
 60 | Active |  80 | 2.0.17             | OpenWebBeans Core
 61 | Active |  80 | 2.0.17             | SPI definition
 62 | Active |  80 | 1.1.3              | Apache Aries CDI - CDI Component Runtime (CCR)
 63 | Active |  80 | 1.1.3              | Apache Aries CDI - SPI classes for Portable Extensions
 64 | Active |  80 | 1.1.3              | Apache Aries CDI - Container using Apache OpenWebBeans
 65 | Active |  80 | 1.1.3              | Apache Aries CDI - SPI
 66 | Active |  80 | 1.0.12             | Apache Felix Converter
 67 | Active |  80 | 1.2.0              | Apache Geronimo JSR-330 Spec 1.0
 68 | Active |  80 | 1.1.0              | Apache Geronimo Expression Language Spec 2.2
 69 | Active |  80 | 1.2.0              | Apache Geronimo Interceptor Spec 1.2
 70 | Active |  80 | 1.2.0              | Apache Geronimo JCDI Spec 2.0
 71 | Active |  80 | 1.3.0.3            | Apache ServiceMix :: Specs :: Annotation API 1.3
 72 | Active |  80 | 4.17.0             | Apache XBean :: ASM shaded (repackaged)
 73 | Active |  80 | 4.17.0             | Apache XBean OSGI Bundle Utilities
 74 | Active |  80 | 4.17.0             | Apache XBean :: Finder shaded (repackaged)
 75 | Active |  80 | 1.0.1.201505202024 | org.osgi:org.osgi.namespace.extender
 76 | Active |  80 | 1.0.0.201505202024 | org.osgi:org.osgi.namespace.implementation
 77 | Active |  80 | 1.0.0.201505202024 | org.osgi:org.osgi.namespace.service
 78 | Active |  80 | 1.0.0.201810101357 | org.osgi:org.osgi.service.cdi
 79 | Active |  80 | 1.1.0.201802012106 | org.osgi:org.osgi.util.function
 80 | Active |  80 | 1.1.0.201802012106 | org.osgi:org.osgi.util.promise
 81 | Active |  80 | 4.3.1.SNAPSHOT     | Apache Karaf :: OSGi Services :: Event
134 | Active |  80 | 1.0.0              | Virtual reality server
135 | Active |  80 | 0.0.1.SNAPSHOT     | VRComponent
136 | Active |  80 | 0.0.1.SNAPSHOT     | VRFrame
142 | Active |  80 | 0.0.1.SNAPSHOT     | Virtual reality integrated development environment (VRIDE)
karaf@root()>

But when I tried to start the VRIDE bundle, I get this exception:
10:00:42.193 WARN [fileinstall-C:\Program Files\apache-karaf-4.3.0/deploy] Error while creating extension
java.lang.NoClassDefFoundError: Lde/e_nexus/vr/tk/VRFrame;
        at java.lang.Class.getDeclaredFields0(Native Method) ~[?:1.8.0_232]
        at java.lang.Class.privateGetDeclaredFields(Class.java:2611) ~[?:1.8.0_232]
        at java.lang.Class.getDeclaredFields(Class.java:1944) ~[?:1.8.0_232]
        at org.apache.aries.cdi.container.internal.util.Reflection.allFields(Reflection.java:47) ~[!/:1.1.3]
        at org.apache.aries.cdi.container.internal.annotated.AnnotatedTypeImpl.<init>(AnnotatedTypeImpl.java:42) ~[!/:1.1.3]
        at org.apache.aries.cdi.container.internal.container.Discovery.lambda$discover$8(Discovery.java:133) ~[!/:1.1.3]
        at java.util.HashMap$Values.forEach(HashMap.java:981) ~[?:1.8.0_232]

This is the Java-Class I tried to load:
package de.e_nexus.desktop.vr.ide;

import java.util.logging.Logger;

import javax.annotation.PostConstruct;
import javax.annotation.PreDestroy;
import javax.enterprise.context.ApplicationScoped;
import javax.inject.Inject;

import org.osgi.service.cdi.annotations.Reference;
import org.osgi.service.cdi.annotations.Service;

import de.e_nexus.vr.server.ClientKeyboardScancode;
import de.e_nexus.vr.server.VRClientHelmetAndControllerListener;
import de.e_nexus.vr.server.VRClientKeyboardListener;
import de.e_nexus.vr.server.VRServer;
import de.e_nexus.vr.server.listeners.VRClientRequestAppInfo;
import de.e_nexus.vr.server.listeners.VRClientStatusListener;
import de.e_nexus.vr.server.listeners.interaction.HelmetAndControllerInfo;
import de.e_nexus.vr.server.osgi.inter.VRServerService;
import de.e_nexus.vr.tk.VRFrame;

@ApplicationScoped
@Service
public class StartIDE implements VRClientStatusListener, VRClientRequestAppInfo, VRClientHelmetAndControllerListener, VRClientKeyboardListener {
    /**
     * The logger for this class.
     */
    private static final Logger LOG = Logger.getLogger(StartIDE.class.getCanonicalName());

    private final static Object lock = new Object();

    private VRServer vrServer;

    public void notifyStatus(boolean connected) {
        if (!connected) {
            synchronized (lock) {
                lock.notify();
            }
        }
    }

    @Reference
    @Inject
    private VRServerService vrServerService;

    private VRFrame applicationFrame;

    @PostConstruct
    public void initialize() {
        LOG.fine(""Starting VR IDE"");
        vrServer = vrServerService.getVRServer();
        applicationFrame = new VRFrame(vrServer, ""test"");

        LOG.info(getLatin1Title() + "" started successfully!"");
    }

    @PreDestroy
    public void teardown() {
        applicationFrame.remove();
        LOG.info(""Stoped "" + getLatin1Title() + "" successfully!"");
        vrServer = null;
    }

    public VRServer getVrServer() {
        return vrServer;
    }

    @Override
    public void notify(HelmetAndControllerInfo haci) {

    }

    @Override
    public String getLatin1Title() {
        return ""VR IDE"";
    }

    @Override
    public void notifyKeyboardEvent(ClientKeyboardScancode[] downs, ClientKeyboardScancode[] ups, long incomingTime) {
        System.out.println(""me"");
    }
}

VRServer-Codebase
VRToolkit-Codebase
VRIDE-Codebase

EDIT:
Here is the complete stacktrace:
java.lang.NoClassDefFoundError: Lde/e_nexus/vr/tk/VRFrame;
        at java.lang.Class.getDeclaredFields0(Native Method) ~[?:1.8.0_232]
        at java.lang.Class.privateGetDeclaredFields(Class.java:2611) ~[?:1.8.0_232]
        at java.lang.Class.getDeclaredFields(Class.java:1944) ~[?:1.8.0_232]
        at org.apache.aries.cdi.container.internal.util.Reflection.allFields(Reflection.java:47) ~[!/:1.1.3]
        at org.apache.aries.cdi.container.internal.annotated.AnnotatedTypeImpl.<init>(AnnotatedTypeImpl.java:42) ~[!/:1.1.3]
        at org.apache.aries.cdi.container.internal.container.Discovery.lambda$discover$8(Discovery.java:133) ~[!/:1.1.3]
        at java.util.HashMap$Values.forEach(HashMap.java:981) ~[?:1.8.0_232]
        at org.apache.aries.cdi.container.internal.container.Discovery.discover(Discovery.java:130) ~[!/:1.1.3]
        at org.apache.aries.cdi.container.internal.container.ContainerState.<init>(ContainerState.java:178) ~[!/:1.1.3]
        at org.apache.aries.cdi.container.internal.Activator.doCreateExtension(Activator.java:209) [!/:1.1.3]
        at org.apache.felix.utils.extender.AbstractExtender.createExtension(AbstractExtender.java:242) [!/:1.1.3]
        at org.apache.felix.utils.extender.AbstractExtender.modifiedBundle(AbstractExtender.java:227) [!/:1.1.3]
        at org.osgi.util.tracker.BundleTracker$Tracked.customizerModified(BundleTracker.java:488) [osgi.core-7.0.0.jar:?]
        at org.osgi.util.tracker.BundleTracker$Tracked.customizerModified(BundleTracker.java:420) [osgi.core-7.0.0.jar:?]
        at org.osgi.util.tracker.AbstractTracked.track(AbstractTracked.java:232) [osgi.core-7.0.0.jar:?]
        at org.osgi.util.tracker.BundleTracker$Tracked.bundleChanged(BundleTracker.java:450) [osgi.core-7.0.0.jar:?]
        at org.apache.felix.framework.EventDispatcher.invokeBundleListenerCallback(EventDispatcher.java:915) [org.apache.felix.framework-6.0.3.jar:?]
        at org.apache.felix.framework.EventDispatcher.fireEventImmediately(EventDispatcher.java:834) [org.apache.felix.framework-6.0.3.jar:?]
        at org.apache.felix.framework.EventDispatcher.fireBundleEvent(EventDispatcher.java:516) [org.apache.felix.framework-6.0.3.jar:?]
        at org.apache.felix.framework.Felix.fireBundleEvent(Felix.java:4817) [org.apache.felix.framework-6.0.3.jar:?]
        at org.apache.felix.framework.Felix.startBundle(Felix.java:2336) [org.apache.felix.framework-6.0.3.jar:?]
        at org.apache.felix.framework.BundleImpl.start(BundleImpl.java:998) [org.apache.felix.framework-6.0.3.jar:?]
        at org.apache.felix.fileinstall.internal.DirectoryWatcher.startBundle(DirectoryWatcher.java:1260) [!/:3.6.8]
        at org.apache.felix.fileinstall.internal.DirectoryWatcher.startBundles(DirectoryWatcher.java:1233) [!/:3.6.8]
        at org.apache.felix.fileinstall.internal.DirectoryWatcher.doProcess(DirectoryWatcher.java:520) [!/:3.6.8]
        at org.apache.felix.fileinstall.internal.DirectoryWatcher.process(DirectoryWatcher.java:365) [!/:3.6.8]
        at org.apache.felix.fileinstall.internal.DirectoryWatcher.run(DirectoryWatcher.java:316) [!/:3.6.8]
Caused by: java.lang.ClassNotFoundException: de.e_nexus.vr.tk.VRFrame not found by de.e-nexus.component [87]
        at org.apache.felix.framework.BundleWiringImpl.findClassOrResourceByDelegation(BundleWiringImpl.java:1597) ~[?:?]
        at org.apache.felix.framework.BundleWiringImpl.access$300(BundleWiringImpl.java:79) ~[?:?]
        at org.apache.felix.framework.BundleWiringImpl$BundleClassLoader.loadClass(BundleWiringImpl.java:1982) ~[?:?]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_232]
        at org.apache.felix.framework.BundleWiringImpl.getClassByDelegation(BundleWiringImpl.java:1375) ~[?:?]
        at org.apache.felix.framework.BundleWiringImpl.searchImports(BundleWiringImpl.java:1618) ~[?:?]
        at org.apache.felix.framework.BundleWiringImpl.findClassOrResourceByDelegation(BundleWiringImpl.java:1548) ~[?:?]
        at org.apache.felix.framework.BundleWiringImpl.access$300(BundleWiringImpl.java:79) ~[?:?]
        at org.apache.felix.framework.BundleWiringImpl$BundleClassLoader.loadClass(BundleWiringImpl.java:1982) ~[?:?]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_232]
        ... 27 more

","
As the Logging points-out:
Caused by: java.lang.ClassNotFoundException: de.e_nexus.vr.tk.VRFrame not found by de.e-nexus.component [87]

What means Karaf is searching the VRFrame from the component submodule, which clearly does not exist.
That makes sense since all java packages are exported, the VRFrame's package is ambiguous. I followed what özkan pakdil said and renamed the java-packages to a unique name. Since that it works like a charm.
Thanks Özkan! Your answer includes some usefull hints and the source of my problem was the ambiguous package-name.
"
"
I cloned the openVR repo and went directly to compile the driver_sample and hellovr_dx12 and hellovr_opengl projects. The builds were successful. But both the helloVR applications failed to launch with an error:
Failed to find interface 'IVRCompositor_027'

I have no clue except My guess is that the variable IVRCompositor_Version might need to be set appropriately, but how to find the right version?
Using SteamVR 1.15.15 on Windows 10, VS2019.
The HMD (HTC Vive) is correctly setup.
","
I updated SteamVR to the lastest version and the issue is solved.
"
"
I just implemented a scoring system for a VR game that I'm working on. But performance is suffering and I think that it's because of how I'm rendering the score. Performance issues started right after I implemented scoring.
I'm creating a new THREE.Mesh with a new THREE.TextGeometry, and then adding it to the scene each time that the score changes. As far as I understand, the string that is being rendered in a TextGeometry object cannot be changed dynamically, so a whole new object needs to be created.
The following happens every time the score changes:
    this.scene.remove( this.text );
    this.material = this.assetStore.mainEmissiveMaterial;
    this.geometry = new THREE.TextGeometry( String(this.score), {
        font: this.assetStore.mainFont,
        size: 6,
        height: 0,
    } );
    this.geometry.translate(-10, 25, -40);
    this.text = new THREE.Mesh( this.geometry, this.material );
    this.scene.add( this.text );

My question: Is there an alternative way of showing text, that doesn't affect performance as much?
","
When dealing with geometries, you shouldn't create and destroy them each time you need to update. This kills your performance. Instead, create all 10 digits 0123456789, save them as your templates, and then re-use them. Show and hide them and duplicate them as necessary, but don't re-build thousands of vertices and faces each time the text changes.
"
"
i am using Unity 2019.4.9f1 with SteamVR. The Teleporting-System works as expected in Playmode. When I build, there are some strange collision-glitches with the TeleportArc. The point at the end does not collide correct with mesh. The colliders seem to be underneath the floor(while still working properly), or the wall mesh etc. I am also able to see the TeleportPoint through mesh.
Editor-Playmode:
Working as expected
Build:
TeleportArc not colliding with Wall
TeleportArc not colliding with Floor
TeleportPoint visible through Wall
Has anyone experienced errors like these? Thanks in advance.
Edit: I would like to show these images in my posting but I am not allowed to because of my karma.
","
Okay, I found a solution:
It is a problem with the Material of the Teleporting Prefab. The URP-MaterialSwitcher was not working correct. I changed the materials manually and now everything is working fine.
https://github.com/ValveSoftware/steamvr_unity_plugin/issues/875
"
"
I am new to Unity and VR. I am trying to create a very basic VR application using Unity and my Android phone via Google Cardboard.
I followed the steps given here, https://developers.google.com/cardboard/develop/unity/quickstart, https://docs.unity3d.com/Manual/android-sdksetup.html and installed android studio sdk on my laptop.
Here is my scene, hierarchy, and build settings. No error while building. The refrigerator model is my game object, and as you can see the camera points at the refrigerator, at least that's what I inferred.
On my phone, I only see the green background when facing down, as I turn my phone up it changes into white. Nothing else is seen.
Setup images:
[1]: https://i.sstatic.net/bOsNl.jpg
[2]: https://i.sstatic.net/aTsrf.jpg
[3]: https://i.sstatic.net/a9QSJ.jpg
Output images:
[4]: https://i.sstatic.net/IgWuy.jpg
[5]: https://i.sstatic.net/LDmQX.jpg
[6]: https://i.sstatic.net/4lfHU.jpg
","
Make sure the HelloCardboard scene is included when building your app, and remove the default empty scene which just contains an extra redundant camera.
You have to do that from the Build Settings window with the button ""Add Open Scenes"".
"
"
The ARKit and RealityKit tutorials I have found all deal with anchors. However, there are VR apps that do not place any objects on surfaces. Instead, they just take the location and orientation of the device to display objects that are far away from the phone:

Star Chart shows stars, planets and the sun at their apparent location.
Peakfinder shows the mountains which are currently visible.

Both these apps do not need any real-world anchors. They just take the camera's location and orientation, and then render a model.
Can I create a similar app with ARKit or RealityKit, or is this a use case beyond these two frameworks?
","
It depends on what you need – AR or VR app. Generally speaking, you definitely need anchors for AR app, and don't need or do need anchors for VR (RealityKit supports anchoring from scratch, but SceneKit doesn't support anchoring).

If you need a comprehensive info about ARKit and RealityKit anchors – read this post.

Using RealityKit framework you can easily create both VR and AR apps (like games, visualisations and scientific apps). In case you place a 3D models in VR scene you tether these models (like mentioned distant stars, or mountains) with AnchorEntity(.world) anchors. In case you place 3D models in AR scene you tether a model with any of the following anchors' types: .world, .image, .face, .plane, .body, etc.
Using pure SceneKit framework you can create just VR apps. SceneKit doesn't have any anchors under its hood. But if you're using SceneKit with ARKit you have to create AR apps with all the corresponding anchors that ARKit has. This post will tell you about RealityKit/SceneKit differences. In addition to the above I should say that ARKit can't render VR and AR scenes, ARKit's divine purpose is world tracking and scene understanding.
"
"
I am using the latest XR Toolkit. I have a XR Direct Interactor. I would like to know if there is a way when I pick up an object using this can the objet just retain its current orientation?
It seems to always orient it either to some default orientation or the orientation of an Attach gameobject.
I would like it to not change the orientation of the object you are picking up. Just leave it oriented however it currently is and pick it up.
","
Two things to try, first uncheck 'Track Rotation'. If that still doesn't work, add an empty object with the desired orientation/position as an offset object under 'Attach Transform'

"
"
I'm trying to implement instancing in Aframe using the ThreeJs InstancedMesh based on the example here: https://github.com/mrdoob/three.js/blob/master/examples/webgl_instancing_dynamic.html
Relevant section of code here:
    init: function() {
        const {count, radius, scale, colors, positions} = this.data;  
        this.start = true;
        this.dummy = new THREE.Object3D();
        this.count = count;
        this.startObject = new THREE.Object3D();
        this.endObject = new THREE.Object3D();
        this.instanceColors = new Float32Array(count * 3);
        this.instanceColorsBase = new Float32Array(this.instanceColors.length);

        this.vertices = [];
        this.rotations = [];
        for ( var i = 0; i < this.data.count; i ++ ) {
            var x = this.data.positions[i][0] * this.data.scale;
            var y = this.data.positions[i][1] * this.data.scale;
            var z = this.data.positions[i][2] * this.data.scale;

            var xEnd = x + this.data.endPositions[i][0] * this.data.scale;
            var yEnd = y + this.data.endPositions[i][1] * this.data.scale;
            var zEnd = z + this.data.endPositions[i][2] * this.data.scale;

            this.vertices.push( x, y, z );
            const rotation = this.getDirection({'x':x,'y':y,'z':z}, 
                                          {'x':xEnd,'y':yEnd,'z':zEnd});
            this.rotations.push(rotation.x, rotation.y, rotation.z);
        }

        let mesh;
        let geometry;
        let material;
        const loader = new THREE.GLTFLoader();
        const el = this.el;
        loader.load(""/assets/arrow/arrow.gltf"", function ( model ) {
            geometry = model.scene.children[0].children[0].geometry;
           
            geometry.computeVertexNormals();
            geometry.scale( 0.03, 0.03, 0.03 );

            material = new THREE.MeshNormalMaterial();

            mesh = new THREE.InstancedMesh( geometry, material, count );
            mesh.instanceMatrix.setUsage( THREE.DynamicDrawUsage );
            el.object3D.add(mesh);
        } );
        this.el.setAttribute(""id"", ""cells"");
    },
    setMatrix: function (start) {
        if (this.mesh) {
            for ( let i = 0; i < this.count; i ++ ) {
                var x = this.data.positions[i][0] * this.data.scale;
                var y = this.data.positions[i][1] * this.data.scale;
                var z = this.data.positions[i][2] * this.data.scale;

                var xEnd = x + this.data.endPositions[i][0] * this.data.scale;
                var yEnd = y + this.data.endPositions[i][1] * this.data.scale;
                var zEnd = z + this.data.endPositions[i][2] * this.data.scale;
                if (start) {
                    this.dummy.position.set(xEnd, yEnd, zEnd);
                } else {
                    this.dummy.position.set(x, y, z);
                }
                this.dummy.rotation.x = this.rotations[i][0];
                this.dummy.rotation.y = this.rotations[i][1];
                this.dummy.rotation.z = this.rotations[i][2];
                this.dummy.updateMatrix();
                this.mesh.setMatrixAt( i, this.dummy.matrix );
            }
            this.mesh.instanceMatrix.needsUpdate = true;
        }
    }
    tick: function() {
        this.setMatrix(this.start);
        this.start = !this.start;
    },

No errors or relevant messages that I can see, but none of the instanced objects are rendering. I don't really have a good way to post an example unfortunately. Anyone know what I'm doing wrong? Thanks in advance!
Note: It seems that the objects are being rendered because the number of triangles being drawn increases drastically when I add this component. However, they are not visible anywhere and I can't find them in the aframe inspector either
","
It's a very case specific question with a quite extensive topic, so:
In general, using THREE.InstancedMeshes is simple, and you got it right:
// create an instanced mesh
let iMesh = new THREE.InstancedMesh(geometry, material, count)
element.object3D.add(iMesh)

// manipulate the instances
let mtx = new Matrix4()
// set the position, rotation, scale of the matrix
// ...
// update the instance
iMesh.setMatrixAt(index, mtx);
iMesh.instanceMatrix.needsUpdate = true;

Example of an instanced gltf model here

Your code is doing a lot, and it would be easier if it could be stripped to a bare minimum. Yet I think there is only one major issue - this.model isn't set anywhere, so the setMatrix function does nothing. Other than that you may need to disable frustum culling (mesh.frustumCulling = false), or set a bounding sphere - otherwise the objects may dissapear when the base object is out of sight.
Once it's set, your code seems to be working
"
"
So I am trying to implement teleportation in my VR application (not in Unity). I am able to get the pose matrices for each controller from
if (auto error = vr::VRInput()->GetPoseActionDataForNextFrame(hand[eHand].pose_handle, vr::TrackingUniverseStanding, &poseData, sizeof(poseData), vr::k_ulInvalidInputValueHandle) != vr::VRInputError_None
        || !poseData.bActive || !poseData.pose.bPoseIsValid)
    {
        std::cerr << ""pose invalid "" << error << std::endl;
    }
    else
    {
        hand[eHand].pose = ConvertSteamVRMatrixToMatrix4(poseData.pose.mDeviceToAbsoluteTracking);
    }

I then use glm::decompose() to get the position and orientation (orientation must be conjugated). Then I try to get the forward direction from it by multiplying the orientation matrix by vec4(0,0,1,0) but the resultant vector is incorrect. Is there a flaw in my logic?
","
So it turns out I had a few issues with my methodology. Firstly, OpenVR defines the forward direction of the controllers as vec4(0,0,-1,0), and secondly, it is defined with respect to the HMD camera. In order to move around the scene, I use a second camera matrix for translation and rotation. Thus had to take this into account.
My final calculation is as follows
auto forward = glm::normalize(glm::inverse(nonHMDViewMat) *
                              vr.GetControllerPose(Right) * glm::vec4(0,0,-1,0));

Where vr.GetControllerPose(Right) returns the matrix in hand[eHand].pose for the right hand.
"
"
I'm using this Microsoft Documentation to work with Spatial meshes. https://learn.microsoft.com/en-us/windows/mixed-reality/develop/platform-capabilities-and-apis/scene-understanding-sdk
Under the ""Dealing with Transforms"" section, they have this example code :
private System.Numerics.Matrix4x4? GetSceneToUnityTransformAsMatrix4x4(SceneUnderstanding.Scene scene)


  System.Numerics.Matrix4x4? sceneToUnityTransform = System.Numerics.Matrix4x4.Identity;

  Windows.Perception.Spatial.SpatialCoordinateSystem sceneCoordinateSystem = Microsoft.Windows.Perception.Spatial.Preview.SpatialGraphInteropPreview.CreateCoordinateSystemForNode(scene.OriginSpatialGraphNodeId);
  HolograhicFrameData holoFrameData =  Marshal.PtrToStructure<HolograhicFrameData>(UnityEngine.XR.XRDevice.GetNativePtr());
  Windows.Perception.Spatial.SpatialCoordinateSystem unityCoordinateSystem = Microsoft.Windows.Perception.Spatial.SpatialCoordinateSystem.FromNativePtr(holoFrameData.ISpatialCoordinateSystemPtr);

  sceneToUnityTransform = sceneCoordinateSystem.TryGetTransformTo(unityCoordinateSystem);

  if(sceneToUnityTransform != null)
  {
      sceneToUnityTransform = ConvertRightHandedMatrix4x4ToLeftHanded(sceneToUnityTransform.Value);
  }
  else
  {
      return null;
  }

return sceneToUnityTransform;

I am unable to use the HolograhicFrameData datatype, as it cannot be found. I have searched through all of the documentation, but it explicitly says at the top : ""All of the types described below reside in the Microsoft.MixedReality.SceneUnderstanding namespace"". Searching for this type on Google only leads to the documentation I previously mentioned. I also tried using Microsoft.Windows.Graphics.Holographic; But to no avail.
What namespace includes the type HolograhicFrameData?
","
There is a typo in the name and is probably a custom class. A little mistake, the closest class I found was HolographicFrame.
The variable holoFrameData instance of HolographicFrameData class uses ISpatialCoordinateSystemPtr property, which is inherited by IWindowsMixedRealityUtilitiesProvider interface
Found another reference, but not the same HolographicFrameNativeData
It's used here as well Mixed Reality (Unity)
"
"
Basically I am working on a mixed reality experience using the Hololens2 and Unity, where the player has several physical objects they need to interact with, as well as virtual objects. One of the physical objects is a gun controller that has an IMU to detect acceleration and orientation. My main challenge is this : how do I get the physical object's position in Unity, in order to accurately fire virtual projectiles at a virtual enemy?
My current idea is to have the player position the physical weapon inside a virtual bounding box at the start of the game. I can then track the position of the virtual box through collision with the player's hands when they pick up the physical controller. Does OnCollisionEnter, or a similar method, work with the Players hands? (see attached image)

I am also looking into the use of spatial awareness / image recognition / pose estimation to accomplish this task, as well as researching the use of a tracking base station to determine object position (similar to HTC Vive / Oculus Rift ).
Any suggestions, resources, and assistance is greatly appreciated here. Thank you!
EDIT UPDATE 11/30/2020 :
Hernando commented below suggesting QR codes, assume for this project we are not allowed to use QR codes, and we want as as precise orientation data as possible. Thanks Hernando!
","
For locating the object, QR code would definitely be the recommendation to find quickly with HL2 device.  I have seen the QR approach in multiple venues too for VR LBE experiences like being described here.  QR code is just sitting on top the device.
Otherwise, if the controller in question supports Bluetooth, can possibly pair the device and if device has location information, can possible transmit the location of where it is at.  Based on what I am seeing from all of the above, this would be a custom solution and highly dependent on the controller abilities to be seen if QR codes are out of the equation.  I have witnessed some controller solutions to first start the user experience to do something like touch the floor to get an initial reference point.  Or alternatively doing something like always picking up the gun from specific location in the real world like some local based experiences do before starting.
Good luck with project, just my advice from using systems with VR
"
"
I'm trying to create a VR app for iOS using Unity and the Google Cardboard SDK.
Here is a short video about it:
https://streamable.com/vql157
I've repeated the steps from https://developers.google.com/cardboard/develop/unity/quickstart?authuser=2 a hundred times, created several projects and just don't know where this error comes from.
When I create a new Unity project and only import the Google Cardboard Sample Scene, the error also occurs. (For the sphere game object)
Has anyone already experienced this or knows how to work around this problem?
","
I haven't found a solution to work around the problem, so I decided to do without VR plugins completely, i.e. no XR management plugin and no G-Cardboard XR plugin.
My solution:
I have created two independent cameras, which are combined by the father object and thus form the main camera. Then I assigned a script to the parent object of the main camera, which reads the gyroscope data from my iPhone and thus adjusts the rotation of the camera.
"
"
I'm trying to create a VR app for iOS using Unity and the Google Cardboard SDK.
Here is a short video about it:
https://streamable.com/vql157
I've repeated the steps from https://developers.google.com/cardboard/develop/unity/quickstart?authuser=2 a hundred times, created several projects and just don't know where this error comes from.
When I create a new Unity project and only import the Google Cardboard Sample Scene, the error also occurs. (For the sphere game object)
Has anyone already experienced this or knows how to work around this problem?
","
I haven't found a solution to work around the problem, so I decided to do without VR plugins completely, i.e. no XR management plugin and no G-Cardboard XR plugin.
My solution:
I have created two independent cameras, which are combined by the father object and thus form the main camera. Then I assigned a script to the parent object of the main camera, which reads the gyroscope data from my iPhone and thus adjusts the rotation of the camera.
"
"
This is my first project using the Hololens and MRTK for Unity. I want to make a button that restarts the experience for the player when pushed.
I connected a button to a new script with the following code:
SceneManager.LoadScene(SceneManager.GetActiveScene().name);

When the button is pushed the scene seems to reload, but all MRTK/Hololens functionality stops. The hand and eye-tracking are unresponsive, and I am unable to interact with holograms.
I know I can manually make a reset method that moves all the objects in the scene and resets specific scripts, but I'm trying to completely unload and reload as much of the Unity application as possible from a push of a button.
So, how do I properly reload a Unity Scene on the Hololens2? I'm still doing research on this and will update what I find here. I greatly appreciate any assistance here, Thank you!
","
MRTK actually has a scene system that neatly wraps loading/unloading scenes as well as optional concepts like lighting.
Take a look at the docs/guides here: https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/SceneSystem/SceneSystemGettingStarted.html
"
"
I am using XR class from Unity and trying to add all button inputs to m_SelectUsage. But unfortunately it does not take an array of values. Here in the picture as you can see, only one button can be used ""Grip"". Is there a way to add all the button inputs to this class?


using UnityEngine.SpatialTracking;

#if LIH_PRESENT
using UnityEngine.Experimental.XR.Interaction;
#endif

namespace UnityEngine.XR.Interaction.Toolkit
{
    /// <summary>
    /// <see cref=""XRBaseController""/> <see cref=""MonoBehaviour""/> that interprets
    /// feature values on an input device in the XR input subsystem into
    /// XR Interaction Interactor position, rotation, and interaction states.
    /// </summary>
    [AddComponentMenu(""XR/XR Controller (Device-based)"")]
    public class XRController : XRBaseController
    {
        [SerializeField]
        [Tooltip(""The XRNode for this controller."")]
        XRNode m_ControllerNode = XRNode.RightHand;

        /// <summary>
        /// The <see cref=""XRNode""/> for this controller.
        /// </summary>
        public XRNode controllerNode
        {
            get => m_ControllerNode;
            set => m_ControllerNode = value;
        }

        [SerializeField]
        [Tooltip(""The input to use for detecting a select."")]
        InputHelpers.Button m_SelectUsage = InputHelpers.Button.Grip;

        /// <summary>
        /// The input to use for detecting a select.
        /// </summary>
        public InputHelpers.Button selectUsage
        {
            get => m_SelectUsage;
            set => m_SelectUsage = value;
        }

        [SerializeField]
        [Tooltip(""The input to use for detecting activation."")]
        InputHelpers.Button m_ActivateUsage = InputHelpers.Button.Trigger;

        /// <summary>
        /// The input to use for detecting activation.
        /// </summary>
        public InputHelpers.Button activateUsage
        {
            get => m_ActivateUsage;
            set => m_ActivateUsage = value;
        }

        [SerializeField]
        [Tooltip(""The input to use for detecting a UI press."")]
        InputHelpers.Button m_UIPressUsage = InputHelpers.Button.Trigger;

        /// <summary>
        /// The input to use for detecting a UI press.
        /// </summary>
        public InputHelpers.Button uiPressUsage
        {
            get => m_UIPressUsage;
            set => m_UIPressUsage = value;
        }

        [SerializeField]
        [Tooltip(""The amount an axis needs to be pressed to trigger an interaction event."")]
        float m_AxisToPressThreshold = 0.1f;

        /// <summary>
        /// The amount an axis needs to be pressed to trigger an interaction event.
        /// </summary>
        public float axisToPressThreshold
        {
            get => m_AxisToPressThreshold;
            set => m_AxisToPressThreshold = value;
        }

        [SerializeField]
        [Tooltip(""The input to use to rotate an anchor to the Left."")]
        InputHelpers.Button m_RotateAnchorLeft = InputHelpers.Button.PrimaryAxis2DLeft;

        /// <summary>
        /// The input to use to rotate an anchor to the Left.
        /// </summary>
        public InputHelpers.Button rotateObjectLeft
        {
            get => m_RotateAnchorLeft;
            set => m_RotateAnchorLeft = value;
        }

        [SerializeField]
        [Tooltip(""The input to use to rotate an anchor to the Right."")]
        InputHelpers.Button m_RotateAnchorRight = InputHelpers.Button.PrimaryAxis2DRight;

        /// <summary>
        /// The input to use to rotate an anchor to the Right.
        /// </summary>
        public InputHelpers.Button rotateObjectRight
        {
            get => m_RotateAnchorRight;
            set => m_RotateAnchorRight = value;
        }

        [SerializeField]
        [Tooltip(""The input that will be used to translate the anchor away from the interactor."")]
        InputHelpers.Button m_MoveObjectIn = InputHelpers.Button.PrimaryAxis2DUp;

        /// <summary>
        /// The input that will be used to translate the anchor away from the interactor.
        /// </summary>
        public InputHelpers.Button moveObjectIn
        {
            get => m_MoveObjectIn;
            set => m_MoveObjectIn = value;
        }

        [SerializeField]
        [Tooltip(""The input that will be used to translate the anchor towards the interactor."")]
        InputHelpers.Button m_MoveObjectOut = InputHelpers.Button.PrimaryAxis2DDown;

        /// <summary>
        /// The input that will be used to translate the anchor towards the interactor.
        /// </summary>
        public InputHelpers.Button moveObjectOut
        {
            get => m_MoveObjectOut;
            set => m_MoveObjectOut = value;
        }

#if LIH_PRESENT
        [SerializeField, Tooltip(""Pose provider used to provide tracking data separate from the XR Node."")]
        BasePoseProvider m_PoseProvider;

        /// <summary>
        /// Pose provider used to provide tracking data separate from the <see cref=""XRNode""/>.
        /// </summary>
        public BasePoseProvider poseProvider
        {
            get => m_PoseProvider;
            set => m_PoseProvider = value;
        }
#endif

        InputDevice m_InputDevice;
        /// <summary>
        /// (Read Only) The <see cref=""InputDevice""/> being used to read data from.
        /// </summary>
        public InputDevice inputDevice => m_InputDevice.isValid ? m_InputDevice : m_InputDevice = InputDevices.GetDeviceAtXRNode(controllerNode);

        /// <inheritdoc />
        protected override void UpdateTrackingInput(XRControllerState controllerState)
        {
            controllerState.poseDataFlags = PoseDataFlags.NoData;
#if LIH_PRESENT_V1API
            if (m_PoseProvider != null)
            {
                if (m_PoseProvider.TryGetPoseFromProvider(out var poseProviderPose))
                {
                    controllerState.position = poseProviderPose.position;
                    controllerState.rotation = poseProviderPose.rotation;
                    controllerState.poseDataFlags = PoseDataFlags.Position | PoseDataFlags.Rotation;
                }
            }
            else
#elif LIH_PRESENT_V2API
            if (m_PoseProvider != null)
            {
                var retFlags = m_PoseProvider.GetPoseFromProvider(out var poseProviderPose);
                if ((retFlags & PoseDataFlags.Position) != 0)
                {
                    controllerState.position = poseProviderPose.position;
                    controllerState.poseDataFlags |= PoseDataFlags.Position;
                }
                if ((retFlags & PoseDataFlags.Rotation) != 0)
                {
                    controllerState.rotation = poseProviderPose.rotation;
                    controllerState.poseDataFlags |= PoseDataFlags.Rotation;
                }
            }
            else
#endif
            {
                if (inputDevice.TryGetFeatureValue(CommonUsages.devicePosition, out controllerState.position))
                {
                    controllerState.poseDataFlags |= PoseDataFlags.Position;
                }

                if (inputDevice.TryGetFeatureValue(CommonUsages.deviceRotation, out controllerState.rotation))
                {
                    controllerState.poseDataFlags |= PoseDataFlags.Rotation;
                }
            }
        }

        /// <inheritdoc />
        protected override void UpdateInput(XRControllerState controllerState)
        {
            controllerState.ResetFrameDependentStates();
            
            HandleInteractionAction(m_SelectUsage, ref controllerState.selectInteractionState);
            HandleInteractionAction(m_ActivateUsage, ref controllerState.activateInteractionState);
            HandleInteractionAction(m_UIPressUsage, ref controllerState.uiPressInteractionState);
        }

        void HandleInteractionAction(InputHelpers.Button button, ref InteractionState interactionState)
        {
            inputDevice.IsPressed(button, out var pressed, m_AxisToPressThreshold);

            if (pressed)
            {
                Debug.Log(""Detect"");
                if (!interactionState.active)
                {
                    interactionState.activatedThisFrame = true;
                    interactionState.active = true;
                }
            }
            else
            {
                if (interactionState.active)
                {
                    interactionState.deactivatedThisFrame = true;
                    interactionState.active = false;
                }
            }
        }

        /// <inheritdoc />
        public override bool SendHapticImpulse(float amplitude, float duration)
        {
            if (inputDevice.TryGetHapticCapabilities(out var capabilities) &&
                capabilities.supportsImpulse)
            {
                return inputDevice.SendHapticImpulse(0u, amplitude, duration);
            }
            return false;
        }

    }
}

","
You can overwrite XRController to accept an array for each of the input values.
using UnityEngine;
using UnityEngine.XR.Interaction.Toolkit;


public class MyXRController : XRController
{
    [SerializeField]
    InputHelpers.Button[] m_SelectUsageArray;

    protected override void UpdateInput(XRControllerState controllerState)
    {
        // Because we use base input m_SelectUsage will still be accepted if provided
        base.UpdateInput(controllerState);
        
        // Iterate over all buttons
        foreach (var selectUsage in m_SelectUsageArray)
        {
            HandleInteractionAction(selectUsage, ref controllerState.selectInteractionState);
        }
    }
}

You can overwrite other interactions in a similar manner.
Then in your GameObject, use MyXRController instead of XRController
"
"
I have my camera and cursor set up this way:
    <a-camera>
      <a-cursor 
        fuse=""true""
        animation__fusing=""property: scale; startEvents: fusing; easing: easeInCubic; dur: 1500; from: 1 1 1; to: 0.1 0.1 0.1""
        animation__mouseleave=""property: scale; startEvents: mouseleave; easing: easeInCubic; dur: 500; to: 1 1 1"">
      </a-cursor>
    </a-camera>

And a scene looking like this:
    <a-entity id=""scene1"">
      <a-box position=""-1 0.5 -3"" rotation=""0 45 0"" color=""#4CC3D9"" onclick=""changeLevel('scene1', 'scene2')""></a-box>
      <a-sky src=""background1.png""></a-sky>
    </a-entity>

Now the cursor is always fusing, even when looking at the skybox. Is it possible to start the fusing animation only when looking at clickable entities?
Thanks
","
You can configure the cursor through the raycaster component. In your case:
  <a-cursor fuse=""true"" raycaster=""objects: a-box"" ....

You can set a class for your interactive objects, to make it clear:
  <a-box class=""clickable""></a-box>
  ...
  <a-cursor raycaster=""objects: .clickable"" ...

Check it out here.
"
"
I am making an Android app for Unity, using my own VR engine. It is small, and I have got looking around working perfectly. The only problem I am experiencing is where I cannot get my eyes to focus on the objects in front of them. I get double vision where my left eye sees objects too far to the left, and right eye to the right. I have tried pointing the eye cameras slightly inwards and moving them based on a raycast to find out where they are looking.
I am guessing it could be something to do with pointing the eyes outwards, my rig - nintendo labo headset with android phone inside [ making do with what I've got ;) ] - unfortunately the phone and lenses don't quite line up but this doesn't seem to affect one of my other projects, or perhaps I need to distort my camera in a special way.

Honestly, I have no idea! Some help from an expert or anyone who is slightly clued up in the subject would be greatly appreciated :D
","
It turns out I literally just need to point the cameras out rather than in
"
"
Trying to use my RealityKit project as the foundation for an on screen app (VR) instead of projecting onto the real-world (AR) out the back camera.
Anyone know how to load a RealityKit project asynchronously with the .nonAR camera option, so it project in an app instead of leveraging the rear facing camera?
Do I create position information in the Swift code or the Reality Composer project?
","
Here's how you can asynchronously load .usdz VR-model with a help of RealityKit's .loadModelAsync() instance method and Combine's AnyCancellable type.
import UIKit
import RealityKit
import Combine

class VRViewController: UIViewController {
    
    @IBOutlet var arView: ARView!
    var anyCancellable: AnyCancellable? = nil
    let anchorEntity = AnchorEntity(world: [0, 0,-2])
    
    override func viewWillAppear(_ animated: Bool) {
        super.viewWillAppear(animated)
        arView.backgroundColor = .black
        arView.cameraMode = .nonAR

        anyCancellable = ModelEntity.loadModelAsync(named: ""horse"").sink(
                        
            receiveCompletion: { _ in
                self.anyCancellable?.cancel()
            },
            
            receiveValue: { [self] (object: Entity) in
                if let model = object as? ModelEntity {
                    self.anchorEntity.addChild(model)
                    self.arView.scene.anchors.append(self.anchorEntity)
                } else {
                    print(""Can't load a model due to some issues"")
                }
            }
        )
    }
}

However, if you wanna move inside 3D environment, instead of using .nonAR camera mode use:
arView.environment.background = .color(.black)

"
"
I am an inexperienced programmer, I am looking for advice on a new unity project:
I need to generate terrain for a 3d game from fairly large tiles. For now I only need one type of tile, but I was thinking, I better set up a registry system now and dynamically generate that default tile in an infinite grid. I have a few concerns though, like will the objects continue to load as the character moves into the render distance of a new tile (or chunk if you rather). Also, all the tutorials I have found are wrong for me in some way, like it only works in 2d and doesn't have collision, or is just a static registry and does not allow for changing the content of the tiles in-game.
Right now I don't even know what the code looks like to place a 3d object in the scene without building them from vectors, which maybe I could do. I also don't know how I would want to trigger the code.
Could someone give me an idea of  what the code would look like / terminology to look up / a tutorial that gives me what I need?
","
This looks like a pretty big scope for a new programmer but lets give it a shot. Generating terrain will be a large learning experience when it comes to performance and optimization when you don't know what you're doing.
First off, you'll probably want to make a script that acts as a controller for generating your objects and put this inside of the player. I would start by only making a small area, or one chunk, generate and then move on to making multiple chunks generate when you understand what you're doing. To 'place' an object in your scene you will want to make an instance of the object. I'd start by trying to make your grid of objects, this can be done pretty easily on initialization (Start() function) through a for loop, for testing purposes. IE, if you are trying to make 16x16 squares like minecraft; have a for loop that runs 16 times (For the x) and a for loop inside of that to run 16 times (for the z). That way you can make a complete square of, in this case, cubes. Here is some very untested code just to give you an example of what I'm talking about.
public GameObject cube; //Cube you want to make a copy of, this will appear in the editor
void Start(){
    for(var x=0; x < 16; x++){
        for(var z=0; z < 16; z++){
           GameObject newCube = Instantiate(cube); //Creates an instance of the 'cube' object, think of this like a copy.
           newCube.transform.position = new Vector3(x, 0, z); //Places the cube on the x and z which is updated in the for loops
        }
    }
}

Now where you go from here will be very different depending on what you're trying to do exactly but you can start by looking into perlin noise to add in a randomized y level that looks good. It's very easy to use once you grasp the general concept and this example I provided should help you understand how to use it. They even give good examples of how to use this on the Unity docs.
In all, programming is all about learning. You'll have to learn how to only take the parts of resources that you need for what you're trying to create. I think what I provided you should give you a good start on what you want to create but it will take a deeper understanding to carry things out on your own. Just test different things and truly try and understand how they work and you'll be able to implement parts of them into your own project.
I hope this helps, good luck!
"
"
I am new at networking so i have a lot of questions also i have a dream about making an open-source and more affordable VR headset and software using rasberry pi so nowadays i am curious about how does popular systems like SteamVR and HTC Vive work . I couldn't really find any documentation online so i decided to ask it here.
My question is shortly how does a vr headset send data (tracking , button clicks etc.) to SteamVR and how does games communucate SteamVR and get this data?
I am not an expert so if you will describe it with python terms it will be way more easier for me to understand.
","
Generally there's not really TCP style networking going on. There's some low level embedded computation going on to determine where controllers and the headset are. Those results along with button presses and trigger pulls are sent through a low level API through OpenVR where XR toolkits abstract all those inputs and allow developers to build applications.
These toolkits include MRTK: https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/GettingStartedWithTheMRTK.html
VRTK: https://assetstore.unity.com/packages/tools/integration/vrtk-virtual-reality-toolkit-vr-toolkit-64131
If you want to dive into how the OpenVR API works then you're free to dig into the
SteamVR plugin: https://github.com/ValveSoftware/openvr
OpenVR is counter intuitively not open source so you can't see how they're actually getting and processing the raw inputs.
OpenXR however is open source and is a open standard by Khronos so you can look into how inputs are handled: https://github.com/KhronosGroup/OpenXR-SDK
"
"
I'm new to VR development,I'm using oculus integration so I made a ray line with the line renderer , but now I need to make this line interactive with everything like a gaze pointer (objects,UI, Scrolls..),, I'm testing my project on Oculus Go

I have tried a lot of tutorials but no one focuses on this point.
Here's my attempt at the code:
RaycastHit hit;
if(Physics.Raycast(transform.position,transform.forward ,out hit,0.8f))
{
    if(hit.collider != null)
    {
        if (OVRInput.Get(OVRInput.Button.PrimaryIndexTrigger)) 
        { 
            go = hit.transform.gameObject;
            go.SetActive(false);
        }
    }
}

This works for objects but not with UI like how a GazePointer works.
","
I was also interessted in something like this so I got a bit deeper into the mentioned SteamVR Laser Pointer Menus.
The general idea is quite simple: 

On all interactable UI use BoxCollider
Then you can simply stick to Physics.Raycast
If pointing on a UI element additionally fire the according EventSystems.ExecuteEvents

In reality this looks a bit more complex. I created my own script based on the steamvr_laserpointer.cs but had to change it a lot in order to cover most of the interactions.
It is not fully working yet - while it basicaly already works perfect for 3D objects and simple button interactions I have a little trouble getting the dragging to work properly. But I hope this is a good startpoint for everyone trying something similar:
LaserPointerController.cs
(Originally based on steamvr_laserpointer.cs)
This is attached to your controller GameObject. It will create a child object ""LaserHolder"" which also has a child ""LaserPointer"". The ""LaserPointer"" is basically simply a cube which is scaled so it looks like a laser beam.-
#define USE_CUSTOM_EDITOR

using System.Linq;
using UnityEngine;
using UnityEngine.EventSystems;
#if UNITY_EDITOR && USE_CUSTOM_EDITOR
using UnityEditor;
#endif

public class LaserPointerController : MonoBehaviour
{
    #region Inspector

    [Header(""Pointer Settings"")]
    [Tooltip(""Should the LaserPointer origin have an offset to the controller position (in local coordinates)?"")]
    [SerializeField] private bool useOffset;
    [SerializeField] private Vector3 offset = Vector3.zero;

    [Tooltip(""The maximal distance of the laser pointer (in Unity Units)"")]
    [SerializeField] private float maxPointerDistance = 5f;

    [Tooltip(""Should the laser have a Collider attached?\nUseful for e.g. directly using OnTriggerEnter\n\nNote: This will use Physics.RaycastAll which is less efficient."")]
    [SerializeField] private bool useCollider;
    [Tooltip(""Should the laser Collider be a trigger?\nHas to be disabled to e.g. use OnCollisionEnter"")]
    [SerializeField] private bool colliderIsTrigger;
    [Tooltip(""Should the laser additionally have a RigidBody?\nRequired for some types of collisions."")]
    [SerializeField] private bool useRigidBody;

    [Header(""Style Settings"")]
    [Tooltip(""Should the laser change its Color when pressed"")]
    [SerializeField] private bool changeColor;
    [Tooltip(""Should the laser change its thickness when pressed"")]
    [SerializeField] private bool changeThickness;
    [Tooltip(""Color of the Laser in idle"")]
    [SerializeField] private Color idleColor = Color.red;
    [Tooltip(""Color of the beam when pressed"")]
    [SerializeField] private Color pressedColor = Color.green;
    [Tooltip(""Thickness of the laser beam"")]
    [SerializeField] private float idleThickness = 0.005f;
    [Tooltip(""Thickness of the laser beam when pressed"")]
    [SerializeField] private float pressedThickness = 0.007f;

    [Header(""Debug"")]
    [SerializeField] private bool isPressed;
    [SerializeField] private Vector3 pointerPosition3D;
    [SerializeField] private Vector2 pointerPosition2D;

#if UNITY_EDITOR && USE_CUSTOM_EDITOR
    [CustomEditor(typeof(LaserPointerController))]
    private class LaserPointerControllerEditor : Editor
    {
        private SerializedProperty useOffset;
        private SerializedProperty offset;

        private SerializedProperty maxPointerDistance;

        private SerializedProperty useCollider;
        private SerializedProperty colliderIsTrigger;
        private SerializedProperty useRigidBody;

        private SerializedProperty changeColor;
        private SerializedProperty changeThickness;
        private SerializedProperty idleColor;
        private SerializedProperty pressedColor;
        private SerializedProperty idleThickness;
        private SerializedProperty pressedThickness;

        private SerializedProperty _isPressed;
        private SerializedProperty pointerPosition3D;
        private SerializedProperty pointerPosition2D;

        private void OnEnable()
        {
            useOffset = serializedObject.FindProperty(""useOffset"");
            offset = serializedObject.FindProperty(""offset"");

            maxPointerDistance = serializedObject.FindProperty(""maxPointerDistance"");
            useCollider = serializedObject.FindProperty(""useCollider"");
            colliderIsTrigger = serializedObject.FindProperty(""colliderIsTrigger"");
            useRigidBody = serializedObject.FindProperty(""useRigidBody"");

            changeColor = serializedObject.FindProperty(""changeColor"");
            changeThickness = serializedObject.FindProperty(""changeThickness"");
            idleColor = serializedObject.FindProperty(""idleColor"");
            pressedColor = serializedObject.FindProperty(""pressedColor"");
            idleThickness = serializedObject.FindProperty(""idleThickness"");
            pressedThickness = serializedObject.FindProperty(""pressedThickness"");

            _isPressed = serializedObject.FindProperty(""isPressed"");
            pointerPosition3D = serializedObject.FindProperty(""pointerPosition3D"");
            pointerPosition2D = serializedObject.FindProperty(""pointerPosition2D"");
        }

        public override void OnInspectorGUI()
        {
            serializedObject.Update();

            DrawScriptField();

            EditorGUILayout.PropertyField(useOffset);
            if (useOffset.boolValue)
            {
                EditorGUILayout.PropertyField(offset);
                EditorGUILayout.Space();
            }

            EditorGUILayout.PropertyField(maxPointerDistance);

            EditorGUILayout.PropertyField(useCollider);
            if (useCollider.boolValue)
            {
                EditorGUILayout.PropertyField(colliderIsTrigger);
                EditorGUILayout.PropertyField(useRigidBody);
            }
            else
            {
                colliderIsTrigger.boolValue = true;
                useRigidBody.boolValue = false;
            }

            EditorGUILayout.PropertyField(changeColor);
            EditorGUILayout.PropertyField(changeThickness);

            EditorGUILayout.PropertyField(idleColor);
            if (changeColor.boolValue) EditorGUILayout.PropertyField(pressedColor);

            EditorGUILayout.PropertyField(idleThickness);
            if (changeThickness.boolValue) EditorGUILayout.PropertyField(pressedThickness);

            if (EditorApplication.isPlayingOrWillChangePlaymode)
            {
                EditorGUILayout.PropertyField(_isPressed);

                EditorGUI.BeginDisabledGroup(true);
                {
                    EditorGUILayout.PropertyField(pointerPosition3D);
                    EditorGUILayout.PropertyField(pointerPosition2D);
                }
                EditorGUI.EndDisabledGroup();
            }

            serializedObject.ApplyModifiedProperties();
        }

        private void DrawScriptField()
        {
            EditorGUI.BeginDisabledGroup(true);
            {
                EditorGUILayout.ObjectField(""Script"", MonoScript.FromMonoBehaviour((LaserPointerController)target), typeof(LaserPointerController), false);
            }
            EditorGUI.EndDisabledGroup();

            EditorGUILayout.Space();
        }
    }
#endif

    #endregion Inspector


    #region Public Members

    public Vector3 PointerPosition3D
    {
        get { return pointerPosition3D; }
    }

    public Vector2 PointerPosition2D
    {
        get { return pointerPosition2D; }
    }

    public bool IsPressed
    {
        get { return isPressed; }
    }

    public Transform CurrentlyHoveredTransform
    {
        get { return _currentlyHovered; }
    }

    #endregion Public Members


    #region Private Members

    private Transform _holder;
    private Transform _laser;
    private Transform _currentlyHovered;
    private Material _laserMaterial;

    private PointerEventData _pointerEventData;
    private Vector2 _lastPointerPosition;
    private bool _isDragging;
    private bool _lastPressed;
    private Camera _camera;

    #endregion Private Members


    #region MonoBehaviour Messages
    private void Awake()
    {
        _holder = new GameObject(""LaserHolder"").transform;
        _holder.parent = transform;
        _holder.localScale = Vector3.one;
        _holder.localPosition = useOffset ? offset : Vector3.zero;
        _holder.localRotation = Quaternion.identity;

        _laser = GameObject.CreatePrimitive(PrimitiveType.Cube).transform;
        var collider = _laser.GetComponent<Collider>();

        if (!useCollider)
        {
            Destroy(_laser.GetComponent<Collider>());
        }
        else
        {
            collider.isTrigger = colliderIsTrigger;
        }

        if (useRigidBody)
        {
            var rigidBody = _laser.gameObject.AddComponent<Rigidbody>();
            rigidBody.useGravity = false;
            rigidBody.isKinematic = true;
        }

        _laser.name = ""LaserPointer"";
        _laser.transform.parent = _holder;
        _laser.transform.localScale = new Vector3(idleThickness, idleThickness, maxPointerDistance);
        _laser.transform.localPosition = new Vector3(0f, 0f, maxPointerDistance / 2f);
        _laser.transform.localRotation = Quaternion.identity;

        _laserMaterial = new Material(Shader.Find(""Unlit/Color"")) { color = idleColor };
        _laser.GetComponent<MeshRenderer>().material = _laserMaterial;

        _camera = Camera.main;

        _pointerEventData = new PointerEventData(EventSystem.current)
        {
            // most Unity comonents e.g. the ScrollRect expect this to be Left
            // otherwise they ignore pointer input
            button = PointerEventData.InputButton.Left
        };
    }

    private void OnEnable()
    {
        _holder.gameObject.SetActive(true);
    }

    private void OnDisable()
    {
        _holder.gameObject.SetActive(false);
    }

    // Update is called once per frame
    private void Update()
    {
        //TODO uncomment this in order to get the isPressed from the according controller
        // curently I only set it via the Inspector
        //isPressed = OVRInput.Get(OVRInput.Button.PrimaryIndexTrigger);

        // Initially the dist is the maximum pointer distance
        var dist = maxPointerDistance;

        // create the ray from the laserpointer origin
        var ray = new Ray(_holder.position, _holder.forward);

        var hit = new RaycastHit();

        // If using a Collider we have to ignore the Collider of the LaserPointer itself
        if (useCollider)
        {
            var hits = Physics.RaycastAll(ray, maxPointerDistance);
            hit = hits.FirstOrDefault(h => h.transform != _laser.transform);
        }
        else
        {
            Physics.Raycast(ray, out hit, maxPointerDistance);
        }

        // Are we hitting something?
        if (hit.transform)
        {
            // ignore if still pressing the trigger
            if (!isPressed)
            {
                // Are we hitting something different to what we hit before?
                if (_currentlyHovered && _currentlyHovered != hit.transform)
                {
                    ExecuteEvents.ExecuteHierarchy(_currentlyHovered.gameObject, _pointerEventData, ExecuteEvents.pointerExitHandler);

                    _currentlyHovered = null;
                }

                // Are we hitting something new?
                if (_currentlyHovered != hit.transform)
                {
                    ExecuteEvents.ExecuteHierarchy(hit.transform.gameObject, _pointerEventData, ExecuteEvents.pointerEnterHandler);

                    _currentlyHovered = hit.transform;
                }
            }

            if (_currentlyHovered == hit.transform)
            {
                // If we are hitting something correct the dist value
                if (hit.distance < maxPointerDistance)
                {
                    dist = hit.distance;
                }

                if (isPressed)
                {
                    HandlePointerDown(hit.transform);
                }
                else
                {
                    HandlePointerUp(hit.transform);
                }
            }
        }
        else
        {
            if (!isPressed && _currentlyHovered)
            {
                HandlePointerUp(_currentlyHovered, true);

                ExecuteEvents.ExecuteHierarchy(_currentlyHovered.gameObject, _pointerEventData, ExecuteEvents.pointerExitHandler);

                _currentlyHovered = null;
            }
        }

        // Apply changes in the thickness and set the laser dimensions using the dist
        var thickness = !changeThickness || !isPressed ? idleThickness : pressedThickness;
        _laser.transform.localScale = new Vector3(thickness, thickness, dist);
        _laser.transform.localPosition = new Vector3(0f, 0f, dist / 2f);

        // Apply color changes
        if (changeColor) _laserMaterial.color = isPressed ? pressedColor : idleColor;

        // Update the pointerEventData
        pointerPosition3D = _laser.transform.position + _laser.transform.forward * dist / 2f;
        pointerPosition2D = _camera.WorldToScreenPoint(pointerPosition3D);
        _pointerEventData.position = pointerPosition2D;
        _pointerEventData.delta = _pointerEventData.position - _lastPointerPosition;
        _lastPointerPosition = _pointerEventData.position;

        _lastPressed = isPressed;
    }

    #endregion MonoBehaviour Messages


    #region Private Methods

    private void HandlePointerDown(Component target)
    {
        if (!target) return;

        // only call once if IsPressed changed its value this frame
        if (_lastPressed != isPressed)
        {
            // Try pointerClickHandler, only use submitHandler as fallback
            // e.g. UI.Button implements both ... would lead to duplicate clicks
            var done = ExecuteEvents.ExecuteHierarchy(target.gameObject, _pointerEventData, ExecuteEvents.pointerClickHandler);
            if (!done) ExecuteEvents.ExecuteHierarchy(target.gameObject, _pointerEventData, ExecuteEvents.submitHandler);

            ExecuteEvents.ExecuteHierarchy(target.gameObject, _pointerEventData, ExecuteEvents.pointerDownHandler);
            ExecuteEvents.ExecuteHierarchy(target.gameObject, _pointerEventData, ExecuteEvents.selectHandler);
            ExecuteEvents.ExecuteHierarchy(target.gameObject, _pointerEventData, ExecuteEvents.initializePotentialDrag);
        }
        else
        {
            if (!_isDragging)
            {
                // in the first frame use beginDragHandler
                ExecuteEvents.ExecuteHierarchy(target.gameObject, _pointerEventData, ExecuteEvents.beginDragHandler);
                _isDragging = true;
                _pointerEventData.dragging = true;
            }
            else
            {
                // later use dragHandler
                ExecuteEvents.ExecuteHierarchy(target.gameObject, _pointerEventData, ExecuteEvents.dragHandler);
            }
        }
    }

    private void HandlePointerUp(Component target, bool forceInvoke = false)
    {
        if (!target) return;

        // only call once if IsPressed changed its value this frame
        // or forceInvoke is set
        if (_lastPressed != isPressed || forceInvoke)
        {
            ExecuteEvents.ExecuteHierarchy(target.gameObject, _pointerEventData, ExecuteEvents.pointerUpHandler);
            ExecuteEvents.ExecuteHierarchy(target.gameObject, _pointerEventData, ExecuteEvents.deselectHandler);
            ExecuteEvents.ExecuteHierarchy(target.gameObject, _pointerEventData, ExecuteEvents.endDragHandler);
            ExecuteEvents.ExecuteHierarchy(target.gameObject, _pointerEventData, ExecuteEvents.dropHandler);

            _isDragging = false;
            _pointerEventData.dragging = false;
        }
    }

    #endregion Private Methods
}

VRUIItem.cs
(Based on VRUIItem.cs but added the Update so dynamic UI like e.g. the Handles of the ScrollBar in the ScrollRect gets its Collider updated)
This has to be placed on basically every UI element that should be interactable for the laser
using UnityEngine;

[RequireComponent(typeof(RectTransform))]
public class VRUIItem : MonoBehaviour
{
    private BoxCollider _boxCollider;
    private RectTransform _rectTransform;

    private void OnEnable()
    {
        ValidateCollider();
    }

    private void Update()
    {
        ValidateCollider();
    }

    private void OnValidate()
    {
        ValidateCollider();
    }

    private void ValidateCollider()
    {
        if (!_rectTransform) _rectTransform = GetComponent<RectTransform>();

        if (!_boxCollider) _boxCollider = GetComponent<BoxCollider>();
        if (!_boxCollider) _boxCollider = gameObject.AddComponent<BoxCollider>();

        _boxCollider.size = _rectTransform.rect.size;
    }
}

Interactable3D.cs
An example of how you could use the IPointerXYHandler interfaces in order to create interactive 3D GameObjects
using System;
using UnityEngine;
using UnityEngine.EventSystems;

public class Interactable3D : MonoBehaviour, IPointerEnterHandler, IPointerExitHandler, IPointerClickHandler
{
    public Color normalColor;
    public Color hoverColor;

    private Renderer renderer;

    private void Awake()
    {
        renderer = GetComponent<Renderer>();
        renderer.material.color = normalColor;
    }

    public void OnPointerEnter(PointerEventData eventData)
    {
        Debug.Log("" ENTEREd!"");
        transform.localScale *= 1.2f;
        renderer.material.color = hoverColor;
    }

    public void OnPointerExit(PointerEventData eventData)
    {
        Debug.Log(""EXIT!"");
        transform.localScale /= 1.2f;
        renderer.material.color = normalColor;
    }

    public void OnPointerClick(PointerEventData eventData)
    {
        Debug.Log(""CLICKED!"");
    }
}


Demo
As said the scrolling doesn't work quite correct yet, however you can see it is basically possible - maybe just something is wrong about the pointer coordinates I pass in via the _pointerEventData.position - but all other interactions already work:

"
"
I am new to VR application development with unity using SteamVR. I haven't the capability to buy an expensive headset for my studies. So Can I test my unity application with RiftCat.
","
The Windows Mixed Reality Simulator allows you to run UWP and SteamVR applications and use virtual motion controllers.
https://learn.microsoft.com/en-us/windows/mixed-reality/using-the-windows-mixed-reality-simulator
"
"
How do I call a function from a Unity (XR Interaction Toolkit) script that's appended to a Ray Interactor? Precisely what I have is a GameObject made as an XR Ray Interactor and appended the XR Direct Interactor script. I want to call the function GetValidTargets from that script to see with what GameObject it targets. How do I call this function in another script? I already have the GameObject specified and tried different methods to implement it, for example GetComponent<>().
This is what the function says in the script so that comes after I call it.
        /// Retrieve the list of interactables that this interactor could possibly interact with this frame.
        /// This list is sorted by priority (in this case distance).
        /// <param name=""validTargets"">Populated List of interactables that are valid for selection or hover.</param>
        public override void GetValidTargets(List<XRBaseInteractable> validTargets)

How do I call this function?
","
Calling the function if you want to call the function, append the XRBaseInteractable component to every GameObject that you want to able to hover or interact with. Create a List (which are in the System.Collections.Generic namespace), add each XRBaseInteractable instance to it and pass it in to the GetValidTargets() function.
GameObject[] interactableObjects; // Set in inspector or somewhere in code
List<XRBaseInteractable> interactables = new List<XRBaseInteractable>();
foreach(GameObject interactableObject in interactableObjects)
{
    interactables.Add(interactableObject.GetComponent<XRBaseInteractable>());
}
GetComponent<XRDirectInteractor>().GetValidTargets(interactables);

"
"
Odd question, but I'm having trouble boiling it down to a coherent question.
I have sampled data (60Hz) from Brekel OpenVR recorder, which includes the following parameters for the HMD itself:

X, Y, Z positional coordinates
Euler angles rotX, rotY, rot.

I'm processing the data in python. Overall, what I'm trying to do is to calculate measures of mobility: did someone look around more or less during a segment, and did they move around more or less?
For the positional coordinates that wasn't too difficult, I was able to calculate displacement, velocity, etc., by using the distances between subsequent positions. Now for the Euler angles, I'm having more trouble.
I've searched for the answer to my question, but none of the answer seemed to 'click'. What I think I need, is to convert the Euler angles to a directional vector, and then calculate the angle between the directional vectors of subsequent samples to see how much the gaze direction shifted. Once I have those, I can calculate means and SDs per subject, to see which of them looked around more (that's the idea anyway). I am unclear on the mathematics though. It would have been easier if my coordinates were roll, pitch, yaw, but I'm struggling with the Euler angles.
Suppose the Euler angles for two subsequent samples are:

(rotX, rotY, rot) = (20°, 25°, 50°)
(rotX2, rotY2, rot2) = (30°, 35°, 60°)

How can I quantify with what angle the direction of the HMD changed between those two samples?
","
You can write a function to convert Euler angles to unit vectors, and another to take the angle between two unit vectors
from math import cos, sin, atan2, degrees, radians, sqrt
from numpy import dot, cross

def to_unit_vector(pitch, yaw, roll):
    # Convert degrees to radians
    pitch = radians(pitch)
    yaw = radians(yaw)
    roll = radians(roll)
    # Calculate unit vector components
    x = cos(yaw) * cos(pitch)
    y = sin(yaw) * cos(pitch)
    z = sin(pitch)
    return x, y, z

def angle_between(a, b):
    # returns angle in degrees
    return degrees(atan2(sqrt(dot(cross(a, b), cross(a, b))), dot(a, b)))

Then for example
>>> u = to_unit_vector(20, 25, 50)
>>> u
(0.8516507396391465, 0.39713126196710286, 0.3420201433256687)
>>> v = to_unit_vector(30, 35, 60)
>>> v
(0.7094064799162225, 0.49673176489215404, 0.49999999999999994)
>>> angle_between(u, v)
13.481723224800309

"
"
Processing was designed to make drawing with Java much easier.  Processing for Android has the power of its desktop sibling plus information from sensors.  Putting these things together, shouldn't it be easy to display a stereoscopic image and move around it like Oculus Rift or Google Cardboard?
","
The code below displays an image in two viewports - one for the left eye and one for the right eye. The result is that the image looks 3D when viewed from a Google Cardboard device.  Accelerometer and gyroscope data are used to move the 3D image as the head is moved around.  The only bug is that of Processing for Android in that Landscape mode makes the program crash if you do not start it in this mode. I am using Processing 2.0.3 and Android 4.3, so this problem may have been addressed in current versions. (Although I did see it was still an open issue in Processing-Bugs discussion on Github). The texture image is a 100 x 100 pixel image of a favorite cartoon character. You can use whatever you want – just store the image in the data folder. 
//Scott Little 2015, GPLv3
//pBoard is Processing for Cardboard

import android.os.Bundle; //for preventing sleep
import android.view.WindowManager;
import ketai.sensors.*; //ketai library for sensors
KetaiSensor sensor;

float ax,ay,az,mx,my,mz; //sensor variables
float eyex = 50; //camera variables
float eyey = 50;
float eyez = 0;
float panx = 0;
float pany = 0;
PGraphics lv; //left viewport
PGraphics rv; //right viewport
PShape s; //the object to be displayed

//********************************************************************
// The following code is required to prevent sleep.
//********************************************************************
void onCreate(Bundle savedInstanceState) {
super.onCreate(savedInstanceState);
// fix so screen doesn't go to sleep when app is active
getWindow().addFlags(WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON);
}
//********************************************************************

void setup() {
sensor = new KetaiSensor(this);
sensor.start();

size(displayWidth,displayHeight,P3D); //used to set P3D renderer
orientation(LANDSCAPE); //causes crashing if not started in this orientation

lv = createGraphics(displayWidth/2,displayHeight,P3D); //size of left viewport
rv = createGraphics(displayWidth/2,displayHeight,P3D);

PImage img = loadImage(""jake.jpg"");  //texture image
s = createShape();
TexturedCube(img, s, 50, 50);
}

void draw(){
//draw something fancy on every viewports
panx = panx-mx*10;
pany = 0;
eyex = 0;
eyey = -20*az;

ViewPort(lv, eyex, eyey, panx, pany, -15); //left viewport
ViewPort(rv, eyex, eyey, panx, pany, 15);  //right viewport

//add the two viewports to your main panel
image(lv, 0, 0);
image(rv, displayWidth/2, 0);
}

//sensor data
void onAccelerometerEvent(float x, float y, float z){
ax = x;
ay = y;
az = z;
}

void onGyroscopeEvent(float x, float y, float z){
mx = x;
my = y;
mz = z;
}

//
void ViewPort(PGraphics v, float x, float y, float px, float py, int eyeoff){
v.beginDraw();
v.background(102);
v.lights();
v.pushMatrix();
v.camera(x+eyeoff, y, 300, px, py, 0, 0.0, 1.0, 0.0);
v.noStroke();
//v.box(100);
v.shape(s);
v.popMatrix();
v.endDraw();
}

//put a texture on PShape object, 6 faces for a cube
void TexturedCube(PImage tex, PShape s, int a, int b) {
s.beginShape(QUADS);
s.texture(tex);

// +Z ""front"" face
s.vertex(-a, -a, a, 0, b);
s.vertex( a, -a, a, b, b);
s.vertex( a, a, a, b, 0);
s.vertex(-a, a, a, 0, 0);

// -Z ""back"" face
s.vertex( a, -a, -a, 0, 0);
s.vertex(-a, -a, -a, b, 0);
s.vertex(-a, a, -a, b, b);
s.vertex( a, a, -a, 0, b);

// +Y ""bottom"" face
s.vertex(-a, a, a, 0, 0);
s.vertex( a, a, a, b, 0);
s.vertex( a, a, -a, b, b);
s.vertex(-a, a, -a, 0, b);

// -Y ""top"" face
s.vertex(-a, -a, -a, 0, 0);
s.vertex( a, -a, -a, b, 0);
s.vertex( a, -a, a, b, b);
s.vertex(-a, -a, a, 0, b);

// +X ""right"" face
s.vertex( a, -a, a, 0, 0);
s.vertex( a, -a, -a, b, 0);
s.vertex( a, a, -a, b, b);
s.vertex( a, a, a, 0, b);

// -X ""left"" face
s.vertex(-a, -a, -a, 0, 0);
s.vertex(-a, -a, a, b, 0);
s.vertex(-a, a, a, b, b);
s.vertex(-a, a, -a, 0, b);

s.endShape();
}

"
"
Processing was designed to make drawing with Java much easier.  Processing for Android has the power of its desktop sibling plus information from sensors.  Putting these things together, shouldn't it be easy to display a stereoscopic image and move around it like Oculus Rift or Google Cardboard?
","
The code below displays an image in two viewports - one for the left eye and one for the right eye. The result is that the image looks 3D when viewed from a Google Cardboard device.  Accelerometer and gyroscope data are used to move the 3D image as the head is moved around.  The only bug is that of Processing for Android in that Landscape mode makes the program crash if you do not start it in this mode. I am using Processing 2.0.3 and Android 4.3, so this problem may have been addressed in current versions. (Although I did see it was still an open issue in Processing-Bugs discussion on Github). The texture image is a 100 x 100 pixel image of a favorite cartoon character. You can use whatever you want – just store the image in the data folder. 
//Scott Little 2015, GPLv3
//pBoard is Processing for Cardboard

import android.os.Bundle; //for preventing sleep
import android.view.WindowManager;
import ketai.sensors.*; //ketai library for sensors
KetaiSensor sensor;

float ax,ay,az,mx,my,mz; //sensor variables
float eyex = 50; //camera variables
float eyey = 50;
float eyez = 0;
float panx = 0;
float pany = 0;
PGraphics lv; //left viewport
PGraphics rv; //right viewport
PShape s; //the object to be displayed

//********************************************************************
// The following code is required to prevent sleep.
//********************************************************************
void onCreate(Bundle savedInstanceState) {
super.onCreate(savedInstanceState);
// fix so screen doesn't go to sleep when app is active
getWindow().addFlags(WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON);
}
//********************************************************************

void setup() {
sensor = new KetaiSensor(this);
sensor.start();

size(displayWidth,displayHeight,P3D); //used to set P3D renderer
orientation(LANDSCAPE); //causes crashing if not started in this orientation

lv = createGraphics(displayWidth/2,displayHeight,P3D); //size of left viewport
rv = createGraphics(displayWidth/2,displayHeight,P3D);

PImage img = loadImage(""jake.jpg"");  //texture image
s = createShape();
TexturedCube(img, s, 50, 50);
}

void draw(){
//draw something fancy on every viewports
panx = panx-mx*10;
pany = 0;
eyex = 0;
eyey = -20*az;

ViewPort(lv, eyex, eyey, panx, pany, -15); //left viewport
ViewPort(rv, eyex, eyey, panx, pany, 15);  //right viewport

//add the two viewports to your main panel
image(lv, 0, 0);
image(rv, displayWidth/2, 0);
}

//sensor data
void onAccelerometerEvent(float x, float y, float z){
ax = x;
ay = y;
az = z;
}

void onGyroscopeEvent(float x, float y, float z){
mx = x;
my = y;
mz = z;
}

//
void ViewPort(PGraphics v, float x, float y, float px, float py, int eyeoff){
v.beginDraw();
v.background(102);
v.lights();
v.pushMatrix();
v.camera(x+eyeoff, y, 300, px, py, 0, 0.0, 1.0, 0.0);
v.noStroke();
//v.box(100);
v.shape(s);
v.popMatrix();
v.endDraw();
}

//put a texture on PShape object, 6 faces for a cube
void TexturedCube(PImage tex, PShape s, int a, int b) {
s.beginShape(QUADS);
s.texture(tex);

// +Z ""front"" face
s.vertex(-a, -a, a, 0, b);
s.vertex( a, -a, a, b, b);
s.vertex( a, a, a, b, 0);
s.vertex(-a, a, a, 0, 0);

// -Z ""back"" face
s.vertex( a, -a, -a, 0, 0);
s.vertex(-a, -a, -a, b, 0);
s.vertex(-a, a, -a, b, b);
s.vertex( a, a, -a, 0, b);

// +Y ""bottom"" face
s.vertex(-a, a, a, 0, 0);
s.vertex( a, a, a, b, 0);
s.vertex( a, a, -a, b, b);
s.vertex(-a, a, -a, 0, b);

// -Y ""top"" face
s.vertex(-a, -a, -a, 0, 0);
s.vertex( a, -a, -a, b, 0);
s.vertex( a, -a, a, b, b);
s.vertex(-a, -a, a, 0, b);

// +X ""right"" face
s.vertex( a, -a, a, 0, 0);
s.vertex( a, -a, -a, b, 0);
s.vertex( a, a, -a, b, b);
s.vertex( a, a, a, 0, b);

// -X ""left"" face
s.vertex(-a, -a, -a, 0, 0);
s.vertex(-a, -a, a, b, 0);
s.vertex(-a, a, a, b, b);
s.vertex(-a, a, -a, 0, b);

s.endShape();
}

"
"
The Rigged Hands provided by Leap Motion is used in this project.

","
This article might help you to think about how you manage interactions between virtual hands and object. Rather than preventing penetration another method is to look at how you can use the penetration to show the user what is happening and increase the sense of presence.
"
"







Closed. This question needs debugging details. It is not currently accepting answers.
                                
                            











 Edit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question.


Closed 3 years ago.







                        Improve this question
                    



I recently updated my VR C++ project from 4.21 to 4.24 and GetComponentLocation() no longer works.
USceneComponent::GetComponentLocation()

Did this change recently? 
This is what my hand blueprint(s) look like:

Here is what my code looks like for spawning and attaching the hand blueprints:

This is the code for getting the location of the motion controller:

Edit: bump
","
I figured it out, I needed to set the owner of the spawned hands, here is the solution:

"
"
I´m using the asset management to load a .mp4 Clip (5 MB) and want to display it via A-Frame (1.0.4)
a-video src=""ID""
Now the problem appears that it wont load on the first try.
I´m just getting a black plane.
After a reload of the page it appears.
That not a problem with only one clip...it won't work on multiple clips within the scene.
I´m using pace.min.js for preloading but that doesnt look like the problem.
Maybe someone here has some suggestions?
Regards
Pascal
","
It's complicated to get the media autoplay on modern browsers, so it's better to add the a-video dynamically. As said in my comment, you can have a modal window or anything with a button to add the video to the scene and launch it. Here is a simple example on how to do it.
// This will create the tag and remove the button from the page when it's done.
document.getElementById(""video-button"").addEventListener(""click"", function() {
        var video = document.createElement(""a-video"");
        video.setAttribute(""src"", ""#videoclip"");
        video.setAttribute(""width"", ""16"");
        video.setAttribute(""height"", ""9"");
        video.setAttribute(""rotation"", ""0 180 0"");
        video.setAttribute(""position"", ""-2 2 -4.15"");
        video.addEventListener(""loaded"", function() {
            document.getElementById(""videoclip"").load();
            document.getElementById(""videoclip"").play();
        });
        document.querySelector(""a-scene"").appendChild(video);
        this.remove();
} );

"
"
I know some websites that provide 3D/360 VR videos within the browser when phone is placed into a Google Cardboard ( non-electronic device, two plastic lenses and a cardboard box that holds your phone ) without using VR engines.
I guess such videos are based on two files that include 2 views ( left eye / right eye ) for 3D and they are embedded on the web page with access to some browser APIs to get the position of the device for the 360 experience.
I would be grateful if you could tell me how to make such videos. Let's consider I already have mp4 files created by a 3D/360 cam. How can I embed them on website to create the 3D/360 experience within a web browser on a phone?
","
If you are using a camera which produces a format compatible with YoutUbe or a similar service you can simply upload the video there and use their service.
If you want to host it your self you will need a player which can support the format - different players will have different configurations and setup but some examples:

Mobile Android: https://developers.google.com/vr/android/samples/video360
Web: three.js based: https://threejs.org/docs/#api/en/textures/VideoTexture (example here although it is slightly old now: https://github.com/gbentaieb/simple360Player
Commercial cross platform solutions: https://bitmovin.com/demos/vr-360, https://www.omnivirt.com/vr-player/

There is also a Google Codelabs which includes building a VR video for viewing in Google Carboard - you need to be aware that Google changes support and approach for VR over time but generally supports older API's for some while also:

https://codelabs.developers.google.com/codelabs/vr_view_101/#0

"
"
I have written a Win32 C++ program. 
I would like to duplicate the visual content of the left window every frame, so it appears at another location on the screen. (One rectangle on the left side of the screen, and an identical rectangle on the right side of the screen.)
This is for stereo rendering, where the left rectangle is seen in the user's left eye, and the right rectangle in the right eye.

The two rectangles must be ""frame synced"", meaning that they must show the same frame at all times. Or to put it another way, if the right eye is behind the left by ""one draw call"", then the viewer becomes ill.
NOTE: I have the source code to both windows, because I created them. (Both windows reside within the same exe at the moment.)
What I have tried:

BitBlt(), by drawing a rectangular portion of the desktop HDC into the right-side window. I am calling BitBlt() in the WM_PAINT message of the right-side window: This works, BUT the right-side window is 1-2 frames behind the left. 

Potential Solutions:

In the left-window's WM_PAINT message, draw both windows with the same contents. Is this possible somehow?
Use one, single giant window to draw both rectangles. This would mean all drawing occurs in the same WM_PAINT call. However, I do not know how to copy part of a window to another part. I wish I could just tell Win32 to 'draw again', but this time start 1000 pixels to the right... 
Do ""something"" with DirectX. (No idea what.)

Any help would be greatly appreciated!
","
I don’t know if you still need an answer, but I don’t think you can do it better than using the DWM Thumbnails API [1]. You basically do an association of a region on your window <-> a region of a foreign window, and then each time the windows are composed and presented on the screen, the specified rectangle of the foreign window will be painted on the specified rectangle of your window. DWM will do scaling if necessary and you can also specify a transparency level. This is the most efficient way to do it, as the compositor does all the heavy lifting, so it is at the lowest level you could go without writing kernel mode code (custom drivers etc). You can’t draw on top, and won’t “see” the foreign window data on your window if you BitBlt it for example, as the image is not actually drawn on your window, but overlaid on top when the final image of your window is composed and presented to the user by the window manager. But for most purposes, it works really well. 
[1] https://learn.microsoft.com/en-us/windows/win32/dwm/thumbnail-ovw
"
"
From the code on my previous answer, I get a WebVR page that works fine in Desktop but in the mobile phone I'm not able to go closer to the model (looks more like a skybox).

Would like to be able to go towards / away from the model (using two fingers going, respectively, away from each other or closer), apart from looking around.
After some searching, found an example of where this functionality I look for is used.

How can I get that same mobile functionality working for my case?
","
1) Orbit controls
If you want your experience to revolve around a model, moving the camera around it, and zooming in and out - I'd say the orbit controls are exacly what You're looking for:
<a-entity camera look-controls 
          orbit-controls=""target: 0 1.6 -0.5; initialPosition: 0 5 15""></a-entity>

The camera will move around the defined target. Works with mobile touch events.
Check it out with gltf models here.
2) Moving the camera with pinching
If you want to move the camera towars or away the direction it's looking at, as far as i know, You'll have to implement such behavior in a custom component. A poor mans version could look a bit like this:
AFRAME.registerComponent('pinch-controls', {
  init: function() {
    // moveCamera uses variables from 'this' scope
    this.moveCamera.bind(this);

    // we'll use this to get the 'pinch direction'
    this.distance = 0;
    // we'll keep here the camera's current direction
    this.direction = new THREE.Vector3();
    // camera entity reference
    this.camera = document.querySelector(""[camera]"");

    // listeners
    document.body.addEventListener('touchstart', event => {
      // we're interested only in two - finger pinches
      if (event.touches.length != 2) return 0;
      // calculate the distance
      this.distance = this.calculatePinchDistance(event);
      // we don't want the touch to rotate the camera around
      this.el.setAttribute('look-controls', 'touchEnabled', false);
    }, false);

    document.body.addEventListener('touchend', event => {
      // when the pinch ends - restore the look-controls
      if (event.touches.length != 1) this.el.setAttribute('look-controls', 'touchEnabled', true);
    }, false);

    document.body.addEventListener('touchmove', event => {
      // we're interested only in two - finger pinches
      if (event.touches.length != 2) return 0;

      // compare the distances to determine which direction should we move
      var distance = this.calculatePinchDistance(event);
      let speed = (distance < this.distance) ? -0.2 : 0.2;
      this.moveCamera(speed);

      // keep the distance for the next callback
      this.distance = distance;
    }, false);
  }, 
  calculatePinchDistance(event) {
      var dx = event.touches[0].pageX - event.touches[1].pageX;
      var dy = event.touches[0].pageY - event.touches[1].pageY;
      return Math.sqrt(dx * dx + dy * dy);
  },
  moveCamera: function(speed) {
        // get the camera direction, and multiply it by the desired 'speed'
        this.el.sceneEl.camera.getWorldDirection(this.direction);
        this.direction.multiplyScalar(speed);
        // apply the change to the actual position
        var pos = this.el.getAttribute(""position"");
        pos.add(this.direction);
        this.el.setAttribute(""position"", pos);
    }
}

// HTML
// <a-entity camera look-controls pinch-controls></a-entity>

Check it out here

If you want to move the camera towards the model (instead of the direction it's facing), you can just replace extracting the camera direction (in moveCamera) with calculating the camera <-> model direction:
// instead of
// this.el.sceneEl.camera.getWorldDirection(this.direction);
// grab the direction towards the model
this.direction.copy(this.el.object3D.position)
this.direction.add(modelReference.object3D.position)
this.direction.normalize();
// (...)

Tiago edit:
The final code used was
<!DOCTYPE html>
<html>
  <head>
    <meta charset=""utf-8"">
    <title>Octant Cube - by Dodds, H. & Peres, T.</title>
    <meta name=""description"" content=""Present in the Taxonomy article"">
        <script src=""https://kit.fontawesome.com/c9500776a0.js"" crossorigin=""anonymous""></script>
    <script src=""misc/codeBtn.js""></script>
    <script src=""https://aframe.io/releases/1.0.4/aframe.min.js""></script>
    <script src=""https://unpkg.com/[email protected]/dist/aframe-orbit-controls.min.js""></script>
    <script src=""https://unpkg.com/[email protected]/dist/aframe-supercraft-loader.js""></script>
    <style>
    #toggleBtn {
        position: fixed;
        z-index: 9999;
        margin: 25px;
        font-size: 3em;
        color: rgb(128, 0, 128);
        cursor: pointer;
    }

    #toggleBtn:hover {
        color: rgba(128, 0, 128, 0.6);
    }
    </style>
    <script>
    // Models swap component
    AFRAME.registerComponent('content-manager', {
        init: function() {
            const btn = document.querySelector(""#toggleBtn"")
            const castle = document.querySelector(""#castle"")
            const fish = document.querySelector(""#fish"")

            btn.addEventListener(""click"", e => {
                if (castle.getAttribute(""visible"")) {
                    castle.emit(""fadeOut"")
                    btn.classList.remove(""fa-fish"")
                    btn.classList.add(""fa-landmark"")
                } else {
                    fish.emit(""fadeOut"")
                    btn.classList.remove(""fa-landmark"")
                    btn.classList.add(""fa-fish"")
                }
            })
            fish.addEventListener('animationcomplete__fadeout', e => {
                fish.setAttribute(""visible"", ""false"")
                castle.setAttribute(""visible"", ""true"")
                castle.emit(""fadeIn"")
            })
            castle.addEventListener('animationcomplete__fadeout', e => {
                castle.setAttribute(""visible"", ""false"")
                fish.setAttribute(""visible"", ""true"")
                fish.emit(""fadeIn"")
            })
        }
    })

    // move the camera on pinching
    AFRAME.registerComponent('pinch-controls', {
        init: function() {
            // bind the methods that use the scope variables
            this.moveCamera.bind(this);

            // use this to keep track whether the user is moving forward or backwards
            this.distance = 0;
            // store the camera direction here
            this.direction = new THREE.Vector3();
            // camera entity reference
            this.camera = document.querySelector(""[camera]"");

            document.body.addEventListener('touchstart', event => {
                // react only on two finger pinches
                if (event.touches.length != 2) return 0;

                this.distance = this.calculatePinchDistance(event);
                // prevent the look controls to rotate the camera while pinching
                this.el.setAttribute('look-controls', 'touchEnabled', false);
            }, false);

            document.body.addEventListener('touchend', event => {
                // restore the look-controls
                if (event.touches.length != 1) this.el.setAttribute('look-controls', 'touchEnabled', true)
            }, false);
            document.body.addEventListener('wheel', e => {
                this.moveCamera(e.deltaY < 0 ? -0.2 : 0.2);
            })
            document.body.addEventListener('touchmove', event => {
                // we're interested only in pinching
                if (event.touches.length != 2) return 0;

                // calculate the pinch difference and move the camera
                var distance = this.calculatePinchDistance(event);
                let multiplier = (distance < this.distance) ? -0.2 : 0.2;;
                if (!isNaN(multiplier)) this.moveCamera(multiplier);

                // for later use
                this.distance = distance;
            }, false);
        },
        calculatePinchDistance: function(mouseEvent) {
            var dx = event.touches[0].pageX - event.touches[1].pageX;
            var dy = event.touches[0].pageY - event.touches[1].pageY;
            return Math.sqrt(dx * dx + dy * dy);
        },
        moveCamera: function(speed) {
            this.el.sceneEl.camera.getWorldDirection(this.direction);
            this.direction.multiplyScalar(speed);
            var pos = this.el.getAttribute(""position"");
            pos.add(this.direction);
            this.el.setAttribute(""position"", pos)
        }
    })
    </script>
  </head>
  <body>
    <a-scene background=""color: #FAFAFA"">
      <a-assets>
        <a-asset-item id=""octant"" src=""octant.glb""></a-asset-item>
      </a-assets>
      <a-entity position=""0 0.347 -4"" rotation=""0 60 -1""  gltf-model=""#octant""  scale=""5 5 5"" animation__fadeIn=""property: scale; dur: 150; from: 0.001 0.001 0.001; to: 0.5 0.5 0.5; easing: easeInQuad; startEvents: fadeIn"" animation__fadeOut=""property: scale; dur: 150; from: 0.5 0.5 0.5; to: 0.001 0.001 0.001; easing: easeInQuad; startEvents: fadeOut""></a-entity>
      <a-entity camera look-controls 
          orbit-controls=""target: 0 1.6 -0.5; initialPosition: 0 5 15""></a-entity>
    </a-scene>
  </body>
</html>

And this was the final result

"
"
I am using the latest A-Frame 1.0.4 and I have tried both with spot light (in the picture attached) and with direct light.
The shadowBias is set to -0.0001, changing it affects the artifact only slightly.
If I set the shadowMapHeight and shadowMapWidth to 1024 the artifact shrinks back a bit from the source of light. To have it completely disappear I have to put 4096, which is a bit too much, performance-wise. I can't have all lights with a 4k shadow map.
A similar effect has increasing the shadowCamera far, from 500 to 5000 for example. but this introduces wrong shadows, distant from the objects (even if I tweak the shadowBias).
Am I doing something wrong? Why the lights with default settings have these problems? Does it perhaps depend on the models ?
Any clue would be appreciated 

","
I was able to find the culprit! My models are double sided, and this resulted into the front face casting shadow on the back face together with the back face casting shadow on the front face, hence the striped glitch.
After having a look at three.js - material and confirming with material.side that it was actually double sided, I used material.shadowSide = THREE.BackSide to force the backside to cast shadow onto the frontside, regardless of the model/material double side property.
This solved the glitch!
Of course be mindful that if a model has wrong normals i.e. don't facing towards the outside / inverted , that face will have the glitch again.
"
"
Got a simple .html file using A-Frame that loads a specific model.
<!DOCTYPE html>
<html>
  <head>
    <meta charset=""utf-8"">
    <title>Octant Cube - by Dodds, H. & Peres, T.</title>
    <meta name=""description"" content=""Present in the Taxonomy article"">
    <script src=""https://aframe.io/releases/1.0.4/aframe.min.js""></script>
  </head>
  <body>
    <a-scene background=""color: #FAFAFA"">
      <a-assets>
        <a-asset-item id=""octant"" src=""octant.glb""></a-asset-item>
      </a-assets>
      <a-entity gltf-model=""#octant""></a-entity>
  </body>
</html>

The object loads fine on the page (as the documentation suggests) but this is what I see at first

If I look all down I'm able to see one plane

But would like to see the following at start

Using the A-Frame Inspector I was able to see this would suit nice in the position 0 , 0.347, -3 and rotation 0, 58.0274, -0.27.

I tried adding the position and rotation to <a-assets> and <a-asset-item ... > but no change was possible to be seen.
How can I change the initial position and the rotation of the model?
","
One needs to add that information in <a-entity> instead, like this
<!DOCTYPE html>
<html>
  <head>
    <meta charset=""utf-8"">
    <title>Octant Cube - by Dodds, H. & Peres, T.</title>
    <meta name=""description"" content=""Present in the Taxonomy article"">
    <script src=""https://aframe.io/releases/1.0.4/aframe.min.js""></script>
  </head>
  <body>
    <a-scene background=""color: #FAFAFA"">
      <a-assets>
        <a-asset-item id=""octant"" src=""octant.glb""></a-asset-item>
      </a-assets>
      <a-entity position=""0 0.347 -4"" rotation=""0 60 -1""  gltf-model=""#octant""></a-entity>
  </body>
</html>

This is the final result

"
"
I have an a-frame component that leverages three js instancing to render a large number of spheres efficiently. Based on examples from the three js library such as https://github.com/mrdoob/three.js/blob/master/examples/webgl_instancing_scatter.html, I should be able to set the colors for each of the instanced renderings individually. I thought I had followed the example but my per instance colors are not taking effect. 
AFRAME.registerComponent('spheres', {
    schema: {
        count: {type: 'number'},
        radius: {type: 'number'},
        scale: {type: 'number'},
        colors: {type: 'array'},
        positions: {type: 'array'}
    },
    init: function() {
        const {count, radius, scale, colors, positions} = this.data;  

        const material = new THREE.MeshNormalMaterial();

        const geometry = new THREE.SphereBufferGeometry( radius, 3, 2 );
        const instancedGeometry = new THREE.InstancedBufferGeometry().copy(geometry); 
        var instanceColors = [];

        for ( var i = 0; i < count; i ++ ) {

            instanceColors.push( Math.random() );
            instanceColors.push( Math.random() );
            instanceColors.push( Math.random() );

        }
        instancedGeometry.setAttribute(""instanceColor"", new THREE.InstancedBufferAttribute( new Float32Array( instanceColors ), 3 ))

        instancedGeometry.computeVertexNormals();

        material.vertexColors = true;

        const matrix = new THREE.Matrix4();
        const mesh = new THREE.InstancedMesh( instancedGeometry, material, count );

        for ( var i = 0; i < count; i ++ ) {

            this.setMatrix(positions[i], scale)( matrix );
            mesh.setMatrixAt( i, matrix );

        }

        this.el.object3D.add( mesh );
    },

What am I doing wrong?
","
Got it to work with the following based on @prisoner849 example. I had to use MeshPhongMaterial rather than MeshNormalMaterial. Not quite sure why. Also not sure why I need to use an InstancedBufferAttribute and a BufferAttribute for the colors. maybe @prisoner849 can follow up with details. This solution is a bit ugly, hopefully three will provide a cleaner way to tweak instanced colors soon, or maybe there is a better way already and I just dont know about it
AFRAME.registerComponent('spheres', {
    schema: {
        count: {type: 'number'},
        radius: {type: 'number'},
        scale: {type: 'number'},
        colors: {type: 'array'},
        positions: {type: 'array'}
    },
    init: function() {
        const {count, radius, scale, colors, positions} = this.data;  

        var geometry = new THREE.SphereBufferGeometry(radius);
        var material = new THREE.MeshPhongMaterial({ flatShading: true });
        var colorParsChunk = [
            'attribute vec3 instanceColor;',
            'varying vec3 vInstanceColor;',
            '#include <common>'
        ].join( '\n' );

        var instanceColorChunk = [
            '#include <begin_vertex>',
            '\tvInstanceColor = instanceColor;'
        ].join( '\n' );

        var fragmentParsChunk = [
            'varying vec3 vInstanceColor;',
            '#include <common>'
        ].join( '\n' );

        var colorChunk = [
            'vec4 diffuseColor = vec4( diffuse * vInstanceColor, opacity );'
        ].join( '\n' );

        material.onBeforeCompile = function ( shader ) {

            shader.vertexShader = shader.vertexShader
                .replace( '#include <common>', colorParsChunk )
                .replace( '#include <begin_vertex>', instanceColorChunk );

            shader.fragmentShader = shader.fragmentShader
                .replace( '#include <common>', fragmentParsChunk )
                .replace( 'vec4 diffuseColor = vec4( diffuse, opacity );', colorChunk );

        };

        var instanceColors = [];

        for ( var i = 0; i < count; i ++ ) {

            instanceColors.push( Math.random() );
            instanceColors.push( Math.random() );
            instanceColors.push( Math.random() );

        }

        const matrix = new THREE.Matrix4();
        const mesh = new THREE.InstancedMesh( geometry, material, count );

        var instanceColorsBase = new Float32Array(instanceColors.length);
        instanceColorsBase.set(instanceColors);
        geometry.setAttribute( 'instanceColor', new THREE.InstancedBufferAttribute( new Float32Array( instanceColors ), 3 ) );
        geometry.setAttribute( 'instanceColorBase', new THREE.BufferAttribute(new Float32Array( instanceColorsBase ), 3 ) );

        for ( var i = 0; i < count; i ++ ) {

            this.setMatrix(positions[i], scale)( matrix );
            mesh.setMatrixAt( i, matrix );

        }

        this.el.object3D.add( mesh );
    },
    setMatrix: function( pos, scaler ) {

        var position = new THREE.Vector3();
        var rotation = new THREE.Euler();
        var quaternion = new THREE.Quaternion();
        var scale = new THREE.Vector3();

        return function ( matrix ) {

            position.x = pos[0];
            position.y = pos[1];
            position.z = pos[2];

            rotation.x = 0;
            rotation.y = 0;
            rotation.z = 0;

            quaternion.setFromEuler( rotation );

            scale.x = scale.y = scale.z = scaler;

            matrix.compose( position, quaternion, scale );

        };

    }
});

"
"
TL;DR: For a youtube channel, I am going to try and control an avatar in a similar way to the gif below. Essentially recording my movements in VR and then using them in a video.
At my disposal I have an Oculus Rift S and free time to try things out in unity and other programs. 
What I have tried so far:

Recording within other games which have built in recording features
This does not provide me with the customizeability I need
Recording within unity
I have not had success implementing recording at all into unity.

There are clearly a variety of ways to solve this, and I have spent a few hours just trying to get a solution, but I cannot find one which works.
","
Pretty big project but
Create rigged/animated avatar in Unity ->
Setup Animation state machine ->
Setup logic to switch to different states based on input (different states for hand/mouth animations) ->
Have logic to fire different animation state triggers, perhaps also play an audio clip from an audio source
Optional:
Take microphone input ->
Have AI speech recognition or just sample microphone volume for general sound input ->
"
"
I am building a Daydream app via Unity, have a basic first person demo working but I have not developed on Unity before. Looking to add characters to the app and have dialogs with them. I am considering two ways: 

When I go near to a character a dialog box would come up next to that character and say ""Hello"".
When I point to a character with the pointer it would say ""start conversation""

After either case I would have multiple options to choose from eg. ""Where is X"" or ""what is your name"" etc then depending on my selection the character would respond with something else and so on. I couldn't see something that would solve this in standard assets. Are there any best practices, templates around this? If not any ideas / directions? 
Unity 2017.3.1f1
GVR SDK: v1.130.1
Android Studio 3.0.1
Java jdk1.8.0_162.jdk
Pixel 2 phone
MacBook High Sierra
","
It is pretty easy just follow these steps;
Add a collider to the character,
and put a 3d text on where you want ""Hello"" to pop up and add a script like this;
public text HelloText = null;

void OnCollisionEnter(Collision collision)
{
   HelloText.text.enabled = true;
}//Search more on google ; Unity 3d text.

Create a cube scale it as you desire and put a text in it like ""Start Conversation"". Add a collider to the cube and a script like this;
  void OnMouseDown()
    {
        //Create dialog boxes LIKE THIS.
    }

This is not the perfect way to do it. If you are new this might be the best option for you.
You can create a prefab which made of cube and a text. Then you can set their values thru script.
EDIT:
Create an empty game object and attach this script to it.
Then put 3rdPersonController gameobject to empty gameobject slot in empty game object's script
Example:
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class AScriptName: MonoBehaviour {

 public GameObject PlayerController = null;
 TextMesh DialogBox = null;

void Start()
{

//Note: Attach your 3d text to your 3rdPersonController in the editor..

DialogBox = PlayerController.GetComponentInChildren(typeof(TextMesh)) as TextMesh;

DialogBox.text = ""Start Conversation"";

//or you can pass a string value;
//Example string text = ""Start Conversation""; DialogBox.text = text;

}

}

"
"
I'm trying to build a Flower of Life VR example using A-Frame. 
The closest thing that I found is using the opacity property in <a-circle>. 
Is there a way to create just the border of the circle?
","
Not sure if it fits your use case but you could use a cylinder with a small height:
https://aframe.io/docs/1.0.0/primitives/a-cylinder.html
Or a tube:
https://github.com/donmccurdy/aframe-extras/tree/master/src/primitives
Or just a flat plane with png texture with a circle with transparency
"
"
I am developing a Multiplayer game where a Normal Character Controller User and VR User can participate. So there are two modes of the games through a user can join, 

Normal Mode (Character Controller)
VR Mode (HTC VR headset)

I am able to manage these modes and user can participate with any mode but the problem is Normal Player (character controller) is unable to view VR controller (HTC Vive Controllers) as these objects become turn off on the side of character controller. So how do I show VR controller to the Normal player side that what VR player is doing currently.
(What I have tried) I make an object and tried to imitate its position and rotation with respect to VR controller but it is not working as Imitating source (VR controllers) objects are off (at the normal player side but working in VR mode) and their positions is not changing. How can I show the VR controller to other users in unity3d??
","
I have managed to solve this problem using these  steps after trying hard three four days. These step are given below so that future user don't get stuck in this problem as there are no other comprehensive guide available:

Make one Gameobject for Head (simple cube) with Network Identity and network transform
Make one Gameobject for(right) Controller (simple cube) with Network Identity and network transform
Make one Gameobject for(left) other Controller (simple cube) with network Identity and network transform
Make prefab of all above gameobjects.
Add all three prefabs into Network Manager (registered Spawnable prefabs list)
delete three prefab from scene
Added below script(See comments for details) into my vr player and assign respective prefabs and gameobjects
public class VRPlayerCtrl : NetworkTransform {

//source gameobjects head, left and right controller object of htc vive prefab

public GameObject rightContSource;

public GameObject leftContSource;

public GameObject headObjSource;


//prefabs to assign head, left controller, and right controller
public GameObject vrHeadObjPrefab;
public GameObject vrLeftCtrlPrefab;
public GameObject vrRightCtrlPrefab;

GameObject vrHeadObj;
GameObject vrLeftCtrl;
GameObject vrRightCtrl;

void Start()
{

    Debug.Log(""Start of the vr player"");

    if (isLocalPlayer)
    {
        //instantiate prefabs
        CmdInstantiteHeadAndController();
        //disabled conroller meshes at VR player side so it cannont be view by him
        vrLeftCtrl.GetComponent<MeshRenderer>().enabled = false;
        vrRightCtrl.GetComponent<MeshRenderer>().enabled = false;
    }
}

//Instantiate on start head and vr controller object so that it can be view by normal players
void CmdInstantiteHeadAndController()
{
    Debug.Log(""instantiateing the controller and head object"");
    vrHeadObj = (GameObject)Instantiate(vrHeadObjPrefab);
    vrLeftCtrl = (GameObject)Instantiate(vrLeftCtrlPrefab);
    vrRightCtrl = (GameObject)Instantiate(vrRightCtrlPrefab);

    // spawn the bullet on the clients
    NetworkServer.Spawn(vrHeadObj);
    NetworkServer.Spawn(vrLeftCtrl);
    NetworkServer.Spawn(vrRightCtrl);
}

void Update()
{
    if (!isLocalPlayer)
    {
        return;
    }

    //sync pos on network
    CmdControllerPositionSync();
}
//sync position on VR controller objects so that VR player movemnts/action can be viewd by normal user
[Command]
public void CmdControllerPositionSync()
{

    vrHeadObj.transform.localRotation = headObjSource.transform.localRotation;
    vrHeadObj.transform.position = headObjSource.transform.position;
    vrLeftCtrl.transform.localRotation = leftContSource.transform.localRotation;
    vrRightCtrl.transform.localRotation = rightContSource.transform.localRotation;
    vrLeftCtrl.transform.localPosition = leftContSource.transform.position;
    vrRightCtrl.transform.localPosition = rightContSource.transform.position;
}

}


congrats you have done!
"
"
In android Chrome 80, when the WebXR device API flag is disabled, I expect the Webxr polyfill should play the VR output. But the content is flickering and I am not able to hit any object in VR world. Any idea why this is happening?
If the WebXR device API flag is enabled, I am getting the output correctly. I included the webxr polyfill (https://github.com/immersive-web/webxr-polyfill) , but not sure whether it is working or not. How to check whether polyfill is giving the correct output? Do we need to build the WebXR Polyfill separately to support chrome 80?
","
I fixed this issue by setting the preserverDrawingBuffer flag to true while creating the renderer https://threejs.org/docs/#api/en/renderers/WebGLRenderer.preserveDrawingBuffer 
"
"
Can someone who understands stereo rendering give an explanation of what each of these functions is doing to create the VR stereo effect. There's little to no documentation on functions like StereoCamera(), setScissor(), setViewPort() in the three.js library. 
I would greatly appreciate any sort of high/low-level explanation. 
Also, a bug I'm having is when I try to change the eyeSep value it has no effect on the final render.  
/**
 * @author alteredq / http://alteredqualia.com/
 * @authod mrdoob / http://mrdoob.com/
 * @authod arodic / http://aleksandarrodic.com/
 * @authod fonserbc / http://fonserbc.github.io/
*/

THREE.StereoEffect = function ( renderer ) {

    var _stereo = new THREE.StereoCamera();
    _stereo.aspect = 0.5;
    var size = new THREE.Vector2();

    this.setEyeSeparation = function ( eyeSep ) {

        _stereo.eyeSep = eyeSep;

    };

    this.setSize = function ( width, height ) {

        renderer.setSize( width, height );

    };

    this.render = function ( scene, camera ) {

        scene.updateMatrixWorld();

        if ( camera.parent === null ) camera.updateMatrixWorld();

        _stereo.update( camera );

        renderer.getSize( size );

        if ( renderer.autoClear ) renderer.clear();
        renderer.setScissorTest( true );

        renderer.setScissor( 0, 0, size.width / 2, size.height );
        renderer.setViewport( 0, 0, size.width / 2, size.height );
        renderer.render( scene, _stereo.cameraL );

        renderer.setScissor( size.width / 2, 0, size.width / 2, size.height );
        renderer.setViewport( size.width / 2, 0, size.width / 2, size.height );
        renderer.render( scene, _stereo.cameraR );

        renderer.setScissorTest( false );

    };

};

module.exports = THREE.StereoEffect;

","
setScissor and setViewport
set the area of the canvas to render to. More specifically setViewport sets how to convert from a shader's clip space to some portion of the canvas's pixel space and setScissor sets a rectangle outside of which nothing can be rendered.
See this. 
Otherwise StereoCamera just provides 2 cameras that are eyeSep apart so you only have to manipulate one camera, the PerspectiveCamera then you update the StereoCamera and it will automatically update the 2 eye cameras you can use for rendering.


body { margin: 0; }
#c { width: 100vw; height: 100vh; display: block; }
#ui { position: absolute; left: 1em; top: 1em; }
<canvas id=""c""></canvas>
<script type=""module"">
import * as THREE from 'https://threejsfundamentals.org/threejs/resources/threejs/r115/build/three.module.js';
import {GUI} from 'https://threejsfundamentals.org/threejs/../3rdparty/dat.gui.module.js';

function main() {
  const canvas = document.querySelector('#c');
  const renderer = new THREE.WebGLRenderer({canvas});

  const fov = 75;
  const aspect = 2;  // the canvas default
  const near = 0.1;
  const far = 5;
  const camera = new THREE.PerspectiveCamera(fov, aspect, near, far);
  camera.position.z = 2;
  
  const stereo = new THREE.StereoCamera();

  const gui = new GUI();
  gui.add(stereo, 'eyeSep', 0, 2, 0.001);

  const scene = new THREE.Scene();

  {
    const color = 0xFFFFFF;
    const intensity = 1;
    const light = new THREE.DirectionalLight(color, intensity);
    light.position.set(-1, 2, 4);
    scene.add(light);
  }

  const geometry = new THREE.SphereBufferGeometry(0.5, 6, 3);

  function makeInstance(geometry, color, x) {
    const material = new THREE.MeshPhongMaterial({color, flatShading: true});

    const cube = new THREE.Mesh(geometry, material);
    scene.add(cube);

    cube.position.x = x;

    return cube;
  }

  const cubes = [
    makeInstance(geometry, 0x44aa88,  0),
    makeInstance(geometry, 0x8844aa, -1),
    makeInstance(geometry, 0xaa8844,  1),
  ];

  function resizeRendererToDisplaySize(renderer) {
    const canvas = renderer.domElement;
    const width = canvas.clientWidth;
    const height = canvas.clientHeight;
    const needResize = canvas.width !== width || canvas.height !== height;
    if (needResize) {
      renderer.setSize(width, height, false);
    }
    return needResize;
  }

  function render(time) {
    time *= 0.001;

    if (resizeRendererToDisplaySize(renderer)) {
      const canvas = renderer.domElement;
      camera.aspect = canvas.clientWidth / canvas.clientHeight / 2;  
      camera.updateProjectionMatrix();
    }

    cubes.forEach((cube, ndx) => {
      const speed = 1 + ndx * .1;
      const rot = time * speed;
      cube.rotation.x = rot;
      cube.rotation.y = rot;
    });

    {
        // we need to manually update camera matrix
        // because it will not be passed directly to
        // renderer.render were it would normally be
        // updated

        camera.updateWorldMatrix();
        stereo.update(camera);

        const size = new THREE.Vector2();
        renderer.getSize(size);

        renderer.setScissorTest(true);

        renderer.setScissor(0, 0, size.width / 2, size.height);
        renderer.setViewport(0, 0, size.width / 2, size.height);
        renderer.render(scene, stereo.cameraL);

        renderer.setScissor(size.width / 2, 0, size.width / 2, size.height);
        renderer.setViewport(size.width / 2, 0, size.width / 2, size.height);
        renderer.render(scene, stereo.cameraR);

        renderer.setScissorTest(false);
    }

    requestAnimationFrame(render);
  }

  requestAnimationFrame(render);
}

main();
</script>



"
"
Working on a Unity hybrid VR (cardboard) /2D app. The cardboard side of it works fine. I am having trouble with the 2D/VR switching.
When I am in 2D mode, reticle does not move, although screen taps register. So the app seems unaware of the gyro.
I feel like I am missing something fundamental here. I have a GvrEventSystem prefab that has both an EventSystem and GvrPointerInputModule components.
What obvious thing am I over-looking?
ETA:
I have been asked to add relevant code.  Here is the code for 2D-VR switching on-the-fly.  This code executes w/out error, and the app switches between VR and 2D mode every 3 seconds:
  readonly string NONE_STRING = """";
  readonly string CARDBOARD_STRING = ""cardboard"";

  void Start()
  {
    Invoke(""GoPhone"", 3.0f);
  }

  void GoPhone()
  {
    SetVREnabled(false);
    Invoke(""GoVR"", 3.0f);
  }

  void GoVR()
  {
    SetVREnabled(true);
    Invoke(""GoPhone"", 3.0f);
  }

  void SetVREnabled(bool isEnabled)
  {
    if (isEnabled)
    {
      StartCoroutine(LoadDevice(CARDBOARD_STRING));
    }
    else
    {
      StartCoroutine(LoadDevice(NONE_STRING));
    }
  }

  IEnumerator LoadDevice(string newDevice)
  {
    if (String.Compare(XRSettings.loadedDeviceName, newDevice, true) != 0)
    {
      XRSettings.LoadDeviceByName(newDevice);

      yield return null;

      if (!XRSettings.loadedDeviceName.Equals(NONE_STRING))
        XRSettings.enabled = true;
    }
  }

Although I feel like my problem is a configuration problem, and not a code problem.  In the editor, which does not support VR mode, the app behaves in 2D mode as expected.
Also ETA:


JIC
","
User error!  I did not follow the ""Magic Window"" instructions as detailed at https://developers.google.com/vr/develop/unity/guides/magic-window... let my folly be a warning to future generations!
"
"
I need some help about VR camera which VR SDK is best and support all Device like(Mixed Reality, Oculus Rift and HTC Vive).any idea quick..
","
If you want to build an VR-Application for Desktop-VR (like Oculus, WMR, Vive), you can just roll with SteamVR which supports them all.
"
"
I'm building a VR/AR calendar in Unity, and looking for the easiest way to read calendar event data from Google and Facebook (let's just start with Google).  I've tried using the Google Calendar C# API, but Unity doesn't support .NET 4.0 or above (Google Calendar API only works with .NET 4.0 and above). Unity does have a .NET 4.6 Experimental version, but in Visual Studio after installing Google.Apis.Auth etc and other packages needed in PuTTY package manager the C# code would still not recognize the imported packages below:
using Google.Apis.Auth.OAuth2;
using Google.Apis.Calendar.v3;
using Google.Apis.Calendar.v3.Data;
using Google.Apis.Services;
using Google.Apis.Util.Store;

So next I tried a different approach: I tried installing the Unity Firebase plugin.  It only works for iOS and Android for now, so I started a new project just to start clean.  There are only a few YouTube videos about Firebase in Unity in general, and they don't get into Google Auth, just Google Analytics and regular plain email/password, so it's been trial and error for a few weeks now. I feel like this guy, except I'm not working with a FitBit, and I don't have it figured out. Does anybody have insight on how to go about this?
More important question: If I were to get Google Auth working through Firebase in Unity, how would I go about pulling the Google Calendar event data associated with that account? Would that be back through the C# script? 
Is there another way all together that I'm probably missing? Thanks for any help!
","
To clarify your question. The Google Calendar API supports any language that can send HTTP POST and HTTP Gets.   The Google .net client library currently requires a min of .net 4.5 to run.   There is currently no support in the library for unity there is an issue here which states that we will probably not support it.
That being said you can still add google calendar to your project you will just have to code your own library for it.
"
"
I am currently trying to get my project in Oculus Quest working using the Lightweight Render Pipeline. I am using VRTK4 and UnityXRCameraRig for my VR cam.
However, since I got the render pipeline the right eye is distorted (has a different setting), and I cannot change it's setting. Here are the settings:
LWRP:

HeadAnchor (UnityXRCameraRig):

Virtual Reality Settings:

Any idea how to work around this issue using the UnityXRCameraRig prefab?
","
Solved it by upgrading to the newest version of Unity (2019.3.2f1)
"
"
So, I've tried this code here and it always spawns an object in front of the player every n-seconds (other function in Update() )
  private void SpawnTarget(int prefabIndex = -1)
    {
        GameObject target;
        target = Instantiate(targetPrefabs[0]) as GameObject;
        target.transform.SetParent(transform);
        target.transform.position = Vector3.forward * 3;
        activeTarget.Add(target);
    }

Now, I'm trying to randomize the place where it spawns, so that it's not always on the same X and Y coordinates (so it moves around the screen). I've tried the following, which does move the GameObject randomly in X and Y every n-seconds, but it does not follow the player's Z axis anymore, i.e., if the player moves past the target, now it keeps respawning behind the player.
  private void SpawnTarget(int prefabIndex = -1)
    {
        GameObject target;
        target = Instantiate(targetPrefabs[0]) as GameObject;
        target.transform.SetParent(transform);
        target.transform.position = target.transform.position + new Vector3(Random.Range(-1.0f, 1.0f), Random.Range(-1.0f, 1.0f), 3.0f);
        activeTarget.Add(target);
    }

Alternatively (also same behavior):
  private Vector3 newVector;
  private void SpawnTarget(int prefabIndex = -1)
    {
        GameObject target;
        target = Instantiate(targetPrefabs[0]) as GameObject;
        target.transform.SetParent(transform);
        newVector = new Vector3(Random.Range(-1.0f, 1.0f), Random.Range(-1.0f, 1.0f), 3.0f);
        target.transform.position = target.transform.position + newVector;
        activeTarget.Add(target);
    }

I feel like I'm missing something very basic and very stupid here.
","
what you want is to spawn the prefab at a set position from the player. But you're spawning it in world coordinates.
One solution is to parent the prefabs to the parent when you spawn them, and set their local position, not world position to the desired spot.
But parenting things to the player is bad news, especially in VR. Trust me.
A better solution would be to store a Vector3 that is the relative location from the player, and then set the prefab's position to that every frame. Something like this.
public class SpawnedPrefab : MonoBehaviour {
// this is the position in the player's local space. 
// In your example, random x and y, and z =3
Vector3 positionRelativeToPlayer; 

Transform player;

void Update() {
    this.transform.position = player.TransformPoint(positionRelativeToPlayer);
}

}
TransformFrom point takes a point in local space and figures out what it would be in world space.
https://docs.unity3d.com/ScriptReference/Transform.TransformPoint.html
"
"
Ok Im trying to do a standard blur material like this (the darkened beat saber menu)

Or 
But on a 3D object, so not a camera effect or canvas material. I found some assets that provide a low quality blur but I need it glossy, and a nice Gaussian blur. The one I have has weird streaks:

// Upgrade NOTE: replaced 'mul(UNITY_MATRIX_MVP,)' with 'UnityObjectToClipPos()'
Shader ""Custom/WaterBlur"" {
    Properties {
    _blurSizeXY(""BlurSizeXY"", Range(0,10)) = 0
}
    SubShader {

        // Draw ourselves after all opaque geometry
        Tags { ""Queue"" = ""Transparent"" }

        // Grab the screen behind the object into _GrabTexture
        GrabPass { }

        // Render the object with the texture generated above
        Pass {


CGPROGRAM
#pragma debug
#pragma vertex vert
#pragma fragment frag 
#ifndef SHADER_API_D3D11

    #pragma target 3.0

#else

    #pragma target 4.0

#endif

            sampler2D _GrabTexture : register(s0);
            float _blurSizeXY;

struct data {

    float4 vertex : POSITION;

    float3 normal : NORMAL;

};



struct v2f {

    float4 position : POSITION;

    float4 screenPos : TEXCOORD0;

};



v2f vert(data i){

    v2f o;

    o.position = UnityObjectToClipPos(i.vertex);

    o.screenPos = o.position;

    return o;

}



half4 frag( v2f i ) : COLOR

{

    float2 screenPos = i.screenPos.xy / i.screenPos.w;
    float depth= _blurSizeXY*0.0005;

    screenPos.x = (screenPos.x + 1) * 0.5;

    screenPos.y = 1-(screenPos.y + 1) * 0.5;

    half4 sum = half4(0.0h,0.0h,0.0h,0.0h);   
    sum += tex2D( _GrabTexture, float2(screenPos.x-5.0 * depth, screenPos.y+5.0 * depth)) * 0.025;    
    sum += tex2D( _GrabTexture, float2(screenPos.x+5.0 * depth, screenPos.y-5.0 * depth)) * 0.025;

    sum += tex2D( _GrabTexture, float2(screenPos.x-4.0 * depth, screenPos.y+4.0 * depth)) * 0.05;
    sum += tex2D( _GrabTexture, float2(screenPos.x+4.0 * depth, screenPos.y-4.0 * depth)) * 0.05;


    sum += tex2D( _GrabTexture, float2(screenPos.x-3.0 * depth, screenPos.y+3.0 * depth)) * 0.09;
    sum += tex2D( _GrabTexture, float2(screenPos.x+3.0 * depth, screenPos.y-3.0 * depth)) * 0.09;

    sum += tex2D( _GrabTexture, float2(screenPos.x-2.0 * depth, screenPos.y+2.0 * depth)) * 0.12;
    sum += tex2D( _GrabTexture, float2(screenPos.x+2.0 * depth, screenPos.y-2.0 * depth)) * 0.12;

    sum += tex2D( _GrabTexture, float2(screenPos.x-1.0 * depth, screenPos.y+1.0 * depth)) *  0.15;
    sum += tex2D( _GrabTexture, float2(screenPos.x+1.0 * depth, screenPos.y-1.0 * depth)) *  0.15;



    sum += tex2D( _GrabTexture, screenPos-5.0 * depth) * 0.025;    
    sum += tex2D( _GrabTexture, screenPos-4.0 * depth) * 0.05;
    sum += tex2D( _GrabTexture, screenPos-3.0 * depth) * 0.09;
    sum += tex2D( _GrabTexture, screenPos-2.0 * depth) * 0.12;
    sum += tex2D( _GrabTexture, screenPos-1.0 * depth) * 0.15;    
    sum += tex2D( _GrabTexture, screenPos) * 0.16; 
    sum += tex2D( _GrabTexture, screenPos+5.0 * depth) * 0.15;
    sum += tex2D( _GrabTexture, screenPos+4.0 * depth) * 0.12;
    sum += tex2D( _GrabTexture, screenPos+3.0 * depth) * 0.09;
    sum += tex2D( _GrabTexture, screenPos+2.0 * depth) * 0.05;
    sum += tex2D( _GrabTexture, screenPos+1.0 * depth) * 0.025;

    return sum/2;

}
ENDCG
        }
    }

Fallback Off
} 

How can I accomplish a glossy, or even just a highqaulity gaussian blur material for a mesh?
","
If you want to include a gloss onto your surface, here is one way to do it, based on the shader in the Glossy Textures article in the Unity Community wiki copied below:

Shader ""Cg per-pixel lighting with texture"" {
   Properties {
      _MainTex (""RGBA Texture For Material Color"", 2D) = ""white"" {} 
      _Color (""Diffuse Material Color"", Color) = (1,1,1,1) 
      _SpecColor (""Specular Material Color"", Color) = (1,1,1,1) 
      _Shininess (""Shininess"", Float) = 10
   }
   SubShader {
      Pass {    
         Tags { ""LightMode"" = ""ForwardBase"" } 
            // pass for ambient light and first light source

         CGPROGRAM

         #pragma vertex vert  
         #pragma fragment frag 

         #include ""UnityCG.cginc""
         uniform float4 _LightColor0; 
            // color of light source (from ""Lighting.cginc"")

         // User-specified properties
         uniform sampler2D _MainTex;    
         uniform float4 _Color; 
         uniform float4 _SpecColor; 
         uniform float _Shininess;

         struct vertexInput {
            float4 vertex : POSITION;
            float3 normal : NORMAL;
            float4 texcoord : TEXCOORD0;
        };
         struct vertexOutput {
            float4 pos : SV_POSITION;
            float4 posWorld : TEXCOORD0;
            float3 normalDir : TEXCOORD1;
            float4 tex : TEXCOORD2;
        };

         vertexOutput vert(vertexInput input) 
         {
            vertexOutput output;

            float4x4 modelMatrix = unity_ObjectToWorld;
            float4x4 modelMatrixInverse = unity_WorldToObject;

            output.posWorld = mul(modelMatrix, input.vertex);
            output.normalDir = normalize(
               mul(float4(input.normal, 0.0), modelMatrixInverse).xyz);
            output.tex = input.texcoord;
            output.pos = mul(UNITY_MATRIX_MVP, input.vertex);
            return output;
         }

         float4 frag(vertexOutput input) : COLOR
         {
            float3 normalDirection = normalize(input.normalDir);

            float3 viewDirection = normalize(
               _WorldSpaceCameraPos - input.posWorld.xyz);
            float3 lightDirection;
            float attenuation;

            float4 textureColor = tex2D(_MainTex, input.tex.xy);

            if (0.0 == _WorldSpaceLightPos0.w) // directional light?
            {
               attenuation = 1.0; // no attenuation
               lightDirection = 
                  normalize(_WorldSpaceLightPos0.xyz);
            } 
            else // point or spot light
            {
               float3 vertexToLightSource = 
                  _WorldSpaceLightPos0.xyz - input.posWorld.xyz;
               float distance = length(vertexToLightSource);
               attenuation = 1.0 / distance; // linear attenuation 
               lightDirection = normalize(vertexToLightSource);
            }

            float3 ambientLighting = textureColor.rgb  
               * UNITY_LIGHTMODEL_AMBIENT.rgb * _Color.rgb;

            float3 diffuseReflection = textureColor.rgb  
               * attenuation * _LightColor0.rgb * _Color.rgb
               * max(0.0, dot(normalDirection, lightDirection));

            float3 specularReflection;
            if (dot(normalDirection, lightDirection) < 0.0) 
               // light source on the wrong side?
            {
               specularReflection = float3(0.0, 0.0, 0.0); 
                  // no specular reflection
            }
            else // light source on the right side
            {
               specularReflection = attenuation * _LightColor0.rgb 
                  * _SpecColor.rgb * (1.0 - textureColor.a) 
                     // for usual gloss maps: ""... * textureColor.a"" 
                  * pow(max(0.0, dot(
                  reflect(-lightDirection, normalDirection), 
                  viewDirection)), _Shininess);
            }

            return float4(ambientLighting + diffuseReflection 
               + specularReflection, 1.0);
         }

         ENDCG
      }

      Pass {    
         Tags { ""LightMode"" = ""ForwardAdd"" } 
            // pass for additional light sources
         Blend One One // additive blending 

          CGPROGRAM

         #pragma vertex vert  
         #pragma fragment frag 

         #include ""UnityCG.cginc""
         uniform float4 _LightColor0; 
            // color of light source (from ""Lighting.cginc"")

         // User-specified properties
         uniform sampler2D _MainTex;    
         uniform float4 _Color; 
         uniform float4 _SpecColor; 
         uniform float _Shininess;

        struct vertexInput {
            float4 vertex : POSITION;
            float3 normal : NORMAL;
            float4 texcoord : TEXCOORD0;
        };
         struct vertexOutput {
            float4 pos : SV_POSITION;
            float4 posWorld : TEXCOORD0;
            float3 normalDir : TEXCOORD1;
            float4 tex : TEXCOORD2;
        };

         vertexOutput vert(vertexInput input) 
         {
            vertexOutput output;

            float4x4 modelMatrix = unity_ObjectToWorld;
            float4x4 modelMatrixInverse = unity_WorldToObject;

            output.posWorld = mul(modelMatrix, input.vertex);
            output.normalDir = normalize(
               mul(float4(input.normal, 0.0), modelMatrixInverse).xyz);
            output.tex = input.texcoord;
            output.pos = mul(UNITY_MATRIX_MVP, input.vertex);
            return output;
         }

         float4 frag(vertexOutput input) : COLOR
         {
            float3 normalDirection = normalize(input.normalDir);

            float3 viewDirection = normalize(
               _WorldSpaceCameraPos - input.posWorld.xyz);
            float3 lightDirection;
            float attenuation;

            float4 textureColor = tex2D(_MainTex, input.tex.xy);

            if (0.0 == _WorldSpaceLightPos0.w) // directional light?
            {
               attenuation = 1.0; // no attenuation
               lightDirection = 
                  normalize(_WorldSpaceLightPos0.xyz);
            } 
            else // point or spot light
            {
               float3 vertexToLightSource = 
                  _WorldSpaceLightPos0.xyz - input.posWorld.xyz;
               float distance = length(vertexToLightSource);
               attenuation = 1.0 / distance; // linear attenuation 
               lightDirection = normalize(vertexToLightSource);
            }

            float3 diffuseReflection = textureColor.rgb  
               * attenuation * _LightColor0.rgb * _Color.rgb
               * max(0.0, dot(normalDirection, lightDirection));

            float3 specularReflection;
            if (dot(normalDirection, lightDirection) < 0.0) 
               // light source on the wrong side?
            {
               specularReflection = float3(0.0, 0.0, 0.0); 
                  // no specular reflection
            }
            else // light source on the right side
            {
               specularReflection = attenuation * _LightColor0.rgb 
                  * _SpecColor.rgb * (1.0 - textureColor.a) 
                     // for usual gloss maps: ""... * textureColor.a"" 
                  * pow(max(0.0, dot(
                  reflect(-lightDirection, normalDirection), 
                  viewDirection)), _Shininess);
            }

            return float4(diffuseReflection 
               + specularReflection, 1.0);
               // no ambient lighting in this pass
         }

         ENDCG
      }
   }
   Fallback ""Specular""
}


The final Pass of your blur shader will need to be merged into the first and second Passes in the above shader.
Shader Properties
Add these properties in the blur shader if they don't already exist (you may need to rename things if they do):
  _MainTex (""RGBA Texture For Material Color"", 2D) = ""white"" {} 
  _Color (""Diffuse Material Color"", Color) = (1,1,1,1) 
  _SpecColor (""Specular Material Color"", Color) = (1,1,1,1) 
  _Shininess (""Shininess"", Float) = 10

The first gloss pass
Set the tag to match the glossy shader: ""LightMode"" = ""ForwardBase"" 
Add these variables to the pass if they don't already exist (you may need to rename things if they do):
 uniform sampler2D _MainTex; 
 uniform float4 _Color; 
 uniform float4 _SpecColor; 
 uniform float _Shininess;

Include float3 normal : NORMAL; in your vertex input structure.
Include float4 posWorld : TEXCOORD0; and float3 normalDir : TEXCOORD1; in your vertex output structure. 
In the vert function, set output.posWorld and output.normalDir the same way the glossy shader vert does.  
Then, in the frag function, take what your blur shader is already returning, and instead of returning it, use it as the textureColor variable in the 1st frag function in the glossy shader, and then do the rest of the 1st glossy frag.
You will probably have to rename other things in the code to make it work with the pass you already have. I don't know what your blur shader looks like, so it's impossible for me to list every step in merging the two passes together.
The second gloss pass
Repeat the same process as the first gloss Pass, but using the code from the 2nd gloss Pass (especially important is the different tag ""LightMode"" = ""ForwardAdd"")
"
"
I'm trying to add PUN2 support to a simple Oculus starter scene (cubes on a table). I was able to run the official Avatar synchronization on top of my basic Photon2 setup.
This is the current sequence:

Player 1 ( P1 - the one using Quest) creates the room and joins it
Player 2 (P2 - the one using Unity Player) joins the room
Player 1 grabs a cube and moves it around

P2 sees the cube moving normally if P1 just drags it horizontally, but if he tries to lift it from the table the cube seems acting crazy from P2 point of view.
It looks like the cube fights with gravity but I'm not totally sure what is going on. You can see an example in this video where there is an alternation between horizontal drag and lifting from the table: https://gofile.io/?c=A9PN41
If I disable gravity on the cube it seems working fine (although the interpolation a bit slow compared to how P1 is moving the object): https://gofile.io/?c=jM4qEE
The point is that I cannot have cubes flying around the scene :)
Using Unity 2019.1 / PUN2 / Oculus Quest
","
For the records, I found out what is the problem but still working on a complete solution. When P1 manipulates an object (kept in sync across clients using Photon Transform View), P2 sees the cube trying to synchronize its position/rotation while fighting against gravity (creating that weird jumpy behavior). 
After an extensive I research I found out a possible approach which could solve the problem:

Add a Photon Rigid Body View (sync velocity and angular velocity)
Add an RPC (activated during manipulation) which:


Disables gravity
Enables isKinematic
Transfer ownership to the Player which is manipulating the object.


This needs to be fully tested on both clients (Master/Normal), but so far the results seem promising.
"
"
I'm working on a Unity VR framework which I'd like to have enable VR for a project as soon as the package is added. I've done some research on how to change the project settings in an editor script, but I can't find this setting anywhere in the files. This setting is found in the editor as:
Edit->Project Settings->Player->XR Settings->Virtual Reality Supported
I tried finding it in the project settings files by opening all of them in Notepad++, changing the setting in the Unity editor, and checking Notepad++ for which file it said had been changed externally. None of them were, so I'm under the impression that this setting in the editor isn't located in the project settings folder, so I have no idea what that little checkbox actually does. How can I enable VR support for the project from code?
","
Please try
PlayerSettings.virtualRealitySupported = true;

(While I didn't see this in Unity's PlayerSettings docs, I remembered the Oculus VR plugin did similar, so I dug there.)
"
"
Hello I am new to aframe and I am creating a component where when I click on a plane some animation occurs and color of plane change. It is a very simple learning application, but I see that animation__complete is fired multiple times 1st time for 1 time second time twice and so on I do not understand what an I doing wrong I have attached the log of the file here link to project is https://glitch.com/~amethyst-lamprey-mltmsu585o also this is my first time posting question on stack overflow
Console log output:
about to emit fadein

(index):43 fadeincompleted

(index):35 about to emit fadein

2(index):43 fadeincompleted

(index):35 about to emit fadein

3(index):43 fadeincompleted

(index):35 about to emit fadein

4(index):43 fadeincompleted

(index):35 about to emit fadein

5(index):43 fadeincompleted

(index):35 about to emit fadein

6(index):43 fadeincompleted

Console log image:

","
It's a jQuery usage issue. Your code is adding new event listeners each time the user clicks resulting in animations triggered multiple times. Move the code below outside of the click handler so it runs only once per animationcomplete__fadein event.
$(loading_sky).on(""animationcomplete__fadein"",()=>{
   console.log(""fadeincompleted"");
   el.setAttribute(""color"",""#""+(Math.floor(100000 + Math. random() * 900000) ).toString());
   $.each(scene,(i,e)=>{ e.emit(""fadein""); });
   setTimeout(()=>{loading_sky.emit(""loadfadeout"")},1100)});
 });

Corrected glitch
"
"
I'm using the following code to perform a raycast and detect the GameObjects that are hit:
var eventDataCurrentPosition = new PointerEventData(EventSystem.current);
eventDataCurrentPosition.position = screenPosition;
var results = new List<RaycastResult>();
EventSystem.current.RaycastAll(eventDataCurrentPosition, results);

However, I want only certain objects to be hit by the EventSystem.current.RaycastAll. Is there any way to allow only certain GameObjects to receive the raycast?
","
Two ways, one you can separate each UI categories to different Canvases and raycast from different graphicRaycaster component every time your need changes.
Or, I think the graphicRaycaster and EventSystem.current.RaycastAl both return an array of all UI objects hit, so why not just filter out manually by script based on their tags or so.
"
"
I'm trying to animate the button while it is being gazed on. I have got the following code in which I Raycast from a sphere to find the button that it hits.
var eventDataCurrentPosition = new PointerEventData(EventSystem.current);
        eventDataCurrentPosition.position = screenPosition;

var results = new List<RaycastResult>();

EventSystem.current.RaycastAll(eventDataCurrentPosition, results);

foreach (var result in results)
{
     Debug.Log(result.gameObject.name);
}  

In order to animate the button, I'm adding the following code to the button. Unfortunately, the OnPointerEnter or ``OnPointerExit is never being called.
[RequireComponent(typeof(Button))]
public class InteractiveItem : MonoBehaviour, IPointerEnterHandler, IPointerExitHandler
{
    public Image  progressImage;
    public bool   isEntered = false;
    RectTransform rt;
    Button        _button;
    float         timeElapsed;
    Image         cursor;
    float         GazeActivationTime = 5;

// Use this for initialization
void Awake()
{
    _button = GetComponent<Button>();
    rt      = GetComponent<RectTransform>();
}


void Update()
{
    if (isEntered)
    {
        timeElapsed              += Time.deltaTime;
        progressImage.fillAmount =  Mathf.Clamp(timeElapsed / GazeActivationTime, 0, 1);
        if (timeElapsed >= GazeActivationTime)
        {
            timeElapsed = 0;
            _button.onClick.Invoke();
            progressImage.fillAmount = 0;
            isEntered                = false;
        }
    }
    else
    {
        timeElapsed = 0;
    }
}

#region IPointerEnterHandler implementation

public void OnPointerEnter(PointerEventData eventData)
{
    CodelabUtils._ShowAndroidToastMessage(""entered"");
    isEntered = true;
}

#endregion

#region IPointerExitHandler implementation

public void OnPointerExit(PointerEventData eventData)
{
    CodelabUtils._ShowAndroidToastMessage(""exit"");

    isEntered                = false;
    progressImage.fillAmount = 0;
}

#endregion
}

am I missing something ? or is there any other way of achieving this?
","
I guess your Raycast supposed to trigger the OnPointerEnter on all results.
You will need to use ExecuteEvents.Execute like e.g. 
ExecuteEvents.Execute(result.gameObject, eventDataCurrentPosition, ExecuteEvents.pointerEnterHandler);

And will also at some point have to invoke pointerExitHandler.
I would therefore store a HashSet like
using System.Linq;

private HashSet<GameObject> previousEnters = new HashSet<GameObject>();

...

foreach (var result in results.Select(r => r.gameObject))
{
    Debug.Log(result.name);
    ExecuteEvents.Execute(result, eventDataCurrentPosition, ExecuteEvents.pointerEnterHandler);
    // Store the item so you can later invoke exit on them
    if(!previousEnters.Contains(result)) previousEnters.Add(result);
}  

// This uses Linq in order to get only those entries from previousEnters that are not in the results
var exits = previousEnters.Except(results);
foreach(var item in exits)
{
    if(item) ExecuteEvents.Execute(item, eventDataCurrentPosition, ExecuteEvents.pointerExitHandler);
}


You might actually want to implement your own custom PointerInputModule.

Alternative/Example
As a starter you could also use my answer to Using Raycast instead of Gaze Pointer from a while ago where I created a script based on Steam's VR Laserpointer which allows to interact with 3D objects and UI elements.

Note: Typed on smartphone but I hope the idea gets clear
"
"
I would like to Raycast from the center of Sphere GameObject to find the text/image element being hit by the ray. I'm using Physics.RaycastAll to detect the UI element being hit. Following is my code.
public class RaycastUI : MonoBehaviour
{

    public GameObject Sphere;

    private Vector3 _origin;
    private Vector3 _direction;

    void Update()
    {
        RaycastHit[] hits;

        // get the origin and direction
        _origin = Sphere.transform.position;
        _direction = Sphere.transform.forward;

        //raycast all
        hits = Physics.RaycastAll(_origin, _direction, 100.0F);

        // retrieve the names of the element that are hit by the sphere.
        for (int i = 0; i < hits.Length; i++) {
            RaycastHit hit  = hits[i];
            Debug.Log(hit.collider.name);
        }
    }
}

I have added BoxCoilloider to the canvas. Unfortunately, hits.Length always returns 0. am I missing something? How do I ray cast from the sphere to the ui element in the canvas to check if it has collided? 
","
I think you have to use the EventSystem if you want to check against UI elements.
Try something like this:
private void GetUIObjectsOfPosition(Vector2 screenPosition)
{
    var eventDataCurrentPosition = new PointerEventData(EventSystem.current);
    eventDataCurrentPosition.position = screenPosition;

    var results = new List<RaycastResult>();
    EventSystem.current.RaycastAll(eventDataCurrentPosition, results);

    foreach (var result in results) {
        Debug.Log(result.gameObject.name);
    }               
}

You can calculate the screen position from your spheres world position using Camera.WorldToScreenPoint.
"
"
I want an UI canvas to follow the camera so it will be in front of the head always and also interactable like VR menu. I'm using the following code to do so. 
public class FollowMe : MonoBehaviour
{
    public GameObject menuCanvas;
    public Camera FirstPersonCamera;
    [Range(0, 1)]
    public float smoothFactor = 0.5f;

// how far to stay away fromt he center

    public float offsetRadius   = 0.3f;
    public float distanceToHead = 4;

    public  void Update()
    {
        // make the UI always face towards the camera
        menuCanvas.transform.rotation = FirstPersonCamera.transform.rotation;

        var cameraCenter = FirstPersonCamera.transform.position + FirstPersonCamera.transform.forward * distanceToHead;

        var currentPos = menuCanvas.transform.position;

        // in which direction from the center?
        var direction = currentPos - cameraCenter;

        // target is in the same direction but offsetRadius
        // from the center
        var targetPosition = cameraCenter + direction.normalized * offsetRadius;

        // finally interpolate towards this position
        menuCanvas.transform.position = Vector3.Lerp(currentPos, targetPosition, smoothFactor);
    }
}

Unfortunately, the canvas is flickering in front fo the camera and it is not properly positioned. How do I make the menu to follow the camera?|
","
If there is no reason against it you can use a ScreenSpace - Camera canvas as stated in the docs. Then you can reference your FPS camera as the rendering camera for the canvas.
"
"
I'm using PSTools to run a VR application, developed in Unreal and using and Oculus Rift S headset. I am able to launch the app remotely but it's ignoring the VR headset. It just executes normally without VR, it doesn't render or take any input from the headset.  The command looks like this:
.\PsExec.exe -i -d \\IP ""path\NetworkTest.exe""

On the other hand, if I launch the .exe locally from my target PC, VR features work perfectly.
Am I missing something? May some execution argument? Has anyone had a similar issue?
","
In case it's useful to someone at some point. OVRPlugin was crashing at initialization and I was able to solve it by adding a user and password to the commmand, so it looks like:
.\PsExec.exe -i -d -u ""DOMAIN\username"" -p ""password"" \\192.168.0.100  ""path\NetworkTest.exe""

"
"
The Aframe API is supporting curved images. However, besides curved images I would like to make curved text and planes. Can someone help me out how to do this? 
I think I have to make a entity, but I dont know which properties must be set.
","
I'd use the https://github.com/mayognaise/aframe-html-shader to create curved text. Curved text is a partial cylinder with the material rendered on the inside using repeat: -1 1.
The HTML shader creates a texture for us using HTML which we can use for text, and then apply that texture to the cylinder. Unfortunately, we will have to apply the repeat: -1 1 manually as that option is not exposed. The rough idea...
<!DOCTYPE html>
<html>
  <head>
    <title>Hello, WebVR! - A-Frame</title>
    <meta name=""description"" content=""Hello, WebVR! - A-Frame"">
    <script src=""https://aframe.io/releases/0.6.0/aframe.min.js""></script>
    <script src=""https://unpkg.com/[email protected]/dist/aframe-html-shader.min.js"">
  </head>
  <body>
    <script>
      AFRAME.registerComponent('update-repeat', {
        dependencies: ['material'], 

        init: function () {
          var texture = this.el.components.material.material.map;
          texture.repeat = new THREE.Vector2(-1, 1);
          texture.needsUpdate = true;
        }
      });
    </script>

    <div id=""texture"" style=""width: 100%; height: 100%; position: fixed; left: 0; top: 0; z-index: -1; overflow: hidden"">
      <p style=""position: relative; top: 20px; font-size: 72px"">HELLO HELLO</p>
      <p style=""position: relative; top: 20px; font-size: 48px"">curvy text</p>
    </div>

    <a-scene>
      <a-entity geometry=""primitive: cylinder; radius: 2; segmentsRadial: 48; thetaLength: -160; openEnded: true""       
                material=""shader: html; target: #texture; side: double; width: 500; height: 300; transparent: true"" update-repeat position=""0 0 -4"" rotation=""0 -90 0""></a-entity>
      <a-sky color=""#FAFAFA""></a-sky>
    </a-scene>
  </body>
</html>

See this example glitch I made for full answer
https://aframe-curved-text.glitch.me/
"
"
The Aframe API is supporting curved images. However, besides curved images I would like to make curved text and planes. Can someone help me out how to do this? 
I think I have to make a entity, but I dont know which properties must be set.
","
I'd use the https://github.com/mayognaise/aframe-html-shader to create curved text. Curved text is a partial cylinder with the material rendered on the inside using repeat: -1 1.
The HTML shader creates a texture for us using HTML which we can use for text, and then apply that texture to the cylinder. Unfortunately, we will have to apply the repeat: -1 1 manually as that option is not exposed. The rough idea...
<!DOCTYPE html>
<html>
  <head>
    <title>Hello, WebVR! - A-Frame</title>
    <meta name=""description"" content=""Hello, WebVR! - A-Frame"">
    <script src=""https://aframe.io/releases/0.6.0/aframe.min.js""></script>
    <script src=""https://unpkg.com/[email protected]/dist/aframe-html-shader.min.js"">
  </head>
  <body>
    <script>
      AFRAME.registerComponent('update-repeat', {
        dependencies: ['material'], 

        init: function () {
          var texture = this.el.components.material.material.map;
          texture.repeat = new THREE.Vector2(-1, 1);
          texture.needsUpdate = true;
        }
      });
    </script>

    <div id=""texture"" style=""width: 100%; height: 100%; position: fixed; left: 0; top: 0; z-index: -1; overflow: hidden"">
      <p style=""position: relative; top: 20px; font-size: 72px"">HELLO HELLO</p>
      <p style=""position: relative; top: 20px; font-size: 48px"">curvy text</p>
    </div>

    <a-scene>
      <a-entity geometry=""primitive: cylinder; radius: 2; segmentsRadial: 48; thetaLength: -160; openEnded: true""       
                material=""shader: html; target: #texture; side: double; width: 500; height: 300; transparent: true"" update-repeat position=""0 0 -4"" rotation=""0 -90 0""></a-entity>
      <a-sky color=""#FAFAFA""></a-sky>
    </a-scene>
  </body>
</html>

See this example glitch I made for full answer
https://aframe-curved-text.glitch.me/
"
"
I use gazed based controls with the A-frame cursor component to navigate. While it works perfectly in my browser on desktop and mobile, the cursor is fixed and not moving in VR mode (not working on Google cardboard and neither with the Oculus browser or Firefox Reality).
Here's the code I use:
<a-camera>
  <a-cursor
    cursor=""fuse: true; fuseTimeout: 2000""
    position=""0 0 -1""
    geometry=""primitive: ring; radiusInner: 0.01; radiusOuter: 0.02""
    material=""color: red; shader: flat"">
  </a-cursor>
</a-camera>


I use the following scripts in the head of my HTML:
<script src=""https://aframe.io/releases/1.0.3/aframe.min.js""></script>
<script src=""https://unpkg.com/aframe-event-set-component@5/dist/aframe-event-set-component.min.js""></script>
<script src=""https://unpkg.com/[email protected]/dist/aframe-layout-component.min.js""></script>
<script src=""https://unpkg.com/[email protected]/dist/aframe-template-component.min.js""></script>
<script src=""https://unpkg.com/[email protected]/dist/aframe-proxy-event-component.min.js""></script>

","
It’s a bug that has been recently fixed. It will ship in 1.0.4 but you can use master builds in the meantime:
https://cdn.jsdelivr.net/gh/aframevr/aframe@130f5846ac7cb99dff0e30d703c15c046c066529/dist/aframe-master.min.js
"
"
I am currently trying to find a way for having a web browser in a VR application. Basically the goal is to open a page like https://stackoverflow.com in a panel and to have it scrollable through the oculus go controller. 
I already did some research on plugins for achieving this like this but none of them seems to work on the oculus go.
","
From what I could find, the best solution is this 3D browser plugin. It's not slow or buggy like the other options I tried and it just works by placing a prefab in your scene that you can click and scroll.
void CreateWebView(float width, float height, Vector3 position) {
    var prefab = WebViewPrefab.Instantiate(width, height);
    prefab.transform.position = position;
    prefab.transform.LookAt(Camera.main.transform);
    prefab.Initialized += (sender, args) => prefab.WebView.LoadUrl(""https://stackoverflow.com"");
}

"
"
I'm using A-Frame and I'm trying to figure out how to easily support multiple types of controllers at once (Oculus Touch, HTC Vive controllers, and Windows Mixed Reality controllers), preferably with controller models rendered in the scene and with lasers that would allow the user to click on things.
How do I do this?
","
I figured out how to do this, so here's my solution.
In your HTML, you can have these to create the controllers (this should be inside an a-scene element):
<a-entity laser-controls=""hand: left"" raycaster=""showLine: true; objects: .clickable;""></a-entity>
<a-entity laser-controls=""hand: right"" raycaster=""showLine: true; objects: .clickable;""></a-entity>

These should also render with the actual controller models in the scene, and each have a laser pointer.
This is what it looks like with the Oculus Touch controllers (ignore the other stuff in the view):

As new types of headsets come out and are supported by A-Frame (e.g. the Valve Index controllers aren't supported yet), the laser-controls component should automatically be updated to support them.
See the docs for a bit more information on how to use controllers in your A-Frame scene.
I still haven't figured out exactly how to make it possible to click on buttons or objects in the environment using the laser, I'll need to figure that out next.
"
"
I created a 3d game in unity and added the google cardboard sdk to my project. I built the game for IOS and opened it with xcode.
When I run the game the virtual glasses (the black frames on the screen for both eyes) are loaded up as well as the thin white line in the middle dividing them up. But the rest of the screen (the view of the game) remains blacked out. The game sound is playing.
I have spent the last 2h or so researching this and found no way so fix it :(
Do you know a solution?
EDIT:
This is the game log when I run the game on my iphone and have it connected to xcode.
W1028 13:14:58.468868       1 commandlineflags.cc:1311] Ignoring RegisterValidateFunction() for flag pointer 0x103dc9060: no flag found at that address
2019-10-28 13:14:58.543700+0800 3D Beginner Game nofader[6067:1880658] Built from '2019.1/staging' branch, Version '2019.1.14f1 (148b5891095a)', Build type 'Release', Scripting Backend 'il2cpp'
2019-10-28 13:14:58.548904+0800 3D Beginner Game nofader[6067:1880658] -> registered mono modules 0x1039b3710
-> applicationDidFinishLaunching()
2019-10-28 13:14:58.618918+0800 3D Beginner Game nofader[6067:1880658] Metal GPU Frame Capture Enabled
-> applicationDidBecomeActive()
GfxDevice: creating device client; threaded=1
Initializing Metal device caps: Apple A13 GPU
Initialize engine version: 2019.1.14f1 (148b5891095a)
2019-10-28 13:14:59.525441+0800 3D Beginner Game nofader[6067:1880658] INFO [vr/gvr/capi/src/gvr.cc:116] Initialized GVR version 1.170.0
2019-10-28 13:14:59.639509+0800 3D Beginner Game nofader[6067:1880658] [framework] CoreUI: RunTimeThemeRefForBundleIdentifierAndName() couldn't find Assets.car in bundle with identifier: 'com.generic.bundleidentifier'
2019-10-28 13:14:59.639745+0800 3D Beginner Game nofader[6067:1880658] [framework] CoreUI: RunTimeThemeRefForBundleIdentifierAndName() couldn't find Assets.car in bundle with identifier: 'com.generic.bundleidentifier'
Created eye textures with a ""separate"" layout.  The ""multi-pass"" stereo mode will be used.

2019-10-28 13:14:59.954764+0800 3D Beginner Game nofader[6067:1880658] [Assert] -[UIApplication setStatusBarOrientation:] and -[UIApplication setStatusBarOrientation:animated:] API have been deprecated on iOS 8.0 and are no-ops on iOS 13.0
WARNING: Shader Unsupported: 'Hidden/PostProcessing/Uber' - Pass '' has no vertex shader
WARNING: Shader Unsupported: 'Hidden/PostProcessing/Uber' - Pass '' has no vertex shader
WARNING: Shader Unsupported: 'Hidden/PostProcessing/FinalPass' - Pass '' has no vertex shader
WARNING: Shader Unsupported: 'Hidden/PostProcessing/FinalPass' - Pass '' has no vertex shader
2019-10-28 13:15:01.223206+0800 3D Beginner Game nofader[6067:1880658] [Common] _BSMachError: port 13203; (os/kern) invalid capability (0x14) ""Unable to insert COPY_SEND""
2019-10-28 13:15:01.239173+0800 3D Beginner Game nofader[6067:1880658] Unbalanced calls to begin/end appearance transitions for <SplashScreenController: 0x131f037a0>.
UnloadTime: 2.187667 ms
GVR SDK for Unity version: 1.200.1

(Filename: ./Runtime/Export/Debug/Debug.bindings.h Line: 48)

No controller support on this platform.
Gvr.Internal.ControllerProviderFactory:CreateControllerProvider(GvrControllerInput)
GvrControllerInput:Awake()

(Filename: ./Runtime/Export/Debug/Debug.bindings.h Line: 48)

","
What worked for me: 
Downgrade your version of unity to 2018.3
"
"
Aframe version: 1.0.3
Platform: android
Browser: chrome and firefox
The cursor (a-cursor tag) on mobile has gone when I enter VR mode.
Is there a way how can I enable/add the cursor when its on VR mode? or
Why the cursor has gone when I enter the VR mode. 


    <a-assets style=""display:none;visibility: hidden;opacity: 0;"" ></a-assets>

    <a-sky id=""sky"" radius=""90"" src="""" rotation=""-3.7000000000000006 192.27000000000004 -3.58"" material="""" geometry=""""
    animation__fadeIn=""property: material.color; type:color; from:#ffffff; to:#000000; dur: 500; dir: normal; easing: easeInSine; startEvents:fadeIn""
    animation__fadeOut=""property: material.color; type:color; from:#000000; to:#ffffff; dur: 500; dir: normal; easing: easeInSine; startEvents:fadeOut""></a-sky>


    <a-camera wasd-controls-enabled=""false"" mouse-cursor="""" active=""true"" look-controls="""" wasd-controls="""" data-aframe-inspector-original-camera="""" device-orientation-permission-ui="""">

        <a-cursor id=""cursorBlack"" fuse=""true"" fuse-timeout=""2000"" raycaster=""objects: .clickable"" geometry=""radiusOuter: 0.04; radiusInner: 0.00001"" material=""color: black"" position=""0 0 -2"" opacity=""0.5"" visible="""" cursor=""""></a-cursor>

        <a-cursor id=""mouseCursor""  raycaster=""objects: .clickable; useWorldCoordinates: true"" cursor=""rayOrigin: mouse""></a-cursor>

        <a-entity id=""cursorBlue"" position=""0 0 -2"" geometry=""primitive: ring; radiusInner: 0.09; radiusOuter: 0.1; thetaStart: 90"" material=""color: #439DC2"" visible=""false""
        animation__loadCursor=""property: geometry.thetaLength;  from:360; to:1; dur: 2000; dir: normal; easing: easeInSine; startEvents:loadCursor"">
        </a-entity>
    </a-camera>

    <a-entity style=""display:none; visibility: hidden;opacity: 0;"" id = ""sceneContainer"" visible = ""true""></a-entity>

</a-scene>


","
It's a known bug that has been fixed and will ship in the upcoming 1.0.4 version. You can use master builds meanwhile:
https://cdn.jsdelivr.net/gh/aframevr/aframe@2c5c131dccaab9d449fcd0383e98d027a5bc07f6/dist/aframe-master.min.js
"
"







Closed. This question needs details or clarity. It is not currently accepting answers.
                                
                            











Want to improve this question? Add details and clarify the problem by editing this post.


Closed 4 years ago.







                        Improve this question
                    



I've got an issue in my VR project. I'm trying to move one object based on rotation of another one and first one should move only in kind of borders. I'm using a raycast to detect and control these objects. Both should go in x direction.

1 - Object that I rotate
2 - Object that should move
Any tips? Thanks in advance!
","
Well this looks like a lathe so I asume that you want to move the tool post over one axis when rotating the wheel.
You want to ""link"" them, so what I would do is rotate the wheel using a method that also moves the tool post.
Try this:
public float ratio = 1.0f;
public GameObject wheel;
public GameObject tool;

public void RotateWheel(float amount)
{
     wheel.transform.Rotate(Vector3.forward * amount);
     tool.transform.Translate(Vector3.left * amount * ratio); 
}

Note the following:

Amount is passed as degrees.
Ratio is the ratio between wheel
rotation and tool translation. At 1, for each degree, the tool moves
1 unit. Adjust it at your will. I assumed the orientation of your
moving parts based on the model. Try changing the vector directions if they do not work as expected.

"
"
I can't manage to get the camera control to work on a phone using babylon defaultVRExperience
I can't understand what's missing. i've tried everything i can think of and i can't find any examples that work outside of the babylonjs playground.
Example of it working perfectly in babylon playground with just a few lines of code: https://www.babylonjs-playground.com/#VIGXA3#38
Example of same code not working outside of babylon: http://jsfiddle.net/dr3k5oqb/
Here's an example with some stuff i found in an article about making vr stuff for phones with babylon.. not working either: https://jsfiddle.net/2cdLw0tk/2/
Phone: A one+ 5 with oxygenOS 9.0.9 
Browser: Chrome Version 79.0.3945.93
Literally any help would be greatly appreciated... 
","
Chrome v76 and forward has removed usage of DeviceMotionEvent for http, meaning that vr accelerometer control for chrome only works when using https
Source: https://www.chromestatus.com/feature/5688035094036480
This can be confirmed by just switching my example links to https and they start working on chrome on my phone.
Mudin's answer could be good to look at if you want to support safari.
"
"
I'm developing a VR game using the Unreal Engine. My target device is Oculus Rift and Rift S. According to the Oculus documentation, the VR application should run at 80-90FPS. But my application(build) is running at only 60fps. But the same project in the unreal editor (engine) is running at 80+ FPS. One thing I found is that when I removed my monitor connection, the Frame rate hits 80+ (Build). Why is this happening? I thought this might be due to some bad polycount mesh, heavy postprocessing or complex material. So this time I exported/packed the sample VR Templated by Unreal Engine instead of testing my game. OMG! This build is also running at 59.1 FPS (a very basic unreal's default VR Template). I think there is something in the Unreal Engine side that caps the fps for the builds. 
System Spec:

Geforce GTX 1060 3GB | 16 GB RAM DDR3 | i7 9th gen 3.7Ghz Processor
Oculus Rift Headset Unreal Engine 4.23 Oculus Lost Frame tool and
VRCValidator were used to test the FPS. I also tried with some other 3rd party fps measuring tools.

Thanks in Advance!
","
Finally after reading a bunch of Oculus VR and Unreal docs, forums, and Reddit discussions, I found the solution for this. We need to explicitly run few console commands. 
In the level blueprint's begin play event add a few second/microsecond delay and execute the following commands one by one.

add a very small dealy node first (0.1 to 1 second is enough)
vr.bEnableHMD 1
vr.bEnableStereo or goto edito -> project settings - > enable ""Start in VR""
t.maxFPS 200 (you can give any value greater than 100 here)
r.ScreenPercentage 75 (lower the value better the performance and lower the quality too)
r.SceneRenderTargetResizeMethod 2 (for builds we need to set the render resize method)
and most importantly goto project settings-> under frame rate settings -> uncheck smooth frame rate then set minimum desired frame rate to 80+.


"
"
I've got an A-Frame scene from which the user can navigate to multiple different sub-scenes. Once the user entered VR mode, he can navigate to these sub scenes. I use window.location.href for this. Unfortunately, when entering one of the sub-scenes, VR mode has to be initiated once more. I tried to automatically enter VR mode with enterVR(); but apparently this is by browser policy not allowed. 
According to the documentation it's supposed to be possible if the browser implements the vrdisplayactivate event. BUT it's limited to Firefox desktop, Oculus Browser, Samsung Internet, and Supermedium. 
In the end my A-Frame application should be primarily used on Android. My big question is whether it is at all possible to achieve hyperlink navigation within VR on mobile, or whether I should look for an alternative approach.
Any help is appreciated!
","
Since it doesn't seem possible by web standards, I found a workaround for my problem with the help of jQuery.
I defined an a-entity with nothing but an ID inside my main scene
<a-entity id=""includedScene""></a-entity>

And instead of using window.location.href = 'xxx.html' I used jQuerys load function to replace the includedScene entity with whatever sub scene I want to load:
$(""#includedScene"").load('xxx.html');

Don't forget to include jQuery in the index.html
<script src=""path/to/jquery-3.4.1.min.js""></script> 

Maybe this is helpful for someone.
"
"
Okay, so, I got the challenge to create a program, in Unity for GearVR. 
I have to make a program which makes use of gaze input, so if you stare at an object for several seconds it'll display you a 360* video. 
I barely can't find any GearVR Gaze Input tutorials around on the web so I wanted to give it a shot on Stackoverflow, and hopefully someone could help me out! :)
","
You have to use Physics.Raycast. This method emits a ray from the camera point to the camera orientation. You can use something like that: 
// Does the Ray hit an object with a component named MyObjectScript?
RaycastHit hit;
Vector3 fwd = transform.TransformDirection(Vector3.forward);
if (Physics.Raycast(transform.position, fwd, out hit) )
{
    var script = hit.transform.GetComponent<MyObjectScript>();
    if (script != null)
    {
        //Do your stuff...
    }
}

Simply put this script on your camera in the FixedUpdate method and another script named MyObjectScript in the object you want to detect.
"
"
I have a scene that I'm working on using Steam VR 2.0, and Unity 2018.3.2f1. I have a simple statement in it that reloads the scene
private void Update()
    {
        if (Input.GetKeyDown(KeyCode.R))
        {
            SceneManager.LoadSceneAsync(""Final"");
        }
    }

The issue is: when I reload the scene, it stops responding properly. I am still able to move my head around, and hover over objects. And the objects I hover over get highlighted, but they stay highlighted. I'm not able to pick them up, or interact with them in any meaningful way, and I don't know why this is happening.
I've attached a screenshot of the issue below. 
As you can see, multiple objects are highlighted, and the hand mesh is weird:

Solutions I've tried--

Using LoadScene instead of LoadSceneAsync
Using Application.LoadScene instead
Tried to edit the Player script in SteamVR library to not add it to Don't Destroy On Load

Any suggestions?
","
The issue was arising because the Player prefab in SteamVR 2.0 had Do Not Destroy on Load checked. So, there were multiple players being instanced when I reloaded the scene. I unchecked that box, and now everything is in order.
The checkbox is located inside the [SteamVR] object under the Player prefab: 
"
"
After iterating through an array of FMotionControllerSource of an OculusInputDevice IMotionController, I found a connected Oculus Right and Left Touch Controller based on it's ETrackingStatus. With the left and right controllers, I can get the location and rotation using the IMotionController API, which Returns the calibration-space orientation of the requested controller's hand.
Here's a reference to the IMotionController API:
https://docs.unrealengine.com/en-US/API/Runtime/HeadMountedDisplay/IMotionController/index.html
I want to apply the location/rotation to a PosableMesh, so that the mesh is shown where the Oculus controller is in reality. Currently, with the code below the 3D model is displayed down from the camera, so the mapping scale is off. I think WorldToMetersScalemight be off. When I use a small number the controller doesn't move the 3D model much, but this might be messing it up.
            FVector position;
            FRotator rotation;
            int id = tracker.deviceIndex;
            FName srcName = tracker.motionControllerSource;
            bool success = tracker.motionController->GetControllerOrientationAndPosition(id, srcName, rotation, position, 250.0f);
            if (success)
            {
                poseMesh->SetWorldLocationAndRotation(position, rotation);
            }

","
Adding the camera position to the controller position seemed to fix the issue:
// get camera reference during BeginPlay:
camManager = GetWorld()->GetFirstPlayerController()->PlayerCameraManager;

// TickComponent
poseMesh->SetWorldLocationAndRotation(camManager->GetCameraLocation() + position, rotation);

"
"
I would like to play the sound get from Oculus Go's microphone through speaker in real-time, but could not made it. I have tried this code, it works well in Editor with headphone's microphone not with Oculus Go, or I have made some mistake?
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using UnityEngine.Video;

public class MicrophoneAudio : MonoBehaviour
{
    void Start()
    {
        AudioSource micAudio = GetComponent<AudioSource>();
        micAudio.clip = Microphone.Start(Microphone.devices[0], true, 10, 44100);
        micAudio.loop = true;
        while(!(Microphone.GetPosition(null) > 0)) { }
        micAudio.Play();
    }
}

Or this the device limitation? I already observed a lot but have no luck and found only start the microphone with no real time playback one. Hope anyone could help me.:(
","
This script should work on Oculus GO, which is used in my audio + video live streaming demo with Oculus GO.
AudioSource AudioMic;
void Start() {
    StartCoroutine(CaptureMic());
}

IEnumerator CaptureMic()
{
    if (AudioMic == null) AudioMic = GetComponent<AudioSource>();
    AudioMic.clip = Microphone.Start(null, true, 1, OutputSampleRate);
    AudioMic.loop = true;
    while (!(Microphone.GetPosition(null) > 0)) { }
    Debug.Log(""Start Mic(pos): "" + Microphone.GetPosition(null));
    AudioMic.Play();

    //capture for live streaming
    //while (!stop)
    //{
    //    AddMicData();
    //    yield return null;
    //}
    //capture for live streaming
    yield return null;
}

"
"
I would like to make the menu follow the camera so when the user gazes at the menu, the menu could be triggered.
I'm setting the position of the menu with reference to the FirstPersonCamera which is the main camera as follows in my Update function,
menuCanvas.transform.position = FirstPersonCamera.transform.position 
                               +(FirstPersonCamera.transform.forward * 4);

Unfortunately, the menu is sticking to the camera center always and hence it is always triggered. How do I position the contextual menu properly?
","
You can e.g. use a Vector3.Lerp in order to make the movement smoother (for far distance object moves faster, for shorter distance slower).
Then you could limit the target position not to the center of the display but allow an offset in each direction. A very simply example might look like
// how far to stay away fromt he center
public float offsetRadius = 0.3f;
public float distanceToHead = 4;

public Camera FirstPersonCamera;

// This is a value between 0 and 1 where
// 0 object never moves
// 1 object jumps to targetPosition immediately
// 0.5 e.g. object is placed in the middle between current and targetPosition every frame
// you can play around with this in the Inspector
[Range(0, 1)]
public float smoothFactor = 0.5f;

private void Update()
{
    // make the UI always face towards the camera
    transform.rotation = FirstPersonCamera.transform.rotation;

    var cameraCenter = FirstPersonCamera.transform.position + FirstPersonCamera.transform.forward * distanceToHead;

    var currentPos = transform.position;
    
    // in which direction from the center?
    var direction = currentPos - cameraCenter;

    // target is in the same direction but offsetRadius
    // from the center
    var targetPosition = cameraCenter + direction.normalized * offsetRadius;
    
    // finally interpolate towards this position
    transform.position = Vector3.Lerp(currentPos, targetPosition, smoothFactor);
}

Ofcourse this is far from perfect but I hope it is a good starting point.

A moe complex/complete solution can e.g. found in the HoloToolkit: SphereBasedTagalong (any version before it was called MixedRealityToolkit 2.x.x)

// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License. See LICENSE in the project root for license information.

using UnityEngine;

namespace HoloToolkit.Unity
{
    /// <summary>
    /// A Tagalong that stays at a fixed distance from the camera and always
    /// seeks to stay on the edge or inside a sphere that is straight in front of the camera.
    /// </summary>
    public class SphereBasedTagalong : MonoBehaviour
    {
        [Tooltip(""Sphere radius."")]
        public float SphereRadius = 1.0f;

        [Tooltip(""How fast the object will move to the target position."")]
        public float MoveSpeed = 2.0f;

        [Tooltip(""When moving, use unscaled time. This is useful for games that have a pause mechanism or otherwise adjust the game timescale."")]
        public bool UseUnscaledTime = true;

        [Tooltip(""Display the sphere in red wireframe for debugging purposes."")]
        public bool DebugDisplaySphere = false;

        [Tooltip(""Display a small green cube where the target position is."")]
        public bool DebugDisplayTargetPosition = false;

        private Vector3 targetPosition;
        private Vector3 optimalPosition;
        private float initialDistanceToCamera;

        void Start()
        {
            initialDistanceToCamera = Vector3.Distance(this.transform.position, Camera.main.transform.position);
        }

        void Update()
        {
            optimalPosition = Camera.main.transform.position + Camera.main.transform.forward * initialDistanceToCamera;

            Vector3 offsetDir = this.transform.position - optimalPosition;
            if (offsetDir.magnitude > SphereRadius)
            {
                targetPosition = optimalPosition + offsetDir.normalized * SphereRadius;

                float deltaTime = UseUnscaledTime
                    ? Time.unscaledDeltaTime
                    : Time.deltaTime;

                this.transform.position = Vector3.Lerp(this.transform.position, targetPosition, MoveSpeed * deltaTime);
            }
        }

        public void OnDrawGizmos()
        {
            if (Application.isPlaying == false) return;

            Color oldColor = Gizmos.color;

            if (DebugDisplaySphere)
            {
                Gizmos.color = Color.red;
                Gizmos.DrawWireSphere(optimalPosition, SphereRadius);
            }

            if (DebugDisplayTargetPosition)
            {
                Gizmos.color = Color.green;
                Gizmos.DrawCube(targetPosition, new Vector3(0.1f, 0.1f, 0.1f));
            }

            Gizmos.color = oldColor;
        }
    }
}


"
"







This question already has an answer here:
                                
                            




How to prepare my game for VR? [closed]

                                (1 answer)
                            

Closed 4 years ago.



I have a working c++ application using openGL and Cuda, and want to convert it to a VR application. I searched the internet and it seems that the standard way to develop VR apps is to use  a game engine (unity or unreal). Is there any other possible ways to avoid using one of these, and just convert my already existing application into VR?
","
The usual approach is, that at the start of render of a frame, you're polling the VR API for each eye's projection and ""camera"" translation matrix, then use that, to render into textures using an FBO, and at the end pass the IDs of those textures back to the API. With OpenVR:
Initialize the FBO
vr::EVRInitError err_hmd = vr::VRInitError_None;
m_hmd = vr::VR_Init(&err_hmd, vr::VRApplication_Scene);
if( err_hmd != vr::VRInitError_None ){
    m_hmd = nullptr;
}
if( m_hmd ){
    m_hmd->GetRecommendedRenderTargetSize(&m_hmd_width, &m_hmd_height);
    mat44_from_hmd44(m_prj_l, m_hmd->GetProjectionMatrix(vr::Eye_Left,  0.01f, 10.f) );
    mat44_from_hmd44(m_prj_r, m_hmd->GetProjectionMatrix(vr::Eye_Right, 0.01f, 10.f) );

    mat44_from_hmd34(m_eye_l, m_hmd->GetEyeToHeadTransform(vr::Eye_Left)  );
    mat44_from_hmd34(m_eye_r, m_hmd->GetEyeToHeadTransform(vr::Eye_Right) );

    if( !vr::VRCompositor() ){
        vr::VR_Shutdown();
        m_hmd = nullptr;
    }

    m_timer_vr = startTimer(1000/50);
} else {
}

if( m_hmd_width && m_hmd_height ){
    qDebug() << ""resize to"" << m_hmd_width << m_hmd_height;
    eye_target_textures.create(m_hmd_width, m_hmd_height);
}

Update the HMD pose:
vr::VREvent_t vrev;
while( m_hmd->PollNextEvent(&vrev, sizeof(vrev)) );

// Process SteamVR action state
// UpdateActionState is called each frame to update the state of the actions themselves. The application
// controls which action sets are active with the provided array of VRActiveActionSet_t structs.
vr::VRActiveActionSet_t actionSet = { 0 };
vr::VRInput()->UpdateActionState( &actionSet, sizeof(actionSet), 1 );

vr::TrackedDevicePose_t tdp[ vr::k_unMaxTrackedDeviceCount ];
vr::VRCompositor()->WaitGetPoses(tdp, vr::k_unMaxTrackedDeviceCount, NULL, 0);

mat4x4_translate(m_pose, 0, 0, -1);

for( int i = 0; i < vr::k_unMaxTrackedDeviceCount; ++i ){
    if( !tdp[i].bPoseIsValid ){ continue; }

    switch( m_hmd->GetTrackedDeviceClass(i) ){
    case vr::TrackedDeviceClass_HMD:
        mat44_from_hmd34(m_pose, tdp[i].mDeviceToAbsoluteTracking );
        break;
    }
}

Then when rendering you use m_pose and the m_eye… matrices.
"
"
I have trouble getting the correct screen coordinates, so I made this small MWE to replicate the kind of behavior I am seeing in my main project. I use Single Pass Rendering and a Windows Mixed Reality Headset in Unity 3D. I have a shader, which is set to a new material ""ppMaterial"", which in turn is then used in
void OnRenderImage(RenderTexture source, RenderTexture destination)
{
    Graphics.Blit(source, destination, ppMaterial);
}

In this MWE, the shader is just supposed to draw circles at the screen edges, so I made this simple shader:
Shader ""Custom/ScreenShader"" {
    Properties {
    }
    SubShader {
        Pass {
            CGPROGRAM
            #pragma fragment frag
            #pragma vertex vert
            #include ""UnityCG.cginc""

            uniform sampler2D _MainTex;
            half4 _MainTex_ST;

            struct appdata
            {
                float4 vertex : POSITION;
                float2 uv : TEXCOORD0;
            };
            struct v2f
            {
                float4 vertex : SV_POSITION;
                float2 uv : TEXCOORD0;
                float4 screenPos : TEXCOORD1;
            };

            float getDist(float2 isIn, float2 pos2){
                return sqrt((isIn.x - pos2.x)*(isIn.x - pos2.x) + (isIn.y - pos2.y)*(isIn.y - pos2.y));
            }

            float4 NearBoundary(float2 isIn, v2f i)
            {
                float2 pos2 = float2(0.5,0.5);
                float2 pos3 = float2(0,0.5);
                float2 pos4 = float2(0.5,0.0);
                float dist2 = getDist(isIn, pos2);
                float dist3 = getDist(isIn, pos3);
                float dist4 = getDist(isIn, pos4);
                if(dist2<0.1){
                    return float4(0,0.5,0.5,1);
                }
                if(dist3.x<0.3){
                    return float4(0.5,0,0.5,1);
                }
                if(dist4.x<0.3){
                    return float4(0.5,0,0.5,1);
                }

                return tex2D(_MainTex, UnityStereoScreenSpaceUVAdjust(i.uv, _MainTex_ST));
            }

            v2f vert (appdata v)
            {
                v2f o;
                o.vertex = UnityObjectToClipPos(v.vertex);
                o.screenPos = ComputeScreenPos(o.vertex);
                o.uv = v.uv;
                return o;
            }

            half4 frag(v2f i) : SV_Target {
                float2 screenPos = i.screenPos;
                return NearBoundary(screenPos, i);
            }
            ENDCG
        }
    }
}

But this does not work, the dots are not drawn at the center and the borders, respectively, where they should be. How to get the correct screen coordinates in the (0,1)-range for each eye?
","
Turns out the issue was that the camera giving the coordinates had a targettexture set, which seems to mess the coordinates up. Then, the screenPos had to be set by
screenPos.x -= unity_StereoEyeIndex * 0.5;
screenPos.x *= 2;

Also, inserting
RenderTextureDescriptor d = XRSettings.eyeTextureDesc;
d.width = d.width / 2; // THIS DOES NOT MAKE SENSE
RenderTexture t = new RenderTexture(d);

in the C#-Code as well as setting
uv.y = 1-uv.y

in the shader got me rid of all problems at last.
Posting this for other people having these super weird problems...
"
"
I attached a First-Person Camera Control script to Unity's camera for navigating in the scene. Then I imported an object from blender and placed it in the scene. When I clicked the play mode, everything works fine (i.e. navigating in the scene). But when I ran the app after build, I can't navigate and got stuck with a view to which I did not set the Unity's camera.

This is the view to which I got stuck but I did not set the Unity's camera to focus this view.
","
I found what the problem is. In Blender, there will a default camera present in the scene.

And this camera was pointing to the view below -_-

I imported the object to unity without deleting this camera which caused the trouble. After deleting this camera, my problem was solved.
"
"
I'm trying to switch scenes when an object in a VR environment is pressed. Should be a simple line of code but when I try to execute it the game crashes. The game is build to the Oculus Go.
I know I've added the scenes to the build to that shouldn't be the problem. I also got the index of '1' right in the build settings.
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using UnityEngine.SceneManagement;

public class SphereScript : MonoBehaviour
{
    public void LoadScene()
    {
        SceneManager.LoadScene(""1"");
    }
}

private void ProcessTouchpadDown()
    {
        if (!m_CurrentObject)
            return;

        Interactable interactable = m_CurrentObject.GetComponent<Interactable>();
        CubeScript.onVRTriggerDown();
        SphereScript.LoadScene();

    }
}

","
There seems to be a small mistake in SceneManager.LoadScene(""1"");. If you want to load a scene by its built number and not by its name, you'll have to put in an integer and not a string. So unless your scene is named ""1"", this won't do the trick. Try SceneManager.LoadScene(1); instead.
"
"
I'm struggling with Google VR SDK and in instance with VrVideoView. The devices on which my app is going to be installed will be for sure placed in landscape orientation and already in VR headset. For instant toggling the fullscreen stereo mode I'm using:
vrVideoView.setDisplayMode(VrWidgetView.DisplayMode.FULLSCREEN_STEREO); 

But when the video starts playing, the message ""Put your your phone into viewer"" appears for a few seconds.

I don't need it as the device is already in position. Is there a way to remove it at all? Thanks in advance.
","
You must call: setDisplayMode and setTransitionViewEnabled.
Set setTransitionViewEnabled as false.
Use like this:
vrVideoView.setDisplayMode(VrWidgetView.DisplayMode.FULLSCREEN_STEREO);
vrVideoView.setTransitionViewEnabled(false);
"
"
Using:
Unity 2018.4.1 LTS &
Oculus GO
I am using a gaze pointer to interact with Unity UI. 
This is supposed to happen when you pull the trigger, exactly how I defined it in my script: a swipe left or right on the touchpad of the go-controller however triggers clicks as well. I did not define this myself, it's just a thing that happens and I can not figure out how to work around it.
I found and fiddled around with solutions proposed in this thread and this thread.
However, none of these solutions provided me a way to get rid of these swipe clicks while keeping alive index trigger clicks.
My Input Manager (Index Trigger Interaction with UI seems to happen in OVRInputModule):

 private void Update()
        {

            if (OVRInput.GetDown(OVRInput.Button.PrimaryTouchpad) || OVRInput.GetDown(OVRInput.Button.Two))
            {
                ToggleVidInterface();

            }

            if (OVRInput.GetDown(OVRInput.Button.PrimaryIndexTrigger))
            {
                StartCoroutine(TouchpadCounter()); 
            }

        }

        IEnumerator TouchpadCounter()
        {
            int secondsForCounting = 2;
            float startTime = Time.time;
            int touchCount = 0;

            while (Time.time - startTime <= secondsForCounting && OVRInput.Get(OVRInput.Button.PrimaryIndexTrigger))
            {
                if (OVRInput.GetDown(OVRInput.Button.PrimaryTouchpad))
                {
                    touchCount++;
                    Debug.Log(""touchCount for Admin Menue: "" + touchCount);
                }

                if (touchCount >= 3)
                {
                    ActivateAdminMenue();
                }

                yield return null;
            }

            yield return null;
        }

Part of the OVRInputModule (I don't know if its the right part):
        /// <summary>
        /// Get state of button corresponding to gaze pointer
        /// </summary>
        /// <returns></returns>
        virtual protected PointerEventData.FramePressState GetGazeButtonState()
        {
            var pressed = Input.GetKeyDown(gazeClickKey) || OVRInput.GetDown(joyPadClickButton);
            var released = Input.GetKeyUp(gazeClickKey) || OVRInput.GetUp(joyPadClickButton);



#if UNITY_ANDROID && !UNITY_EDITOR
            // On Gear VR the mouse button events correspond to touch pad events. We only use these as gaze pointer clicks
            // on Gear VR because on PC the mouse clicks are used for actual mouse pointer interactions.
            pressed |= Input.GetMouseButtonDown(0);
            released |= Input.GetMouseButtonUp(0);
#endif



            if (pressed && released)
                return PointerEventData.FramePressState.PressedAndReleased;
            if (pressed)
                return PointerEventData.FramePressState.Pressed;
            if (released)
                return PointerEventData.FramePressState.Released;
            return PointerEventData.FramePressState.NotChanged;
        }

Messing with the Android if condition just enables or disables clicking as a whole (trigger and touchpad). What am I missing here? How can I seperate between a trigger click and a touchpad swipe?!
","
Just found it out:
katbot's answer is one of two parts to separate Trigger Click from Touchpad swipe Click.
First comment out in OVRInputModule.cs like so:
virtual protected PointerEventData.FramePressState GetGazeButtonState()
{
    var pressed = Input.GetKeyDown(gazeClickKey) || OVRInput.GetDown(joyPadClickButton);
    var released = Input.GetKeyUp(gazeClickKey) || OVRInput.GetUp(joyPadClickButton);

    // 👇 Comment out or remove this block, because it causes touchpad touch events to register as click events.
// #if UNITY_ANDROID && !UNITY_EDITOR
    // On Gear VR the mouse button events correspond to touch pad events. We only use these as gaze pointer clicks
    // on Gear VR because on PC the mouse clicks are used for actual mouse pointer interactions.
    //pressed |= Input.GetMouseButtonDown(0);
    //released |= Input.GetMouseButtonUp(0);
// #endif

    if (pressed && released)
            return PointerEventData.FramePressState.PressedAndReleased;
    if (pressed)
            return PointerEventData.FramePressState.Pressed;
    if (released)
            return PointerEventData.FramePressState.Released;
    return PointerEventData.FramePressState.NotChanged;
}

THEN go to your OVRInputModule in UnityEditor. Under ""Joy Pad Click Button"" now choose ""Primary Index Trigger"". Now you have killed the touchpad swipe click by commenting out the Input.GetMouseButtonDown(0) stuff, which kills normal Trigger Clicks as well. But we can bring them back by linking them to the ""Joy Pad Click Button"" in Unity Editor.
"
"







Closed. This question is opinion-based. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.


Closed 4 years ago.







                        Improve this question
                    



I am making a game for school with my friend, and we want to make it VR. Our school has no VR set but is willing to buy one for our project. We are also planning to upload our game onto Steam (I don't know if that changes anything).
I have seen how I can make a Unity game VR using VIVE and using Oculus, but I haven't seen any tutorials with both. So I was wondering if/how I can make it so that my game can use Oculus and VIVE, without having to make two different games for it.
I hope that by knowing this I can tell my school to buy the right VR set.
","
Its possible, the most common way would be to use SteamVR (aka OpenVR), it primarily targets Vive but works fine with Oculus (at least on the PC, I am not sure about Quest - its likely not supported).
Alternative is to to write your own wrappers to individual API's that would have implementation depending on the actual hardware platform. This should be possible to achieve wihtin a single ebinary but is a bit easier if you allow differen builds for each platform
"
"
I'm trying to add an object to my three.js scene for each item in an array. I can't tell if all of the object are adding to the scene because they are adding to the same position, and I can't figure out how to change the position of each object. 
Here is the code looping and adding a cube for each item in array: 
var geometry = new THREE.BoxGeometry( 1, 1, 1 );
var material = new THREE.MeshBasicMaterial( { color: 0x00ff00 } );
var cube = new THREE.Mesh( geometry, material );

getData()
async function getData() {
    const response = await fetch('/api/indexvr');
    const data = await response.json();
    console.log(data)

    for (var i=0; i<data.length; i++) {
        try {
        scene.add( cube );
        // cube.position.y = i //1++
        } catch (e) {
            console.error(e)
        }
    }
}   

camera.position.z = 5;

I've also tried declaring the cubes and their position in the for loop but that didn't work either. I need for all of the objects to be visible and for their positions to be dynamic depending on the amount in the array, e.g. something like a circle around the camera and if there are more than 10/15, increase the y axis by 4 and continue the circle again.. I can't figure out how or find documentation to help. Any ideas or links to help? Thanks in advance.
EDIT: 
the cubes now look like this with the code received from Mugen87. 

","
Try it like so (implements the feedback if @gman):
var geometry = new THREE.BoxGeometry( 1, 1, 1 );
var material = new THREE.MeshBasicMaterial( { color: 0x00ff00 } );

getData()
async function getData() {
    const response = await fetch('/api/indexvr');
    const data = await response.json();
    console.log(data)

    for (var i=0; i<data.length; i++) {

        var cube = new THREE.Mesh( geometry, material );
        cube.position.y = i;
        scene.add( cube );

    }
}   

camera.position.z = 5;

"
"
I am developing a VR application using ViroReact, https://viromedia.com/viroreact. But I am having problem with using SkyBox (Cubemap) VR scene. Everything is working fine when I used Viro360Image view. 
This is my VR scene using Viro360Image view
export default class HotelRoomVRScene extends Component {
    constructor() {
        super();
        this.state = {} // Set initial state here
    }

    render() {
      return (
        <ViroScene>
          <Viro360Image source={require('./res/hotel-room.jpg')} />
        </ViroScene>
      )
    }
}


module.exports = HotelRoomVRScene;

The above scene is working fine. I can view the VR experience in the VR headset as well. But when I changed it to the Skybox version as below. It stopped working.
export default class HotelRoomVRScene extends Component {
    constructor() {
        super();
        this.state = {} // Set initial state here
    }

    render() {
      return (
        <ViroScene>
          <ViroSkybox source={{nx: require('./res/px.jpg'),
                       px: require('./res/px.jpg'),
                       ny: require('./res/px.jpg'),
                       py: require('./res/px.jpg'),
                       nz: require('./res/px.jpg'),
                       pz: require('./res/px.jpg')}} />
        </ViroScene>
      )
    }
}

module.exports = HotelRoomVRScene;

The above code is throwing the following error.

So why is the Skybox version not working?
","
On your render method:
render() {
  return (
    <ViroScene>
      <ViroSkybox source={{nx: require('./res/px.jpg'),
                   px: require('./res/px.jpg'),
                   ny: require('./res/px.jpg'),
                   py: require('./res/px.jpg'),
                   nz: require('./res/px.jpg'),
                   pz: require('./res/px.jpg')}} />
    </ViroScene>
  )
}

you have a typo for the skybox, its  instead of  
referring to: https://docs.viromedia.com/docs/viroskybox1
and also you are exporting the same class two times, one in:
export default class HotelRoomVRScene extends Component {

the other one in :
module.exports = HotelRoomVRScene;

I suggest you remove the latter one.
"
"







Closed. This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet Stack Overflow guidelines. It is not currently accepting answers.
                                
                            











 We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.


Closed 4 years ago.







                        Improve this question
                    



What approaches can I use, and what skills/documentation/tools do I need, to build scenes for Decentraland and put them on my land?
I have been playing around with the DCL Builder because probably the only thing I can really do. I find the Builder addictive!
So the whole triangle thing is so it could load easier on VR headset right? So when designing I have to keep under those limitations?
I downloaded a recent scene I made it was a zip file then once unzipping it there were no clear files I could see I could share as visual, I was hoping to share some of the builds I've been doing with you and the group. Any assistance would be great thanks.
Is builder worthwhile to build in or should I be trying A-Frame and downloaded that? 
I know I am not going be great at the coding side - I have zero knowledge of the Decentraland SDK, but for the designing building I am very keen on making a go at that and learning as much as I can, I figure with my painting background I could be a natural.
I have some connected private land parcels in Decentraland, and contributed land to the Vegas district.
","
Yes, I agree about the Decentraland Builder. I built several scenes with it for the Decentraland Builder Hackathon and found it easy and fun and quite engaging.  For an artist, being able to create 3D works and visit them in a virtual world is a wonderful new opportunity.
There are several approaches to making scenes for Decentraland - The Decentraland Builder, the Decentraland SDK, and some 3rd party tools. I'll summarize them below, for you and other readers. I'd say they are in order of simplest to most advanced.
You can use these approaches to build scenes and ""visit"" them locally on your PC, without uploading them to land.  And, when the scene looks good, you can use either the Builder or the SDK to upload scenes to land that you own in Decentraland. You can get land at https://market.decentraland.com.  
You could become an artist or builder for other land owners.  There is a ""hiring"" channel on the Decentraland Discord server, at https://discordapp.com/invite/9EcuFgC
1. Building Scenes using the Decentraland Builder
To build a scene, go to https://builder.decentraland.org, choose the size of the scene you want to build, in parcels, and build your scene(s).
Choose things to put into your scene from the Items catalog. Find an item you want to try, and drag/drop it onto your scene
To visit your new local scene and see what it would look like in Decentraland click on the eye button (Preview). Click on the X to return to the Builder.
The scene is saved in your browser memory, but that's not a great way to back it up. But a safer way to store it is to use the Download Scene to export it to a local zip file. You can later upload the scene again by uploading this zip file using the ""UPLOAD SCENE"" feature on the main builder page.
Next, you could use the collection of files in the downloaded zip to create an SDK-compatible version of the scene to do further customization if you wish, and optionally upload it to your land in Decentraland. To do this, proceed to approach 2.
A feature to directly upload your scene from the Builder is on the roadmap and may already exist by the time you read this.
You also have the option in the builder to ""Publish"" it to a scene pool that landowners or districts may use to populate their land.
2. Building and Deploying Scenes Using the Decentraland SDK
The Decentraland SDK allows you to add custom content and, optionally, scripted dynamic behavior, to your scene. 
Good Decentraland documentation can be found at https://docs.decentraland.org.
Be sure to keep your scenes simple enough to stay under the scene limitations published there.  Especially you may have to focus on limiting ""triangle count"", which is necessary for the scene to perform well, especially when living among many other scenes ""in world"".
You will need to first install the SDK, following the instructions provided there.
You can find a growing collection of script modules being created by Decentraland builders.  Check out the builder channel in the Decentraland Discord, or sites like MetaVRS.io, or you can use and learn from example code from the Decentraland examples, which you can find at https://docs.decentraland.org/examples/sample-scenes/ 
Scripting in Decentraland is done using Typescript, which is a good extension of JavaScript.  The main starting file for a scene is the games.ts file in the src subdirectory.  
Once you have installed the SDK, you can either initiate an entirely new scene by creating a new working scene directory, and in it run dcl init; or you can create a working directory and extract into it the contents of the zip file, and in that directory type npm i
When you are ready to preview your scene in the SDK, you type dcl start
An excellent and free development environment for editing SDK scene scripts is ""Microsoft Visual Studio Code"", which you can download. It will do syntax highlighting of your code and show you where you have syntax errors. When you work on game.ts or other subsidiary code modules, the Code editor is aware of the classes and functions of the SDK, as well as of any additional code modules you ""import"" into your scene code, so it can help you get the spelling and parameters right.
If you have used the Decentraland Builder to create your scene, and don't have the desire or skills yet to further customize it, you can still preview it as above, and when you are ready to deploy an SDK scene to your land, use dcl deploy.  You will need to ""sign"" the deployment using your MetaMask or similar crypto wallet, so that it can verify that you own the land.  To specify which parcel(s) you are trying to upload it to, you will need to edit the ""scene.json"" file, and edit the ""parcels"" list, and the ""base parcel"", to both be your parcel coordinates.
3. Building Scenes with JanusVR and Converting Them to the SDK
JanusVR is a good tool that many people know how to use for making 3D/VR scenes.
You can download it for free from http://janusvr.com.
The """" code (""JML markup"") in the JanusVR scene's code editor, can be copied over the same markup in a scene you make in JanusVR's hosting environment, https://vesta.janusvr.com/, and from there you can upgrade to Vesta UV Pro, and have a feature for exporting the Vesta scene to a Decentraland SDK compatible file set (Convert to DCL)
JanusVR and/or the Vesta client can both be pointed at the Vesta URL by one or more people, enabling collaborative design teams to work together.
Be sure to only use low-poly gltf or glb model files when building in JanusVR. JanusVR supports several model file types, but only the gltf or glb formats work in the Decentraland SDK.
4. Building Scenes in Unity and Exporting Them to the SDK
I haven't tried this myself, but check this out:
https://github.com/decentraland/DecentralandUnityPlugin
Again, stick to low poly models, and know that you'll have to have them in gltf or glb format for them to work in the SDK.
5. Building Scenes in A-Frame and porting them to the SDK
If you are comfortable with both A-Frame markup scripts, and the SDK typescript scripting, and you have built scenes in A-Frame that you want to migrate to Decentraland, then you can discover the patterns of converting the A-Frame markup for models and primitives into SDK games.ts syntax.
It's a bit of work, but I've done it with a few scenes.
Again, you must use glb or gltf versions of the models and keep the triangle and other limits in mind.
I hope this helps you and others get off to a great start at building and deploying scenes to Decentraland.
"
"
I am trying to use a 3d object in a react360 project. I followed the steps in setup docs. Created a project with react-360 init Hello360 command and started the server with npm start.
Also, downloaded this obj model, and put it into static_assets folder. Then, put an Entity tag into index.js for displaying the object. Whole index.js is as follows.
import React from 'react';
import {
  AppRegistry,
  Entity,
  asset
} from 'react-360';

export default class Hello360 extends React.Component {
  render() {
    return (
      <Entity source={{ obj: asset('stickman.obj') }} />
    );
  }
};

AppRegistry.registerComponent('Hello360', () => Hello360);

As a result, I can't see anything in the browser but the background image. What am I doing wrong?
","
React360 docs Entity page is missing the import part. Importing Entity as in this blog solves the problem.
import React from 'react';
import {
  AppRegistry,
  asset,
} from 'react-360';

import Entity from 'Entity';

...

"
"
I am trying to find the gaze co-ordinates of a user wearing a HTC Vive with integrated Tobii Eyetracker. The user is viewing a 360-degree video playing inside a Unity Skybox. I am using the Tobii Pro SDK. 
I have used the VREyeTracker Prefab of Tobii and am getting parameters and values in the XML file (more details below in results). How is Unity Data different from Raw Data? Since I am not able to find any document with relevant description, I am not sure if the results obtained in the XML contain the information I am looking for. 
Unity Data:
GazeDirection Value=""(-0.05267334, 0.01541138, 0.99848940)"" Valid=""True"" />
<GazeOrigin Value=""(-0.03057870, 0.00478235, -0.04329382)"" Valid=""True"" />
<PupilDiameter Value=""0.00168019"" Valid=""True"" />
<GazeRayWorld Origin=""(-0.40116220, 1.04663500, -0.38193990)"" Direction=""(0.24876200, 0.05519217, 0.96699090)"" Valid=""True"" />

Raw Data:
<GazeDirection UnitVector=""(0.05267334, 0.01541138, 0.99848940)"" Validity=""Valid"" />
<GazeOrigin PositionInHMDCoordinates=""(30.57870000, 4.78234900, -43.29382000)"" Validity=""Valid"" />
<Pupil PupilDiameter=""1.68019100"" Validity=""Valid"" />
PupilPosition PositionInTrackingArea=""(0.55140850, 0.63337760)"" Validity=""Valid"" />

","
If you compare both data sets you can quite easily see the following relations:

Unity Data
<GazeDirection Value=""(-0.05267334, 0.01541138, 0.99848940)"" Valid=""True"" />

is related to
Raw Data
<GazeDirection UnitVector=""(0.05267334, 0.01541138, 0.99848940)"" Validity=""Valid"" />

but with inverted X axis
Unity Data
<GazeOrigin Value=""(-0.03057870, 0.00478235, -0.04329382)"" Valid=""True"" />

is ralted to
Raw Data
<GazeOrigin PositionInHMDCoordinates=""(30.57870000, 4.78234900, -43.29382000)"" Validity=""Valid"" />

but with inverted X axis and a factor 1000 (Unity measures in meters, tobii in millimeters). This name sounds like this is the origin of the gaze ray relative to the head/hmd device -> a bit (3cm) left, a bit above (1.5cm) the center of the hmd and 4cm behind the device. 
Unity Data
<PupilDiameter Value=""0.00168019"" Valid=""True"" />

is related to 
Raw Data
<Pupil PupilDiameter=""1.68019100"" Validity=""Valid"" />

again with the factor 1000

Finally 
Unity Data
<GazeRayWorld Origin=""(-0.40116220, 1.04663500, -0.38193990)"" Direction=""(0.24876200, 0.05519217, 0.96699090)"" Valid=""True"" />

sounds like it is related to the GazeOrigin but this time not relative to the HDM but in world coordinates where Direction is the fixation vector also in world space. Meaning in world coordinates you are looking 25cm to the left, 6cm up and 1meter in front of the position (-0.40116220, 1.04663500, -0.38193990) (sounds like you were sitting or are very small ;) ) in world space.
"
"
I'm using the showcase example link-traversal 
I'm trying to enable the functionality that once you peek inside the portal it takes you into that link automatically, without having to click. 
I was thinking of creating something like a ""portal peeked"" event listener, but wouldn't really know where to start with that. 
Does anybody perhaps have an idea of how I could achieve this?
","
Here is how I would achieve this: detect when the player moves over an invisible trigger geometry, ie the ""teleportation pad"", and use a raycaster to trigger the event, which can be listened for, and handled, to perform the teleporation.
1) create a pad, or plane (or any shape) that sits on the floor, placed so that when the user is over it, the teleportation will occur. Attach a class='teleport' component to its entity.
2) create a raycaster, attached to the child of the player, that shoots downward (0 -1 0) every frame. When it hits the pad, the event is thrown and the teleportation can be implemented.  
Here is one way to set this up (untested, but the general idea is there):
<a-entity id=""player"" collider-check>
  <a-entity raycaster=""objects: .teleport"" position=""0 1 0"" rotation=""-90 0 0"">  
  </a-entity>    
</a-entity>
<a-entity id=""pad"" class=""teleport"" geometry=""primitive: plane"" position=""0 0 0"" visible=""false""></a-entity>

Here you can find details on making a raycaster.
https://aframe.io/docs/0.9.0/components/raycaster.html#sidebar
Then make a custom component ('collider-check') that is attached to your player. Inside of it, add the  listener for raycasting events, and the function that handles the event with your teleportation.
AFRAME.registerComponent('collider-check', {
  dependencies: ['raycaster'],

  init: function () {
    this.el.addEventListener('raycaster-intersection', function () {
      console.log('Player hit something!');
    });
  }
});

Here is an example of a raycast event that triggers when a known target object is hit.
https://glitch.com/~rollover-rotate
This is a bit different that what you need (uses cursor, which you don't need), but it shows how to set up a raycaster, listen for a collision event, and handle it with a fuction, all in a custom component.
"
"
I try to execute an action when user press a button (An event listener needed because https://webkit.org/blog/6784/new-video-policies-for-ios/), but I guess, I have some syntax/logic mistake. Some help would be nice :D. 
Here is the code:
HTML:
   <a-image id=""playbutton_1"" position=""-5 0 0"" width=""2"" height=""2"" rotation=""0 -90 0"" ></a-image>

JS: 
var playthisvid_1 = document.querySelector('#playbutton_1');

    playthisvid_1.addEventListener('click', () => { console.log(""it clicks"");  })

","
Aframe elements won't simply react to touch / mouse events. You need a ray based cursor.
A good way to simulate the mouse is with the cursors attribute:
<a-scene cursor=""rayOrigin: mouse"">

It should also work with touch events on mobile devices.
Check it out here.

Otherwise use the a-frame cursor.
<a-cursor></a-cursor>

"
"
Since the last update of Oculus Quest (to build 7.0) I can't move both controllers at the same time in my Unity builds:

It's only being tracked the ""active"" controller
I can switch active controller by pressing the trigger of the
controller I want to set as active
This is only happening in builds of my projects in Unity, tracking of
both controllers works great with Oculus Quest applications (Home,
YoutubeVR, JobSim, etc.)

Video showing this behaviour
(Lightweight VR RP default scene. It's the same behaviour with VRTK example scene)
Someone else is having this problem? What could be the cause?
","
Solution:
(In Unity project)

Oculus > Tools > Remove AndroidManifest.xml
Oculus > Tools > Create store-compatible AndroidManifest.xml
Rebuild

"
"
for interacting with an object in Unity via LeapMotion controller you need to attach the ""IneractionBehavior"" script. With this script you can create new interaction events with the unity GUI. 
What i want is to generate a couple of GameObjects with an attached InteractionBehavior script and i want to add these types of events via C# code.
For example i want to generate automaticaly an object with a attached InteractionBehavior script and want to define per code that this object turns to red by BeginContact.
I use: 
- Unityversion 2019.1.10
- LeapMotion Core Asset v4.4.0
- LeapMotion InteractionEngine v1.2.0
- HTC Vive Pro
If it is possible i want to know how i can add an interactionevent to an attached InteractionBehavior script with C#
","
This is the implementation of BeginContact in the InteractionBehaviour :
public void BeginContact(List<InteractionController> controllers) 
{
    foreach (var controller in controllers) 
    {
        _contactingControllers.Add(controller);

        OnPerControllerContactBegin(controller);
    }

    if (_contactingControllers.Count == controllers.Count) 
    {
        OnContactBegin();
    }
}

as you can see the according actions called are
public Action OnContactBegin;
public Action<InteractionController> OnPerControllerContactBegin;

So depending on your needs either
Directly add callbacks via script
e.g. using
private void Awake()
{
    var interactionBehaviour= objectReference.GetComponent<InteractionBehaviour>();

    // either as Lambda Expression
    interactionBehaviour.OnContactBegin += () =>
        {
            // do something here
        };

    // or with a method
    // use this if you also want to be able to remove callbacks later!
    // it is always save to remove a callback before adding it
    // even if it's not there yet. Makes sure it is only added exactly once
    interactionBehaviour.OnContactBegin -= DoSomething;
    interactionBehaviour.OnContactBegin += DoSomething;
}

// make sure to always remove any callbacks as soon as possible or when they are not needed anymore
private void OnDestroy()
{
    interactionBehaviour.OnContactBegin -= DoSomething;
}


private void DoSomething()
{
    // do something here e.g. turn red
    // for efficiency you would ofcourse move the GetComponent
    // call to Awake as well. Just wanted to keep the example clean
    GetComponent<Renderer>().material.color = Color.red;
}

Or make it more general
Inherit from the script and customize it with UnityEvents which bring their own Inspector visiable interface (exactly like the Button.onClick event).
Theoretically you could add UnityEvent for every single Action available .. I will just do it for the two above.
[Serializeable]
public class InteractionControllerEvent : UnityEvent<InteractionController> { }

public class ExtendedInteractionBehaviour : InteractionBehaviour
{
    public UnityEvent OnContactBeginEvent;
    public InteractionControllerEvent OnPerControllerContactBegin;

    // and just as before add those as callbacks to the actions
    private void Awake()
    {
        OnContactBegin -= OnContactBeginInvokeEvent;
        OnContactBegin += OnContactBeginInvokeEvent;

        OnPerControllerContactBegin -= OnPerControllerContactBeginInvokeEvent;
        OnPerControllerContactBegin += OnPerControllerContactBeginInvokeEvent;
    }

    private void OnDestroy()
    {
        OnContactBegin -= OnContactBeginInvokeEvent;
        OnPerControllerContactBegin -= OnPerControllerContactBeginInvokeEvent;
    }

    private void OnContactBeginInvokeEvent()
    {
        OnContactBeginEvent?.Invoke();
    }

    private void OnPerControllerContactBeginInvokeEvent(InteractionController controller)
    {
        OnPerControllerContactBegin?.Invoke(controller);
    }
}

Now you could simply reference GameObjects and their components & methods via the Inspector as you probably know it from the mentioned UI.Button.onClick event (it also is an UnityEvent).
Especially the InteractionControllerEvent is an example that allows to be called with an dynamic parameter, meaning the according InteractionController reference can be passed on to a referenced method taking a InteractionController as paramter.
"
"
I've got a object which has the x rotation same as controller of vr.
transform.rotation = Quaternion.Euler(controller.transform.rotation.eulerAngles.x, 0, 0);

But I need to rotate it 90 degrees that the object will be vertically not horizontally.
transform.rotation = Quaternion.Euler(controller.transform.rotation.eulerAngles.x + 90, 0, 0);

But this doesn't work properly, because you can rotate the object in only one direction.
If you rotate controller right -> object rotate right & if you rotate controller left -> object rotate right.
If you want to rotate object left you have to rotate controller upside down.
I also tried set the object as child of controller but on play the object disappear.
How can I solve?
Thanks in advance.
","
You could assign your rotation to be equal to the controller's right vector.
this.transform.rotation =  Quaternion.LookRotation(controller.transform.right, Vector3.up);

"
"
I'm trying to implement a poor man's AR by showing some objects with A-Frame on top of the active video.
Note: I'm not using ar.js because I don't want to use a marker.
All in all, app is working nicely, but video is half transparent instead of fully opaque as I want it to be.
The problem is that I'm showing the video on top of the A-Frame scene, and making it semi transparent so that the 3d scene is visible through it.
I did it because if the video is below the scene (i.e with a lower z-index), it is not shown, even though I do put background=""transparent"" for the a-scene. If I add ""embedded"" to a-scene, the 3d elements are not appearing.
Here is my code:
<!doctype html>
<html lang=""en"">
<head>
    <script src=""https://aframe.io/releases/0.9.2/aframe.min.js""></script>
    <style>
        a-scene {
            height: 50%; width: 50%;
            z-index: 10;
            opacity:1
        }      
        #webcam {
            z-index: 20;
            opacity:0.4;
            position: absolute;
            background-size: 100% 100%;
            top: 0; left: 0; 
            min-width: 100%; min-height: 100%;
            width: auto; height: auto;
        }
    </style>
    <script>
        var cameraView;

        var constraints = {
            audio: false,
            video: {
                facingMode: ""environment"",
            }
        };

        // Access the device camera and stream to cameraView
        function cameraStart() {
            cameraView = document.querySelector(""#webcam"");
            navigator.mediaDevices
                .getUserMedia(constraints)
                .then(function(stream) {
                cameraView.srcObject = stream;
            })
            .catch(function(error) {
                console.error(""Oops. Something is broken."", error);
            });
        }

        // Start the video stream when the window loads
        window.addEventListener(""load"", cameraStart, false);

        // Component to change to a sequential color on cursor suspension.
        AFRAME.registerComponent('cursor-listener', {
          init: function () {
            var lastIndex = -1;
            var COLORS = ['red', 'green', 'blue'];
            this.el.addEventListener('click', function (evt) {
              lastIndex = (lastIndex + 1) % COLORS.length;
              this.setAttribute('material', 'color', COLORS[lastIndex]);
              console.log('I was clicked at: ', evt.detail.intersection.point);
            });
          }
        });
    </script>      
</head>
<body>
    <video id=""webcam"" autoplay playsinline></video>
    <a-scene vr-mode-ui=""enabled: false"" background=""transparent"">
        <a-sphere cursor-listener position=""-7 0 7"" radius=""1.25"" color=""yellowgreen""></a-sphere>
        <a-sphere cursor-listener position=""-7 0 -7"" radius=""1.25"" color=""green""></a-sphere>
        <a-sphere cursor-listener position=""7 0 7"" radius=""1.25"" color=""aqua""></a-sphere>
        <a-sphere cursor-listener position=""7 0 -7"" radius=""1.25"" color=""orange""></a-sphere>
        <a-entity camera look-controls>
            <a-entity cursor=""fuse: true; fuseTimeout: 500""
                position=""0 0 -0.6""
                geometry=""primitive: ring; radiusInner: 0.02; radiusOuter: 0.03""
                material=""color: blue; shader: flat"">
            </a-entity>
        </a-entity>
    </a-scene>
</body>
</html>

","
You don't need to manipulate the z-index, or embed the scene. With a setup like this:
<video></video>
<a-scene>
</a-scene>

You only need to make sure that:

the <video> element is covering the entire viewport preferably with its width (100%, 100vw etc.), height (100%, 100vh, etc,) and position (absolute, or fixed) set.
no entity in the scene is covering the background ( like a plane, or <a-sky> )

Glitch here. Also here's a similar anwser + examples.
"
"
I'm making a VR experience in Unity (version 2019.1.12f), for the Oculus Quest (android build). Some of my objects have white lines in their edges (see below), and I want to know how to get rid of them.
Whitelines: https://i.ibb.co/bXsN9J2/whitelines.png
The quality is maxed out and the anti-aliasing is disabled in the image but even with 8x I was getting the lines. Using baked global lighting, and all the objects are static.  Also see my Quality Settings, Lighting Settings and Directional Light Settings. 
Quality: https://i.ibb.co/ySHfgcD/Quality.png
Lighting: https://i.ibb.co/dKBQbpG/Lighthing.png
Directional: https://i.ibb.co/ZfDJPSz/Directional.png
Here's a overview of the Scene and all the lights present:
Scene: https://i.ibb.co/x6YtXTv/Scene.png
One last thing.. I read somewhere this could be related to Shaders but as I have no knowledge in that area I would prefer other options.
Thanks in advance!

Removing the anti-aliasing doesn't solve the problem.
I'm using ASTC compression on the textures.

","
Had to go to the Nvidia panel and activate anti-aliasing and ambient occlusion, problem solved!
"
"
I am trying to build a sample project which can be found at this link https://www.coursera.org/learn/xr-introduction/supplement/cARor/project-work-build-a-vr-application-to-your-device and I am getting this error:
Build completed with a result of 'Failed'
UnityEditor.BuildPlayerWindow:BuildPlayerAndRun()
UnityEditor.BuildPlayerWindow+BuildMethodException: 4 errors
at UnityEditor.BuildPlayerWindow+DefaultBuildMethods.BuildPlayer (UnityEditor.BuildPlayerOptions options) [0x00242] in C:\buildslave\unity\build\Editor\Mono\BuildPlayerWindowBuildMethods.cs:194 
at UnityEditor.BuildPlayerWindow.CallBuildMethods (System.Boolean askForBuildLocation, UnityEditor.BuildOptions defaultBuildOptions) [0x0007f] in C:\buildslave\unity\build\Editor\Mono\BuildPlayerWindowBuildMethods.cs:97 
UnityEditor.BuildPlayerWindow:BuildPlayerAndRun()
What to do?
","
Try to build in a different location? Preferably outside your project folder. 
"
"
I‘m using superframe‘s orbit-controls (https://github.com/supermedium/superframe/tree/master/components/orbit-controls) and I have a scene with two entities that the user can toggle on/off with a button. Whenever the user clicks on the button, I‘d like to reset the camera position to its initial state. 
I've tried resetting the camera's world position like I normally would without using the orbit controls, but so far I haven't had any luck.
Here's a jsfiddle of the situation: https://jsfiddle.net/e7akdgf1/5/
<a-scene embedded class=""box"" vr-mode-ui=""enabled: false"" loading-screen=""enabled: false"">

<a-entity id=""cam""
camera
orbit-controls=""target: 0 0 0;
initialPosition: 0 0 5;
minDistance: 1;
maxDistance: 50;"">

</a-entity>

<a-entity id=""cube"" position=""0 0 0"" rotation=""45 30 0"" geometry=""primitive: box; height: 3; width: 3; depth: 3;"" material=""color: #167341; roughness: 1.0; metalness: 0.2;""></a-entity>

<a-cylinder id=""cylinder"" position=""1 1.25 -1"" radius=""1"" height=""2.5"" color=""#FFC65D"" visible=""false""></a-cylinder>

</a-scene>

","
You can change the position like this
// check what setting the initial position does in the source code 
cam.getObject3D('camera').position.set(0, 0, 5)

fiddle here.

There is a 
this.el.components['orbit-controls'].controls.reset()

in the source code, but I had no luck with it.
"
"
I am working on HoloLens (Unity-UWP) and trying to make a connection with PC (UWP) or Android phone work (Xamarin). So far I tried client and host with both Bluetooth and TCP (even two versions with different libraries) on Android and UWP. I kept the code entirely separated from user interface, so that it is easier to use, to understand and modular. An Action<string> is used to output results (error logs and sent messages).
Everything that is not on the HoloLens works fine (even though it's exactly the same code). It worked from PC (UWP) to Android with client and host switched. But it doesn't even work between HoloLens and PC (UWP). The behavior ranged from crashes (mostly for Bluetooth) to instant disconnection. The last tests resulted in disconnection once bytes are about to be received. It could even read the first 4 bytes (uint for the length of the following UTF-8 message), but then it was disconnected. The other devices seemed to work fine.
What I know: Capabilities are set, the code works, the issue is likely something that is common for everything that has to do with networking and HoloLens.
So the question is, is Unity or HoloLens incompatible with something I am using? What I used which is worth mentioning: StreamSocket, BinaryWriter, BinaryReader, Task (async, await). Or is HoloLens actively blocking communication with applications on other devices? I know it can connect to devices with Bluetooth and that it can connect via TCP, and it looks like people succeed to get it to work. Are there known issues? Or is there something with Unity that causes this - a bad setting maybe? Do I have to use async methods or only sync? Are there incompatibility issues with Tasks/Threads and Unity? Is this possibly the issue (inability to consent to permissions)?
Another thing to note is that I cannot ping HoloLens via its IP by using the cmd, even though the IP is correct.
I'd appreciate any advice, answer or guess. I can provide more information if requested (see also the comments below). I would suggest to focus on the TCP connection as it seemed to be working better and appears to be more ""basic."" Here is the code:
using System;
using System.Text;
using System.Threading.Tasks;
using System.IO;
using Windows.Networking;
using Windows.Networking.Sockets;

#region Common
public abstract class TcpCore
{
    protected StreamSocket Socket;
    protected BinaryWriter BWriter;
    protected BinaryReader BReader;
    protected Task ReadingTask;

    public bool DetailedInfos { get; set; } = false;
    public bool Listening { get; protected set; }

    public ActionSingle<string> MessageOutput { get; protected set; } = new ActionSingle<string> (); // Used for message and debug output. They wrap an Action and allow safer use.
    public ActionSingle<string> LogOutput { get; protected set; } = new ActionSingle<string> ();

    protected const string USED_PORT = ""1337"";
    protected readonly Encoding USED_ENCODING = Encoding.UTF8;

    public abstract void Disconnect ();

    protected void StartCommunication ()
    {
        Stream streamOut = Socket.OutputStream.AsStreamForWrite ();
        Stream streamIn = Socket.InputStream.AsStreamForRead ();
        BWriter = new BinaryWriter (streamOut); //{ AutoFlush = true };
        BReader = new BinaryReader (streamIn);

        LogOutput.Trigger (""Connection established."");
        ReadingTask = new Task (() => StartReading ());
        ReadingTask.Start ();
    }

    public void SendMessage (string message)
    {
        // There's no need to send a zero length message.
        if (string.IsNullOrEmpty (message)) return;

        // Make sure that the connection is still up and there is a message to send.
        if (Socket == null || BWriter == null) { LogOutput.Trigger (""Cannot send message: No clients connected.""); return; }

        uint length = (uint) message.Length;
        byte[] countBuffer = BitConverter.GetBytes (length);
        byte[] buffer = USED_ENCODING.GetBytes (message);

        if (DetailedInfos) LogOutput.Trigger (""Sending: "" + message);

        BWriter.Write (countBuffer);
        BWriter.Write (buffer);
        BWriter.Flush ();
    }

    protected void StartReading ()
    {
        if (DetailedInfos) LogOutput.Trigger (""Starting to listen for input."");
        Listening = true;
        while (Listening)
        {
            try
            {
                if (DetailedInfos) LogOutput.Trigger (""Starting a listen iteration."");

                // Based on the protocol we've defined, the first uint is the size of the message. [UInt (4)] + [Message (1*n)] - The UInt describes the length of the message (=n).
                uint length = BReader.ReadUInt32 ();

                if (DetailedInfos) LogOutput.Trigger (""ReadLength: "" + length.ToString ());

                MessageOutput.Trigger (""A"");
                byte[] messageBuffer = BReader.ReadBytes ((int) length);
                MessageOutput.Trigger (""B"");
                string message = USED_ENCODING.GetString (messageBuffer);
                MessageOutput.Trigger (""Received Message: "" + message);
            }
            catch (Exception e)
            {
                // If this is an unknown status it means that the error is fatal and retry will likely fail.
                if (SocketError.GetStatus (e.HResult) == SocketErrorStatus.Unknown)
                {
                    // Seems to occur on disconnects. Let's not throw().
                    Listening = false;
                    Disconnect ();
                    LogOutput.Trigger (""Unknown error occurred: "" + e.Message);
                    break;
                }
                else
                {
                    Listening = false;
                    Disconnect ();
                    break;
                }
            }
        }
        LogOutput.Trigger (""Stopped to listen for input."");
    }
}
#endregion

#region Client
public class GTcpClient : TcpCore
{
    public async void Connect (string target, string port = USED_PORT) // Target is IP address.
    {
        try
        {
            Socket = new StreamSocket ();
            HostName serverHost = new HostName (target);
            await Socket.ConnectAsync (serverHost, port);

            LogOutput.Trigger (""Connection successful to: "" + target + "":"" + port);
            StartCommunication ();
        }
        catch (Exception e)
        {
            LogOutput.Trigger (""Connection error: "" + e.Message);
        }
    }

    public override void Disconnect ()
    {
        Listening = false;
        if (BWriter != null) { BWriter.Dispose (); BWriter.Dispose (); BWriter = null; }
        if (BReader != null) { BReader.Dispose (); BReader.Dispose (); BReader = null; }
        if (Socket != null) { Socket.Dispose (); Socket = null; }
        if (ReadingTask != null) { ReadingTask = null; }
    }
}
#endregion

#region Server
public class GTcpServer : TcpCore
{
    private StreamSocketListener socketListener;

    public bool AutoResponse { get; set; } = false;

    public async void StartServer ()
    {
        try
        {
            //Create a StreamSocketListener to start listening for TCP connections.
            socketListener = new StreamSocketListener ();

            //Hook up an event handler to call when connections are received.
            socketListener.ConnectionReceived += ConnectionReceived;

            //Start listening for incoming TCP connections on the specified port. You can specify any port that's not currently in use.
            await socketListener.BindServiceNameAsync (USED_PORT);
        }
        catch (Exception e)
        {
            LogOutput.Trigger (""Connection error: "" + e.Message);
        }
    }

    private void ConnectionReceived (StreamSocketListener listener, StreamSocketListenerConnectionReceivedEventArgs args)
    {
        try
        {
            listener.Dispose ();
            Socket = args.Socket;
            if (DetailedInfos) LogOutput.Trigger (""Connection received from: "" + Socket.Information.RemoteAddress + "":"" + Socket.Information.RemotePort);
            StartCommunication ();
        }
        catch (Exception e)
        {
            LogOutput.Trigger (""Connection Received error: "" + e.Message);
        }
    }

    public override void Disconnect ()
    {
        Listening = false;
        if (socketListener != null) { socketListener.Dispose (); socketListener = null; }
        if (BWriter != null) { BWriter.Dispose (); BWriter.Dispose (); BWriter = null; }
        if (BReader != null) { BReader.Dispose (); BReader.Dispose (); BReader = null; }
        if (Socket != null) { Socket.Dispose (); Socket = null; }
        if (ReadingTask != null) { ReadingTask = null; }
    }
}
#endregion

","
The answer is Threading.
For whoever may have similar issues, I found the solution. It was due to Unity itself, not HoloLens specifically. My issue was that I wrote my code separately in an own class instead of commingle it with UI code, which would have made it 1. unreadable and 2. not modular to use. So I tried a better coding approach (in my opinion). Everybody could download it and easily integrate it and have basic code for text messaging. While this was no issue for Xamarin and UWP, it was an issue for Unity itself (and there the Unity-UWP solution as well).
The receiving end of Bluetooth and TCP seemed to create an own thread (maybe even something else, but I didn't do it actively), which was unable to write on the main thread in Unity, which solely handles GameObjects (like the output log). Thus I got weird log outputs when I tested it on HoloLens.
I created a new TCP code which works for Unity but not the Unity-UWP solution, using TcpClient/TcpListener, in order to try another version with TCP connection. Luckily when I ran that in the editor it finally pointed on the issue itself: The main thread could not be accessed, which would have written into the UI - the text box for the log output. In order to solve that, I just had to use Unity's Update() method in order to set the text to the output. The variables themselves still could be accessed, but not the GameObjects.
"
"
I am trying to create a script that will reset (to a specific location) the HMD and controller locations whenever a key is pressed for calibration reasons. I am very new to unity so all I have been able to figure out is how to get key input.
public class resetPosition : MonoBehaviour
{
    // Start is called before the first frame update
    void Start()
    {

    }

    // Update is called once per frame
    void Update()
    {
        if (Input.GetKeyDown(KeyCode.Space))
            Debug.Log(""pressed"");
    }
}

","
You shouldn't directly change the position of the VRCamera. 
Rather add a parent GameObject to the camera and change the position of that one instead via e.g. (assuming your script is attahced to the camera)
public class ResetPosition : MonoBehaviour
{
    public Vector3 resetPosition;

    private void Awake()
    {
        // create a new object and make it parent of this object
        var parent = new GameObject(""CameraParent"").transform;

        transform.SetParent(parent);
    }

    // You should use LateUpdate
    // because afaik the oculus position is updated in the normal
    // Update so you are sure it is already done for this frame
    private void LateUpdate()
    {
        if (Input.GetKeyDown(KeyCode.Space))
        {
            Debug.Log(""pressed"");

            // reset parent objects position
            transform.parent.position = resetPosition - transform.position;
        }
    }
}

"
"
I want to lock the player position of the Oculus rift in unity to be in one place the entire time. They are able to move their arms but can look with their head, they also cannot lean or turn their body. For example if they are sitting in a chair the player is locked to that chair and they cannot move from it but they can interact with objects in front of the chair using the controllers.
I have no clue how to do this. Either through code or the inspector. Any solution will be greatly appreciated.
","
I have included a solution to this on a different question I asked and answered. Please see this post for a solution to this.
"
"
Earlier I was facing problem regarding the unity camera problem it always stuck on 0,0,0.08 and also find a solution so I first create an empty gameobject and then drag the camera in that empty gameobject but after doing this the scripts which I applied to the gameobject is working fine but the script which I place in camera is not working at all 
Camera Script 
public float MovementAmplitude = 0.1f;
public float MovementFrequency = 2.25f;
void Update()
{
    transform.position = new Vector3(
         transform.position.x,
         Mathf.Cos(transform.position.z * MovementFrequency) * MovementAmplitude,
         transform.position.z

        );
}

Player Script
public float speed = 4.5f;
public float JumpingForcec = 450f;
void Update()
{
    transform.position += speed * Vector3.forward * Time.deltaTime;
    if (Input.GetKeyDown(""space""))
    {
        Debug.Log(""SPace is pressed"");
        Debug.Log(GetComponent<Rigidbody>());
        GetComponent<Rigidbody>().AddForce(Vector3.up * JumpingForcec);
    }
}

","
First of all when dealing with a Rigidbody (or the Physics in general) you shouldn't set a position directly through the Transform component but rather use Rigidbody.position or in your case for a smooth movement even rather Rigidbody.MovePosition, both in FixedUpdate. 
In general anything related to the Physics (so also everything using Rigidbody) should be done in FixedUpdate while the check for GetKeyDown has to be done in Update.
PlayerScript
public class PlayerScript : MonoBehaviour
{
    public float speed = 4.5f;
    public float JumpingForcec = 450f;

    // If possible reference this in the Inspector already
    [SerializeField] private Rigidbody rigidBody;

    private bool jumpWasPressed;

    private void Awake()
    {
        if (!rigidBody) rigidBody = GetComponent<Rigidbody>();
    }

    private void FixedUpdate()
    {
        rigidBody.MovePosition(transform.position + speed * Vector3.forward * Time.deltaTime);

        if (!jumpWasPressed) return;

        Debug.Log(""SPace was pressed"");

        rigidBody.AddForce(Vector3.up * JumpingForcec);

        jumpWasPressed = false;
    }

    private void Update()
    {
        // Note that currently you can multijump .. later you will want to add 
        // an additional check if you already jump again
        if (Input.GetKeyDown(KeyCode.Space)) jumpWasPressed = true;
    }
}

Make sure that Is Kinematic is disabled in the Rigidbody component! Otherwise AddForce is not processed.

If isKinematic is enabled, Forces, collisions or joints will not affect the rigidbody anymore.

The camera movement I would move to LateUpdate in order to make sure it is the last thing calculated after the other Update calls have finished. Especially after all user input has been processed (in your case maybe not that relevant since movement is processed in FixedUpdate but in general).
Second problem: Here you are not taking the changed Y position by jumping into account so rather add the ""wobbling"" effect to the player's transform.position.y and rather use the localPosition for the Camera:
public class CameraScript : MonoBehaviour
{
    public float MovementAmplitude = 0.1f;
    public float MovementFrequency = 2.25f;

    // reference the player object here
    public Transform playerTransform;

    private float originalLocalPosY;

    private void Start()
    {
        if(!playerTransform) playerTransform = transform.parent;
        originalLocalPosY = transform.localPosition.y;
    }

    private void LateUpdate()
    {
        transform.localPosition = Vector3.up * (originalLocalPosY + Mathf.Cos(playerTransform.position.z * MovementFrequency) * MovementAmplitude);
    }
}


Maybe you want to disable the wobbling effect during a jump later, though ;)

"
"
How to access the cardboard trigger on google Gvr( in code) ? In the earlier version it was GvrViewer.Instance.Triggered
","
I use Input.GetButtonDown(""Fire 1"") and it works for me on the latest SDK version (1.6) and latest Unity version (5.6)
EDIT:
Use Input.GetButtonDown(""Fire1"") if you follow the default naming conventions for input events supported by unity. Please note that there is no space character between ""Fire"" and ""1"".
Otherwise, you might the error - ""ArgumentException: Input Button Fire 1 is not setup.""
"
"
So, I have two game objects P1 and P2, these two game objects are associated with the trackers on my left leg and right leg respectively.
So, I need to find the angle between them.

So, this is how my P1 and P2 would look if I face forward and take my right leg front.
private double calculateAngle(double P1X, double P1Y, double P2X, double P2Y,
        double P3X, double P3Y)
{

    double numerator = P2Y * (P1X - P3X) + P1Y * (P3X - P2X) + P3Y * (P2X - P1X);
    double denominator = (P2X - P1X) * (P1X - P3X) + (P2Y - P1Y) * (P1Y - P3Y);
    double ratio = numerator / denominator;

    double angleRad = Math.Atan(ratio);
    double angleDeg = (angleRad * 180) / Math.PI;

    if (angleDeg < 0)
    {
        angleDeg = 180 + angleDeg;
    }

    return angleDeg;
}

    calculateAngle(LeftLegController.position.x, LeftLegController.position.z,
              RightLegController.position.x, RightLegController.position.z, 
        LeftLegController.position.x, RightLegController.position.z))

It should always be a right angle triangle due to this.
So, this is the code I'm using to calculate the angle between my P1 and P2,
When I face front, the angle between my two game objects is different and when I face left and move my right leg front (Which would look like this)

My Angle 1 and Angle 2 are coming entirely different. So, what is a better way to find an angle (Ignoring the y axis, its as if the points are projected on the ground)
","
I believe the method you're looking for is Vector3.SignedAngle().
var A = Vector3.ProjectOnPlane(P3.position - P1.position, Vector3.up);
var B = Vector3.ProjectOnPlane(P2.position - P1.position, Vector3.up);
print(Vector3.SignedAngle(A, B, Vector3.up));

"
"
I'm working on a walkthrough of a three-story apartment. Navigation constraints are done via a navmesh that covers every floor including the staircase.
I want to provide a shortcut where the user can click on a button and jump to the respective floor. So far the changing of position works but as soon as the player starts walking, they are pulled back to the same Y-position that they’ve been at before clicking the button (x- and z-position change works). For example, the jump from the first to the third floor works, but as soon as the player starts walking, they're back on the first floor again. 
The position.set values aren't chosen randomly – I checked the position of the camRig on the third floor in the loading inspector and used these values, so I'm not sure why this doesn't work. Is there any way to avoid this issue?
Edit: I made a quick example that reproduces the problem: https://glitch.com/~fearless-resolution 
This is my code:
    <a-entity id=""camRig"" 
               movement-controls=""speed: 0.3; 
               constrainToNavMesh: true"" 
               position=""-12.5 0 -2"" 
               rotation=""0 270 0"">
      <a-entity id=""cam"" camera 
              look-controls=""pointerLockEnabled: true;"" 
              position=""0 2.4 0""></a-entity>
    </a-entity>

<a-entity id=""navmesh"" gltf-model=""#collider"" nav-mesh position=""0 0 0"" scale=""1.6 1.6 1.6"" visible=„false“></a-entity>
<a-entity id=""house"" gltf-model=""#apartment"" position=""0 0 0"" scale=""0.016 0.016 0.016""></a-entity>



document.querySelector(""#changeFloorButton"").addEventListener('click', function() {
          document.querySelector(""#camRig"").object3D.position.set(-1, 4, 2);
      });

","
When 'teleporting' around, you need to reset the navigation data:
// provided player has the camRig
player.components['movement-controls'].updateNavLocation()

otherwise the calculations keeping you within the navmesh will teleport you back if you move too soon (seems so in the primitives example).
Fiddle with primitives here.
Fiddle with your stuff here
"
"

So I have a controller and a cube, I want the cube at the right hand side of the controller, and need the cube to face the controller
In Second picture, it isn't facing the controller
I don't get why it isn't rotating around y axis. It is at the right hand side of the controller though, but only the rotation is the problem
void LateUpdate()
{
    RightCube.transform.position = RightHandController.position + RightHandController.right * 2;

      RightCube.rotation = Quaternion.Euler(RightCube.rotation.x, RightHandController.rotation.z, RightCube.rotation.z);


}

","
Because you know the world location you want the RightCube's forward to face, you can just use Transform.LookAt:
void LateUpdate()
{
    RightCube.transform.position = RightHandController.position + RightHandController.right * 2;

    RightCube.transform.LookAt(RightHandController.position);
}

"
"
I tried to render ARCore stereoscopically through cardboard.  Due to the misalignment between the field of view of the ARCore camera and VR, the object appears to be not being tracked. 
To sort this out, I referred to this blog and implemented it by using a barrel distortion shader. However, it doesn't render stereoscopically. 
Is there any other fix for this problem? 
","
For stereo-vision you need two View Controllers i.e. two ArFragments, each running at 60 fps. Ideally you need a frame rate 120 fps but it's impossible in ARCore at this time.


Barrel distortion is just a special type of a warped distortion of a view.

Also, for robust stereo experience you should shift views only along X-axis and never along Y-axis. In real life an effective distance between two camera lens is 64-200 mm.
For further details look at Technicolor Paper: 15 Stereo Issues.
And there are other visual implementations for stereo:


It's up to you which one is more comfortable for watching.
"
"
In our VR trainer scene created in Unity3d we have a vehicle and we are used real 360 degree panorama taken from one camera to create evironment inside a cabin. 
Everything goes fine except feeling of gigantism of the environment in VR. 
How i can avoid this effect? 
This feelings came up only in headset, when i looked at the monitor surface everything looks fine...
screenshot:

","
IMHO it's because of the missing stereo information in your Skybox. The depicted room looks so narrow that in reality you would have differing views for each eye.
Projecting the image on a sphere instead doesn't solve that problem. A big enough sphere will look exactly like a skybox, a small sphere would provide stereo information, but still your texture doesn't, which will make it look like a flat wallpaper on a narrow sphere :)
"
"
I am building a VR application with a menu scene and many other scenes.. I would like to get back into the menu scene from any other scene when the user hits the mobiles back button (android).. What is the script for that and where should i place the script??
","
You use the Escape keycode to detect back button press on Android.
using UnityEngine.SceneManagement;

void Update()
{
    if (Input.GetKey(KeyCode.Escape))
    {
        SceneManager.LoadScene(""Main Menu"");
    }
}

"
"
I'm building a VR application in unity version 2019.1.2f1 but I can't seem to find the ""build system"" option in build settings, has it been removed?
I've when into build settings and searched ""build system"" but still nothing.
Here is my unity screenshot.

What I'm looking for screenshot.

","
The internal build system is not in the latest version of Unity. The internal build system has been removed since 2019.1.0b1. 

Android: Remove internal build system

Unity now only supports the Gradle build system for Android.
"
"
I want to find user movement walking and tracking position and move camera accordingly in virtual reality app , not head movements like rotation , etc. So I am wondering whether this combination Gear VR with Samsung S7 edge will support my requirement or not.
Any suggestion will be appreciated , thank you.
","
GearVR is not meant to be a 6-DoF device, which is what you are asking (3 Degrees of Freedom means tracking of rotation, 6 Degrees of Freedom means rotation + position).
It is possible to achieve 6-DoF tracking with a Samsung Galaxy by using ARCore. A good starting point would be this GitHub project.
However, the results will most likely not meet your expectations in terms of usability. If you intend to use the original GearVR goggles, you'll notice that the camera is covered, so that would be a problem as well.
If you really want quality mobile 6-DoF VR (read: goggles, not a smartphone in your hands), you should look out for Vive Focus or Oculus Quest as a device.
"
"
I clone repository:
https://github.com/googlevr/gvr-android-sdk/
I try to run in Android Studio, did not change anything, but I getting errors:
Error:(39, 13) Failed to resolve: com.google.vr:sdk-base:1.60.
<a href=""openFile:C:/Users/Vesbat/Documents/KODE/gvr-android-sdk/samples/sdk-controllerclient/build.gradle"">Show in File</a><br><a href=""open.dependency.in.project.structure"">Show in Project Structure dialog

Error:(40, 13) Failed to resolve: com.google.vr:sdk-controller:1.60.1
<a href=""openFile:C:/Users/Vesbat/Documents/KODE/gvr-android-sdk/samples/sdk-controllerclient/build.gradle"">Show in File</a><br><a href=""open.dependency.in.project.structure"">Show in Project Structure dialog

Error:(39, 13) Failed to resolve: com.google.vr:sdk-panowidget:1.60.1
<a href=""openFile:C:/Users/Vesbat/Documents/KODE/gvr-android-sdk/samples/sdk-simplepanowidget/build.gradle"">Show in File</a><br><a href=""open.dependency.in.project.structure"">Show in Project Structure dialog</a>

Error:(39, 13) Failed to resolve: com.google.vr:sdk-videowidget:1.60.1
<a href=""openFile:C:/Users/Vesbat/Documents/KODE/gvr-android-sdk/samples/sdk-simplevideowidget/build.gradle"">Show in File</a><br><a href=""open.dependency.in.project.structure"">Show in Project Structure dialog</a>

Error:(40, 13) Failed to resolve: com.google.vr:sdk-base:1.60.1
<a href=""openFile:C:/Users/Vesbat/Documents/KODE/gvr-android-sdk/samples/sdk-treasurehunt/build.gradle"">Show in File</a><br><a href=""open.dependency.in.project.structure"">Show in Project Structure dialog</a>

Error:(39, 13) Failed to resolve: com.google.vr:sdk-audio:1.60.1
<a href=""openFile:C:/Users/Vesbat/Documents/KODE/gvr-android-sdk/samples/sdk-treasurehunt/build.gradle"">Show in File</a><br><a href=""open.dependency.in.project.structure"">Show in Project Structure dialog</a>

Error:(43, 13) Failed to resolve: com.google.vr:sdk-common:1.60.1
<a href=""openFile:C:/Users/Vesbat/Documents/KODE/gvr-android-sdk/samples/sdk-videoplayer/build.gradle"">Show in File</a><br><a href=""open.dependency.in.project.structure"">Show in Project Structure dialog</a>

Error:(42, 13) Failed to resolve: com.google.vr:sdk-base:1.60.1
<a href=""openFile:C:/Users/Vesbat/Documents/KODE/gvr-android-sdk/samples/sdk-videoplayer/build.gradle"">Show in File</a><br><a href=""open.dependency.in.project.structure"">Show in Project Structure dialog</a>

","
Problem #1:
They use since 25.5.2017 libs V 1.60.1 but on global repo there current no 1.60.1 please see here: https://bintray.com/google/googlevr/sdk-audio#files/com/google/vr/sdk-audio
As a temporary solution you can switch to 1.60.0 back
Problem #2:
When you did this, you will run into an incompatible Gradle plugin version (they use an old one)
classpath 'com.android.tools.build:gradle-experimental:0.9.0-beta4'

change to this
classpath 'com.android.tools.build:gradle-experimental:0.9.2'

"
"
I recently bough my brand new CV1. Even though My GTX 980M is not recommended VGA but it should display something. Now about the problem. It started with Oculus Home setup, seem to everything worked fine, even tracking is working, I listened audio from setup process but display was completely black. And LED is orange btw. However I managed to pass settings using audio only with my Oculus remote. Now i can see the Oculus Home, but when I try to run any app from there it simply stays black. I am also a Unity3D developer and when I tried to launch my own apps same happened. In unity HDM is working, gyro is ok (I can see on my laptop display when I'm turning around) but display is still black, with orange LED light. And here some Oculus server logs I've been captured maybe it can help! Please Help anybody!
24/08 15:36:48.824 {INFO}    [LifeCycle] motionSensorEvent() Start
24/08 15:36:48.824 {INFO}    [LifeCycle] HMD Moving: true
24/08 15:36:48.824 {INFO}    [TrackingManager] Setting tracking mode to on
24/08 15:36:48.824 {INFO}    [Server] Tracking Enabled : true
24/08 15:36:48.824 {DEBUG}   [Server] UpdateFocusedVirtualHmd: ConfigureTracking On (112)
24/08 15:36:48.824 {DEBUG}   [Kernel:Default] [VirtualHMD] Applying client-specific caps
24/08 15:36:48.825 {WARNING} [Server] Setting connection focus to pid: 1336, file: oculus-overlays.exe, but no window handle exists
24/08 15:36:48.825 {INFO}    [LifeCycle] motionSensorEvent() End
24/08 15:36:49.118 {INFO}    [HW:Enumeration] HMDs: 1, Displays: 1
24/08 15:36:49.118 {DEBUG}   [HW:HmdSensor] IMU report rate = 500
24/08 15:36:49.118 {INFO}    [HW:HmdSensor] Leaving idle mode
24/08 15:36:49.118 {INFO}    [HW:Enumeration] Setting frameInterval = 19200, ledExposure = 399
24/08 15:36:49.118 {INFO}    [HW:HmdSensor] Setting LED state: true
24/08 15:36:49.320 {DEBUG}   [Server] Connection open for pid: 12020, file: Unity.exe
24/08 15:36:49.320 {DEBUG}   [Server] Accepted a client from pid: 12020(Unity.exe) running version (prod = 1).1.7.0(build = 262766) feature version = 0. Server is version (prod = 1).1.7.0(build = 262766) feature version = 0
24/08 15:36:49.321 {DEBUG}   [Server] UpdateFocusedVirtualHmd: ConfigureTracking On (112)
24/08 15:36:49.321 {DEBUG}   [Kernel:Default] [VirtualHMD] Applying client-specific caps
24/08 15:36:50.450 {WARNING} [Server] Setting connection focus to pid: 1336, file: oculus-overlays.exe, but no window handle exists
24/08 15:36:50.450 {DEBUG}   [Server] Hmd_Create
24/08 15:36:50.450 {DEBUG}   [Kernel:Default] [AppFocusObserver] AddProcess: Recognizing the newly added process as in-focus pid=12020
24/08 15:36:50.450 {INFO}    [LifeCycle] registerPid() Start
24/08 15:36:50.450 {INFO}    [LifeCycle] Register Pid: 12020 (Sideloaded)
24/08 15:36:50.450 {INFO}    [LifeCycle] Registering tracker: 12020 (Sideloaded)
24/08 15:36:50.450 {INFO}    [LifeCycle] App already added.
24/08 15:36:50.450 {INFO}    [LifeCycle] registerPid() End
24/08 15:36:50.450 {DEBUG}   [Server] Hmd_GetHmdInfo
24/08 15:36:50.454 {DEBUG}   [Kernel:Default] [AppFocusObserver] UpdateTextureSetActive: Setting rift window for pid=12020 textureset=1
24/08 15:36:50.454 {INFO}    [LifeCycle] requestFocus() Start
24/08 15:36:50.454 {INFO}    [LifeCycle] Request Focus: 12020 (Sideloaded)
24/08 15:36:50.454 {INFO}    [LifeCycle] Request Location: APP
24/08 15:36:50.454 {INFO}    [LifeCycle] Push Location: APP
24/08 15:36:50.454 {INFO}    [LifeCycle] Update Location: HSW
24/08 15:36:50.454 {INFO}    [LifeCycle] Inactive Focus: 3452 (oculus-home)
24/08 15:36:50.454 {INFO}    [LifeCycle] Already Focused: 0 (Unknown)
24/08 15:36:50.454 {INFO}    [LifeCycle] requestFocus() End
24/08 15:36:50.455 {INFO}    [Kernel:Default] WARNING: [CameraTimeSynchronizer] out of sync. Exposure Delta = 33830.2 ms, cameraDelta = -32430.4 ms.
24/08 15:36:50.455 {WARNING} [Tracking:Tracker] WMTD302S600Z3S: at 0, skipped 1 frames (37403 -> 37405)
24/08 15:36:50.569 {INFO}    [Kernel:Default] [CameraTimeSynchronizer] successfully synchronized.
24/08 15:36:50.667 {DEBUG}   [Tracking:Filter] HMD: Large change, resetting position filter
24/08 15:36:51.031 {INFO}    [DeviceEvent] SENSOR [WMTD302S600Z3S] TrackingChanged 0 
24/08 15:36:51.031 {INFO}    [HardwareManager] Tracker WMTD302S600Z3S got tracking event with error code 0
24/08 15:36:51.518 {DEBUG}   [Kernel:Default] [AppFocusObserver] UpdateRenderingActive: pid=12020 active
24/08 15:36:51.518 {INFO}    [LifeCycle] enterVR() Start
24/08 15:36:51.518 {INFO}    [LifeCycle] Enter VR: 12020 (Sideloaded)
24/08 15:36:51.518 {INFO}    [AppTracker] VR Ready: 12020 (Sideloaded)
24/08 15:36:51.518 {INFO}    [LifeCycle] Request Focus: 12020 (Sideloaded)
24/08 15:36:51.518 {INFO}    [LifeCycle] Request Location: APP
24/08 15:36:51.518 {INFO}    [LifeCycle] Push Location: APP
24/08 15:36:51.518 {INFO}    [LifeCycle] Update Location: HSW
24/08 15:36:51.519 {INFO}    [LifeCycle] Inactive Focus: 3452 (oculus-home)
24/08 15:36:51.519 {INFO}    [LifeCycle] Already Focused: 0 (Unknown)
24/08 15:36:51.519 {INFO}    [LifeCycle] enterVR() End
24/08 15:36:54.815 {DEBUG}   [Tracking:Pnp] RansacMatch: Too many outliers: 1 outliers out of 5, allowed 0
24/08 15:36:58.592 {DEBUG}   [Kernel:Default] [AppFocusObserver] UpdateTextureSetActive: Active process pid=12020 dismantling its last texture set
24/08 15:36:58.592 {DEBUG}   [Kernel:Default] [AppFocusObserver] NextProcess: Switching active rift process to pid=1336 textureset=1 (not updating active render window)
24/08 15:36:58.592 {INFO}    [LifeCycle] requestFocus() Start
24/08 15:36:58.592 {INFO}    [LifeCycle] Request Focus: 1336 (oculus-overlays)
24/08 15:36:58.592 {INFO}    [LifeCycle] Setting focus to Overlays is not allowed.
24/08 15:36:58.592 {INFO}    [LifeCycle] requestFocus() End
24/08 15:36:58.594 {DEBUG}   [Server] UpdateFocusedVirtualHmd: ConfigureTracking On (112)
24/08 15:36:58.594 {DEBUG}   [Kernel:Default] [VirtualHMD] Applying client-specific caps
24/08 15:36:58.598 {WARNING} [Server] Setting connection focus to pid: 1336, file: oculus-overlays.exe, but no window handle exists
24/08 15:36:58.598 {INFO}    [Server] Connection closed for pid: 12020, file: Unity.exe
24/08 15:36:58.598 {DEBUG}   [Server] UpdateFocusedVirtualHmd: ConfigureTracking On (112)
24/08 15:36:58.598 {DEBUG}   [Kernel:Default] [VirtualHMD] Applying client-specific caps
24/08 15:36:58.616 {WARNING} [Server] Setting connection focus to pid: 1336, file: oculus-overlays.exe, but no window handle exists
24/08 15:36:58.616 {INFO}    [Server] Connection destroyed for pid: 12020, file: Unity.exe
24/08 15:37:01.456 {INFO}    [DeviceEvent] SENSOR [WMTD302S600Z3S] TrackingChanged 0 
24/08 15:37:01.456 {INFO}    [HardwareManager] Tracker WMTD302S600Z3S got tracking event with error code 0

","
After few days I've figured out the problem. Despite ASUS ROG G751 doesn't have a Optimus technology enabled, its HDMI port is still connected something intel GPU related. Thats why rift is not getting enough performance and stays black. I tried bunch of adapters from thunderbolt (Mini Display Port) and VGA. Nothing worked out. Then I read server logs from Oculus and there was one saying Not enough bandwidth. I understood that problem is something related to adapter performance so I used 4k capable Mini Displayport to HDMI adapter (link below) and voliah!! Rift is working! No lags or any other problems! Just try 4K enabled adapter!
Here is a working adapter - https://www.amazon.com/Macally-Aluminum-DisplayPort-Thunderbolt-MDHDMI4K/dp/B013H07SX6/ref=sr_1_fkmr0_1?ie=UTF8&qid=1472844784&sr=8-1-fkmr0&keywords=macally+mdp+hdmi
"
"
I'm currently doing my first VR project with Leap Motion, HTC Vive and Unity.
When I create a new project and add the LeapRig, everything is just fine, but after adding the Interaction Manager as a child of LeapRig, I get the following error message, which gets repeated multiple times a second:
INPUT AXIS NOT SET UP.  Go to your Input Manager and add a definition for  on the 9th Joystick Axis.
UnityEngine.Debug:LogError(Object)
Leap.Unity.Interaction.InteractionXRController:fixedUpdateGraspButtonState(Boolean) (at Assets/LeapMotion/Modules/InteractionEngine/Scripts/InteractionXRController.cs:733)
Leap.Unity.Interaction.InteractionXRController:fixedUpdateGraspingState() (at Assets/LeapMotion/Modules/InteractionEngine/Scripts/InteractionXRController.cs:706)
Leap.Unity.Interaction.InteractionController:fixedUpdateGrasping() (at Assets/LeapMotion/Modules/InteractionEngine/Scripts/InteractionController.cs:1783)
Leap.Unity.Interaction.InteractionController:Leap.Unity.Interaction.IInternalInteractionController.FixedUpdateController() (at Assets/LeapMotion/Modules/InteractionEngine/Scripts/InteractionController.cs:259)
Leap.Unity.Interaction.InteractionManager:fixedUpdateInteractionControllers() (at Assets/LeapMotion/Modules/InteractionEngine/Scripts/InteractionManager.cs:372)
Leap.Unity.Interaction.InteractionManager:FixedUpdate() (at Assets/LeapMotion/Modules/InteractionEngine/Scripts/InteractionManager.cs:299)

Has anyone an idea why this happens and how to fix it?
I already worked with the interaction manager, but suddenly this error message occured.
I can run my program just fine too, but the error bothers me anyway and makes it difficult to use the console properly.
Greetings from Germany
Marc
","
From the Interaction Engine docs:

If you intend to use the Interaction Engine with Oculus Touch or Vive
  controllers, you'll need to configure your project's input settings
  before you'll be able to use the controllers to grasp objects. Input
  settings are project settings that cannot be changed by imported
  packages, which is why we can't configure these input settings for
  you. You can skip this section if you are only interested in using
  Leap hands with the Interaction Engine.
Go to your Input Manager (Edit -> Project Settings -> Input) and set
  up the joystick axes you'd like to use for left-hand and right-hand
  grasps. (Controller triggers are still referred to as 'joysticks' in
  Unity's parlance.) Then make sure each InteractionXRController has its
  grasping axis set to the corresponding axis you set up. The default
  prefabs for left and right InteractionXRControllers will look for axes
  named LeftXRTriggerAxis and RightXRTriggerAxis, respectively.
Helpful diagrams and axis labels can be found in Unity's
  documentation.

"
"
I have a  unity project. It is developed for VR headset training usage. However, users have a strong dizzy feeling after playing the game. Now, I want to use 3 monitors to replace the VR headset so the users can look at the 3 monitors to drive. Is it a big effort to change the software code to achieve this? What can I do for the software so that it can be run in monitor?
","
Actually it is quite simple:
See Unity Manual Multi-Display

In your Scene have 3 Camera objects and set their according Camera.targetDisplay via the Inspector (1-indexed). 
To make them follow the vehicle correctly simply make them childs of the vehicle object then they are always rotated and moved along with it. Now position and rotate them according to your needs relative to the vehicle.
In PlayerSettings → XRSettings (at the bottom) disable the Virtual Reality Supported since you do not want any VR-HMD move the Camera but it's only controlled by the vehicle transform.
Then you also have to activate according Displays (0-indexes where 0 is the default monitor which is always enabled) in your case e.g.
private void Start()
{
    Display.displays[1].Activate();
    Display.displays[2].Activate();
}

I don't know how exactly the ""second"" or ""third"" connected Monitor is defined but I guess it should match with the monitor numbering in the system display settings.

"
"
I've been having a problem with the VR implementation in Unity since steam updated to 2.0 in 
Now I'm working with their latest update from Github (2.2b4) and when I add the prefab ""player"" I am getting a constant error that says:
NullReferenceException: Object reference not set to an instance of an object

And it points me to the file SteamVR_Action_Boolean.cs, I understand the error, it is expecting an assignment of an object somewhere, I just cant work out where. Has anyone had this same problem?
","
It was an error from Steam, I had it reported and now is all fixed in their new formal release.
"
"
I want to make React360 split scene work on an ios device, I have tried to use webvr-polyfill , but I dont know how to use it
sorry for my language, I am not a  native speaker
","
How exactly did you try to use the webvr-polyfill? You will need this since iOS does not natively support it.
Simply include the webvr-polyfill in your index.html. Add the webvr-polyfill bevor you run any kind of react-360 code. Example:
<script src='PATH_TO_YOU_POLYFILL_FILE/webvr-polyfill.min.js'></script>
<script>var polyfill = new WebVRPolyfill();</script>

<script src=""./client.bundle?platform=vr""></script>
<script>
    // Initialize the React 360 application
    WebVR.init(
        'index.bundle?platform=vr&dev=true',
        document.getElementById('container'),
        {
            assetRoot: 'static_assets/',
        }
    );
</script>

"
"
Since we upgraded from aframe 0.8 to 0.9 we have a weird rendering behaviour when we enter vr mode on mobile device (i.e Samsung Galaxy S8)

You can see this white space in the middle on both eyes which is simply ""not rendered"" I guess. The hall is a .obj model with a .mtl provided (yes, I know glTF would be better but unfortunately it is not in our hands).
We tested on all mobile browsers with the same result. On Desktop Version everything works fine.  Can someone verify / explain this? 
","
It's quite embarrassing, but we found the solution. 
Our external model designer designed every floor tile as a single object... Now we use a floor where all the tiles are one single object and that seems to work well.
"
"
I'm using the VRExpansionPlugin in UnrealEngine 4. 
Steps I've taken

Created a new ""Default"" level within the template project provided by the VRExpansionPlugin. 
Added a NavMeshBoundsVolume to the level. 
Deleted the existing ""Player Start"" and replaced it with the ""VRPlayer Start"".

I would expect that when I ""Play"" the level I should be able to point at the ground and teleport around, but instead I just see the short blue teleport line that almost immediately cuts off.
I know that I am missing a step (probably only one) but I'm not sure what step I am missing. Also, I can't find any documentation to help figure out what I am missing. So, with that said, what am I missing to enable teleport movement in my UE4 VRExpansionPlugin VR project?
","
Well, it turns out to be both something weird and very simple. I was comparing it to a ""Default"" level I had created in the past that worked and I noticed that on my previous working example my navmesh was halfway into the floor instead of sitting on top. The navmesh on my current level was position by raising it above the floor and using the ""end"" key to tell it sit exactly on top of the floor, which for some reason caused the teleportation to not work. I moved the navmesh down into the floor and everything started working perfectly.
tl;dr; The answer is to make sure the navmesh is not sitting exactly on top of the floor but instead below or inside it.
"
"







Closed. This question needs debugging details. It is not currently accepting answers.
                                
                            











 Edit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question.


Closed 5 years ago.







                        Improve this question
                    



http://trimensions.org/node/83
I am coding by importing the code from the link above.
But this code starts on a small screen.
As soon as I start, I want to start with the Cardboard mode (a mode with both left-eye and right-eye monitors).
However, no matter how I read the code, the code to switch to Cardboard mode is not visible in this code.
I think it is all contained in VrVideoView.
If you know something, let me know. Thanks.
","
Just add this line after declaring VrVideoView variable. I've declared it 'videoWidgetView'.
videoWidgetView.setDisplayMode(3);


1: Mono Display (Normal Screen)
2: Full Screen 
3: Stereo Screen (CardBoard Mode)

"
"
So I have a laser pointer attached to my controller, shooting rays and triggering events whenever I press the tigger.
Tho it still has one or two buggs (Unity VR Controller UI Interaction), I have a more general question.
The UI needs some kind of slider. And I can't figure out how to make my pointer collide with the slider and behave in a slider-ish way. When I press my button, it currently ends up setting the slider to zero and not letting me drag it. 
Thankful for any tipps on how to make them actually slide, since my ridgidbody-cube doesnt move and doesn't allow movement! Unitys documentation on e.g. the beginDragHandler isn't really helpful...
Cheers, Flo
","
If your enter/exit events already works, juste check the x (or the axis corresponding on your slider) of your ""laser"" every frame or so, and increase/decrease the value of the slider based on the delta between two check.
Hope it is clear, it should not be hard if you already have your events sets up !
"
"
So I have a laser pointer attached to my controller, shooting rays and triggering events whenever I press the tigger.
Tho it still has one or two buggs (Unity VR Controller UI Interaction), I have a more general question.
The UI needs some kind of slider. And I can't figure out how to make my pointer collide with the slider and behave in a slider-ish way. When I press my button, it currently ends up setting the slider to zero and not letting me drag it. 
Thankful for any tipps on how to make them actually slide, since my ridgidbody-cube doesnt move and doesn't allow movement! Unitys documentation on e.g. the beginDragHandler isn't really helpful...
Cheers, Flo
","
If your enter/exit events already works, juste check the x (or the axis corresponding on your slider) of your ""laser"" every frame or so, and increase/decrease the value of the slider based on the delta between two check.
Hope it is clear, it should not be hard if you already have your events sets up !
"
"
This is my index.html file
<html>
<head>
  <title>VR Sample</title>
  <script src=""//storage.googleapis.com/vrview/2.0/build/vrview.min.js""></script>

</head>

<body>

  <div id=""vrview"">
    <iframe width=""100%""
    height=""300px""
    allowfullscreen
    frameborder=""0""
    src=""http://storage.googleapis.com/vrview/index.html?image=ffff.jpg&is_stereo=true"">
</iframe>
  </div>



</body>
</html>

And this is the structure of the website folder

I tried hosting it in Webserver for chrome as per the instructions in the google codelabs. But I clicked the 127.0.0.1.8887 url, I got a blank page with no files or folders. Then I tried hosting it on XAMPP and It did work. However, I did not get the panaroma image. Instead I got this error

I took the 360 image with google camera app and converted it to stereo with the google's online converter but got the same error. I also tried downloading the VRView repo from github and modified the code as
src=""vrview/index.html?image=ffff.jpg&is_stereo=true""

that too didn't work.
","
You are using the iframe version of vrview meaning when you request ""ffff.jpg"", you are actually requesting:
http://storage.googleapis.com/ffff.jpg
Try using the javascript version
Try this:
<html>
  <head>
    <script src=""http://storage.googleapis.com/vrview/2.0/build/vrview.min.js""></script>
    <script>
      window.addEventListener('load', onVrViewLoad);

      function onVrViewLoad() {
        var vrView = new VRView.Player('#vrview', {
          image: 'http://storage.googleapis.com/vrview/examples/coral.jpg',
          is_stereo: true,
          width: '100%',
          height: 300
        });
      }
    </script>
  </head>
  <body>
    <div id=""vrview""></div>
  </body>
</html>

Note: chrome cannot access files off a harddrive.
EDIT: This is due to CORS.

Cross-origin resource sharing (CORS) is a mechanism that allows restricted resources on a web page to be requested from another domain outside the domain from which the first resource was served.

Thanks to @Eleanor Zimmermann for noting this.
"
"
I am developing panorama application and I cannot find some ways to implement my Android layouts or Android widgets(TextView, ImageView etc.) to my scene on ViroView. 
Is it possible to do in this 3rd party library(ViroCore)?
Thanks, a lot!
","
first you need to inflate the view , then create an AndoridViewTexture ,and finally use the function attachview and give it the inflated view as a parm.
AndroidViewTexture androidTexture = new AndroidViewTexture(viroView.provideView(), pxWidth, pxHeight, isAccelerated);

androidTexture.attachView(inflated);

"
"
Using the VR framework A-Frame, is there a way to have a 3D Skybox?
Either by having a 3D image, two images (one for each eye) or having two separate skyboxes (also one for each eye). Is there a built in technique for doing this or does anyone know of a way to do it?
","
For stereoscopic 3D, you can use stereo component which lets you set one for each eye. It may need to be updated though, not sure.
"
"
I have website where user can upload 360 image and video . I play that image and video using vrview but vrview doesnot seems that good as it lack enough control button in video player (as i know).
I want to use krpano , but i dont know hoe to use for dynamically added content .
I search it but couldn't get proper knowledge on that .
","
I am posting so anyone who will come here searching for solution could get some help :
Actually the solution comes from how video is played by krpano .
In krpano for playing video we need to give xml file that contain necessary information related to video we are going to play .
So for every video , we want to play ,  we should have xml file of that video mean xml file containing the video source , thumbnail source specified .
we can create xml file for every video file while uploading .
I made xml file using string concatenate and wrote that in xml .
xml file is in sample code inside example folder .
"
"
I'm trying to use the raycaster to draw on a plane.
I know how to be notified when the raycaster and the plane intersect but after that i don't know how to get updated coordinates of the intersection while the raycaster is moving acrosse the plane. 
","
AFRAME.registerComponent('raycaster-listen', {
  init: function () {
    this.raycaster = document.querySelector('#myRay');
  },

  tick: function () {
    var intersection = this.raycaster.components.raycaster.getIntersection(this.el);
    if (!intersection) { return; }
    console.log(intersection);
  }
});

<a-entity geometry material raycaster-listen></a-entity>

"
"
I have a problem about my gaze on VR . What I am trying to do is that gaze upon the button I want to select then hide the first gameobject parent then show the second gameobject parent . Now the second gameobject parent will be shown and when I try to gaze upon the back button it will show the first gameobject parent and hide the second gameobject parent . The problem occurs here, when I am trying to nothing and don't gaze on the buttons it automatically show my second gameobject parent and go back to first parent gameobject and hide and show and hide and show always.
public float gazeTime = 2f;
private float timer;
private bool gazedAt;

public Setting setting;

private void Start()
{

}

public void Update()
{
    if (gazedAt)
    {
        timer += Time.deltaTime;

        if (timer >= gazeTime)
        {
            // execute pointerdown handler
            ExecuteEvents.Execute(gameObject, new PointerEventData(EventSystem.current), ExecuteEvents.pointerDownHandler);
            timer = 0f;
        }
        else
        {
            return;
        }
    }
    else
    {
        return;
    }

}

public void PointerEnter()
{
    gazedAt = true;
    Debug.Log(""PointerEnter"");
}

public void PointerExit()
{
    gazedAt = false;
    Debug.Log(""PointerExit"");
}

//Method for going to setting
public void Setting()
{
    setting.ActivateSetting();
}
//Method for going back to main menu
public void GoBack()
{
    setting.GoBackToMainMenu();
}

Here's how my Setting code is setup
public GameObject setting;
public GameObject back; 

public void ActivateSetting()
{
    setting.SetActive(false);
    back.SetActive(true);
}

public void GoBackToMainMenu()
{
    back.SetActive(false);
    setting.SetActive(true);
}  

What I want is that it will only show the gameobject parent if I gaze upon it.
","
After calling the click once you reset the timer but you didn't reset gazedAt
=> the Update method did still run the timer and call the click again.
It seems that your PointerExit is not called at all and therefore the button never reset.

Instead of the EventTrigger I would strongly recommend to use the interfaces IPointerEnterHandler and IPointerExitHandler like
public class YourClass : MonoBehaviour, IPointerEnterHandler, IPointerExitHandler
{
    //...

    public void OnPointerEnter()
    {

    }

    public void OnPointerExit()
    {

    }


I would actually not use Update at all but prefer a Coroutine. Also don't use the complex call of ExecuteEvents.Execute(gameObject, new PointerEventData(EventSystem.current), ExecuteEvents.pointerDownHandler); instead use either Getcomponent<Button>().onClick.Invoke(); or call the method directly
private Button _button;

private void Awake()
{
    // Get the reference only once to avoid 
    // having to get it over and over again
    _button = GetComponent<Button>();
}

private IEnumerator Gaze()
{
    // wait for given time
    yield return new WaitForSeconds(gazeTime);

    // call the buttons onClick event
    _button.onClick.Invoke();

    // or as said alternatively directly use the methods e.g.
    setting.ActivateSetting();
}

public void OnPointerEnter()
{
    Debug.Log(""PointerEnter"");

    // start the coroutine
    StartCoroutine(Gaze());
}

public void OnPointerExit()
{
    Debug.Log(""PointerExit"");

    // stop/interrupt the coroutine
    StopCoroutine(Gaze());
}

As you can see there is no need at all for the timer and gazedAt values so you can't forget to reset them somewhere. It also avoids that the method is called repeatedly.

If you don't want to use a Button at all you could also add your own UnityEvent like
// add callbacks e.g. in the Inspector or via script
public UnityEvent onGazedClick;

// ...

onGazedClick.Invoke();

"
"
i am creating a vr project i have a simple cube in a scene with a box collider on it with trigger checked on it now what i want is that when my vr hand enters the trigger of that cube the grab animation of my vr hand should play but when i enter my hand into the trigger of cube nothing happens then for testing purpose i debug.log an message that wheather its detecting or not that is fine it is detecting when my hand is entered and when it leaves but animation is not playing here is my code
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class VRHandsController : MonoBehaviour
{
public VRTK.VRTK_ControllerEvents vrcontroller;
public VRTK.VRTK_InteractGrab vrtkgrab;
Animator anim;
// Start is called before the first frame update
void Start()
{
    anim = GetComponent<Animator>();
    //   
        anim.SetBool(""indicate"", false);
    anim.SetBool(""grab"", false);
    anim.SetBool(""idle"", true);
    //anim.SetBool(""isGrabbing"", false);
}

// Update is called once per frame
void Update()
{
    if (vrcontroller.touchpadPressed)
    {

        anim.SetBool(""indicate"", true);
        anim.SetBool(""grab"", false);
        anim.SetBool(""idle"", false);
        //anim.SetBool(""isGrabbing"", true);
    }
    else if (vrtkgrab.IsGrabButtonPressed())
    {

        anim.SetBool(""indicate"", false);
        anim.SetBool(""grab"", true);
        anim.SetBool(""idle"", false);
    }

    else if (vrcontroller.triggerClicked)
    {
        anim.SetBool(""indicate"", true);
        anim.SetBool(""grab"", false);
        anim.SetBool(""idle"", false);
    }
    else
    {

        anim.SetBool(""indicate"", false);
        anim.SetBool(""grab"", false);
        anim.SetBool(""idle"", true);
        //a///nim.SetBool(""isGrabbing"", false);
    }
}

private void OnTriggerEnter(Collider other)
{
    if(other.gameObject.tag == ""object"")
    {
        anim.SetBool(""indicate"", false);
        anim.SetBool(""grab"", true);
        anim.SetBool(""idle"", false);
        Debug.Log(""colliding>>>>>"");
    }
}
private void OnTriggerStay(Collider other)
{
    if (other.gameObject.tag == ""object"")
    {
        anim.SetBool(""indicate"", false);
        anim.SetBool(""grab"", true);
        anim.SetBool(""idle"", false);
        Debug.Log(""colliding>>>>>"");
    }
}

private void OnTriggerExit(Collider other)
{
    if (other.gameObject.tag == ""object"")
    {
        anim.SetBool(""indicate"", false);
        anim.SetBool(""grab"", false);
        anim.SetBool(""idle"", true);
        Debug.Log(""leaving trigger"");
    }
}

}
","
You are setting grab to false, in the else part:
else if (vrcontroller.triggerClicked)
...
} else {
anim.SetBool(""grab"", false);
...

in every Update() call!
So whenever you don't press the trigger, the grab is set to false. Entering the trigger will set it to true, only to be resetted in the same frame. (Collisions run before Update)

You should not set it to false like that. You should set it to false if vrcontroller.triggerReleased (if there is such a thing)
If there is no ""triggerReleased"" you need a helper-boolean.

"
"
I am working very hard to get audio to play on my mobile phone with react 360. Through reading various documentation, I've learned that in order to play audio on mobile, I need to enact an html entity to create a user interaction. Once a user interacts by clicking the button, audio should be able to play on mobile. This does not seem to be the case.
In my index.html file I have the following code:
<html>
  <head>
    <title>ExampleVR</title>
    <style>body { margin: 0; }</style>
    <meta name=""viewport"" content=""width=device-width, initial-scale=1, user-scalable=no"">
  </head>
  <body>
    <!-- Attachment point for your app -->

    <div id=""container"">


      <button id=""enter"" onclick=""enterApp();"">
        Click to Enter VR
      </button>


    </div>
    <script src=""./client.bundle?platform=vr""></script>
    <script>
      // Initialize the React 360 application


      function enterApp() {
        React360.init(
          'index.bundle?platform=vr&dev=true',
          document.getElementById('container'),
          {
            assetRoot: 'static_assets/',
          }
        );  
      }


    </script>
  </body>
</html>

As you can see, I have created am html button that when clicked, loads my React 360 code . However, when I click a VrButton in mobile, it still does not play. I've followed the recommendations as documented and it works on all browsers on desktop with the exception of mobile. Does anyone know how to fix this problem?
","
This was the solution to fix the audio issue:
https://github.com/facebook/react-360/issues/652
"
"
The goal is to teleport with only one controller.
Should it be done via Action Sets or could it be done via Event Trigger?
I could not find anything specific related to the controllers in the Hand script also.
Using the Action Sets should I use the per hand option and customize which settings? Then, how could I create I field in the Inspector? 
Thank you very much! 
","
Try getting a reference to your controllers SteamVR_Behaviour_Pose component and just check if it is the controller you want to do the teleporting with. SteamVR_Input_Sourcesis a enum that is used to identify your vive hardware.
public SteamVR_Behaviour_Pose pose;

if(pose.inputSource == SteamVR_Input_Sources.LeftHand)
{
    //do stuff
}

Be careful tho. I don't quite remember but I think this can cause a NullReferenceExceptionif the hardware you are trying to access is not connected.
You could also, as you already thought about, use the ActionSets and use different actions for both hands. That should also work, I just never tried it.
As a bonus: 
public SteamVR_Action_Boolean action = SteamVR_Input.GetBooleanAction(""YourActionName"");

if (action.GetStateUp(pose.inputSource))
{
    //do even more stuff
}

Thats how you check for actions in your script. Took me a long time of reading docs to find out myself. 
Hope this helps!
"
"
I am trying to show & play a video using Gaze pointer in Unity3D. So when i gaze over an object for a particular time, the video must play and when i look away, the video must stop. It  should restart when i look back again at that object.I am using google vr sdk, swipe menu & a custom gaze pointer for this.When i gaze at one of the slides, the video should play. I am relatively new to Unity3D and c#. So please help me with this.
I would also like to know how this can be made interactive using the controls in a google cardboard based vr?
","
Never thought this would be so simple. Solved this using Google VR SDK.
Used Google's reticle pointer to gaze through the menu items. Use the 'Event trigger' to add events such as video player.play() & videoPlayer.stop(). In order to show  & hide the video player, user videoPlayer.setActive();
Feel free to comment if anybody ever needs more details on this.
"
"
I'm building VR scene using A-frame. I have a few circles in the scene and pointing a-cursor on them triggers ""mouseenter"" which teleports the camera to a new position. It works fine, when I open the website, but when I enter the VR MODE (double screen mode) by clicking this cardboard icon in the down right corner it doesn't work on mobile phone. 
Here is the link to the website, so you can see it doesn't work:
https://kotecki-museum-of-art.neocities.org/mainMuseumOfArt.html
I use the recent 0.8.2 version of A-Frame (0.8.0 version has camera shakes a lot). The fuse attribute of a-cursor is set to ""true"" (if it is set to ""false"" mouseenter doesn't work on mobile devices at all, however it still works on my PC). 
How can this issue be fixed?
I pasted my code below. I got rid off some parts of the code, which are irrelevant to the question (some additional geometrical figures, so the code is easier to read):
 <html>
 <head>
<script src=""https://aframe.io/releases/0.8.2/aframe.min.js""></script>

 <script src=""apple-mobile-web-app-capable""></script>
 </head>
 <body>
 <a-scene debug>



  <a-camera id=""MainCamera"" position=""0 4 0"" shadowMapHeight=""512"" 
  shadowMapWidth=""512"">
     <a-cursor color=""orange"" fuse=""true"" fuseduration = ""1500""  />
  </a-camera>



   <a-light type=""point"" position=""50 50 50"" intensity=""1"" decay=""3"" 
  castShadow =""true""></a-light>


  <!--FLOORS-->

  <a-plane id=""logo"" scale=""0.5 0.5"" position=""-4 3 -8"" rotation=""0 0 0""  
width=""5"" height=""5"" src=""images/logo.png""></a-plane>

   <a-plane id=""floor1"" scale=""40 30"" position=""0 0 0"" rotation=""-90 0 0""  metalness=""0.5"" color=""#7ad5e8""></a-plane>

   <a-plane id=""floor2"" scale=""20 40"" position=""0 0 -35"" rotation=""-90 0 0""  
metalness=""0.5"" color=""#7ad5e8""></a-plane>

    <a-plane id=""wallBack"" scale=""20 10"" position=""0 0 -55"" rotation=""0 0 0""  metalness=""0.5"" color=""white""></a-plane>

    <a-plane id=""wallLeft1"" scale=""40 10"" position=""-10 0 -35"" rotation=""0 90 0""  metalness=""0.5"" color=""white"" side=""double""></a-plane>

  <a-plane id=""wallRight1"" scale=""40 10"" position=""10 0 -35"" rotation=""0 90 0""  metalness=""0.5""  color=""white"" side=""double""></a-plane>

  <a-plane id=""wallBack"" scale=""20 10"" position=""0 0 -55"" rotation=""0 0 0""  metalness=""0.5"" color=""white""></a-plane>

  <a-plane id=""wallMiddleLeft"" scale=""10 10"" position=""-15 0 -15"" rotation=""0 0 0""  metalness=""0.5"" color=""white"" side=""double""></a-plane>

  <a-plane id=""wallMiddleRight"" scale=""10 10"" position=""15 0 -15"" rotation=""0 0 0""  metalness=""0.5"" color=""white"" side=""double""></a-plane>

  <a-plane id=""wallLeft2"" scale=""30 10"" position=""-20 0 0"" rotation=""0 90 0""  metalness=""0.5"" color=""white"" side=""double""></a-plane>

  <a-plane id=""wallLeft2"" scale=""30 10"" position=""20 0 0"" rotation=""0 90 0""  metalness=""0.5"" color=""white"" side=""double""></a-plane>

  <a-plane id=""wallFront"" scale=""40 10"" position=""0 0 15"" rotation=""0 0 0""  metalness=""0.5"" color=""white"" side=""double""></a-plane>


   <!--<a-box position=""-1 0.5 -3"" rotation=""0 45 0"" color=""#4CC3D9"">-->
    <!--<a-animation attribute=""rotation"" dur=""5000"" begin=""mouseenter"" repeat=""indefinite"" to=""0 360 0""></a-animation>-->
    <!--<a-animation attribute=""material.color"" dur=""5000"" begin=""click"" repeat=""indefinite""  from=""#4CC3D9"" to=""#00FF00""></a-animation>-->
    <!--<a-animation attribute=""scale"" begin=""3000"" to=""3 3 3""></a-animation>-->
   <!--</a-box>-->


  <!--MOVEMENT CIRCLES-->

  <a-circle id=""c1"" position=""-10 0.3 3"" radius=""1.25"" rotation=""-90 0 0"" color=""#EF2D5E""></a-circle>

  </a-scene>

 <script src=""scriptMuseum.js""></script>

  </body>
 </html>

Javascript:
 var c1 = document.querySelector(""#c1"");

 var cam1 = document.querySelector(""#MainCamera"");

 c1.addEventListener(""mouseenter"", function () {
 cam1.setAttribute(""position"",""-10 4 3"")  
  });

","
Use and move a camera rig:
<a-entity id=“rig” position=“0 4 0”>
  <a-camera 
    shadowMapHeight=""512"" 
    shadowMapWidth=""512"">
       <a-cursor color=""orange"" fuse=""true"" fuseduration = ""1500""  />
  </a-camera>
</a-entity>

Javascript:
var c1 = document.querySelector(""#c1"");

var rig = document.querySelector(""#rig"");

c1.addEventListener(""mouseenter"", function () {
rig.setAttribute(""position"",""10 4 3"")});

"
"
Is there a tech community agreed term for a photographic (well as close as possible) scene that can be explored by walking around? Obviously, within certain limits. Say, a museum could scan a sculpture with laser and make it available on vr, 3d mesh with properly mapped textures. Is there a name for such thing? The so-called 360 VR photos definitely fall short of such detail.
","
I think the most common names are:

360 if it's just an image from one point containing all the angles, usually a equirectangular or cubemap texture/video. Some have stereoscopy, but it's very limited.
360 with depth it's a 360 but apart from color, it has depth information. This allows stereoscopy and some movement, but because of shadowing and problems with acquiring depth maps its almost never used. In the future AI-based filling of shadowed areas, and perhaps replacing the need for capturing depth, might make this a commonly used format.
photogrammetry if it's converted to a textured mesh, has proper depth and can be viewed from all angles (for example Vanishing of Ethan Carter - unfortunatelly 3d models from that article seem to be missing, sent them an email, maybe they'll fix it)
lightfield if it's a volume containing lots of 360 images with some kind of interpolation between them. Has proper depth but can be viewed only from the mapped volume (see Welcome To Lightfields)

"
"
I've been trying to make a 360 photo viewer similar to the Oculus 360 Photos app. The only problem is when projecting onto a sphere with inverted normals, the image ""warps"" or ""bends"" as the sphere does, and results in straight lines such as door frames turning into bending images; bad result.
Changing the size of the sphere does nothing, and obviously the picture has to bend somewhere to fit onto the inner surface of the sphere, so I don't think this solution will work.
I then tried turning the photo into a cylindrical skybox, and using it as a skybox component of the camera, which works great: no bending lines, everything looks as desired. Except for one thing: there is a shimmering/aliasing effect on the texture, unless I enable mip maps, which then results in a blurred image.
Does anybody know how I could apply my image to appear similar to those in the Oculus 360 Photo app? They render with perfect quality and no bending lines, no shimmering. How do they achieve this result?
I've tried different compression types and different shapes, the only thing I haven't tried is slicing the photo into 6 pieces and rendering it on the inside of a cube around the camera, which, due to it's proximity, might not get the shimmery result that could be cause by distance from the camera?
Thoughts, suggestions, questions? Any assistance or discussion is appreciated
","
I was able to get good results by increasing the renderscale to 1.5 or higher, which eliminated the shimmery aliasing effect. Not 100% sure if this was an issue due to the Samsung s6 resolution, but I just work now with an enhanced render scale for higher quality regardless, and optimise elsewhere to save on framerate
"
"
I am creating a a bare bones react 360 app in order to get familiar with input handling from a variety of sources. Here is the web page for the documentation Input Handling React 360. After reading the page, I can't get the input to work at all and its unclear as to why. Here is my code:
import React from 'react';
import {
  AppRegistry,
  StyleSheet,
  Text,
  View,
} from 'react-360';

export default class ExampleVR extends React.Component {
  render() {
    return (
  <View style={styles.panel}>
    <View style={styles.greetingBox}>

    ***********Why is this not doing anything?**************

      <View onInput={e => {
          const inputEvent = e.nativeEvent.inputEvent;
          console.log(inputEvent);
        }}>
          { /* ... */ }
      </View>

      ****************************
      <Text style={styles.greeting}>
        Welcome to React 360
      </Text>
    </View>
  </View>
);
}
};

The above code does nothing and honestly I'm not sure what it would do if it did work. From the documentation, I would think that it would console.log an input object for me to see. That's what I gathered from a previous react vr question I saw. What I don't get is how is this triggered? I'm clicking on it, pressing the key pad on my laptop and nothing. There's not too much information to go on. Has anyone encountered something like this before?
","
I think I figured out my own question for future reference. In the above code, <View onInput={....}> is 'invisible' when I run the application with npm start. What I did was remove the surrounding views included with the example and then added some stylesheet css stylings to it. Here is that code:
export default class ExampleVR extends React.Component {


render() {
    return (
        <View style={styles.input} onInput={e => {
            const inputEvent = e.nativeEvent.inputEvent;
            console.log(inputEvent);
          }}>
            { /* ... */ }
        </View>
    );
  }
};

const styles = StyleSheet.create({
  input: {
    width: 1000,
    height: 600,
    backgroundColor: 'rgba(255, 255, 255, 0.4)',
    justifyContent: 'center',
    alignItems: 'center',
  },
}

This works as I would expect. When I click on the present pad or a key press, the input object is printed as expected. 
"
"
I am trying to create a First person view in A-frame using the .obj model of a car. 
The camera has been positioned close to the driver's seat, and there is a timed interval in javascript that moves the camera position by doing trignometric calculations and moves the camera accordingly. 
Here are two problems I am facing
1) How do you make this smoother? Is there a hook for the render loop in A-frame which can be accessed in javascript? I cant seem to find the documentation for it.
2) When I view the above page in VR mode on the phone ( Android 9), the animation created stops abruptly, which works well in fullscreen mode.  
The code for the html is here https://raw.githubusercontent.com/CuriosityGym/VRWithAframe/master/templates/class3-models.html
The example can be accessed here  http://cgaframe.herokuapp.com/class3
Thank You!
","
You have a setInterval that fires every 100ms, animating the car at 10fps. That’s why it’s not smooth. Create a component and  use the tick method to animate
In addition, you should not animate the camera directy because its position will be overriden by the headset pose in VR mode. Setup and animate a camera rig
"
"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 5 years ago.







                        Improve this question
                    



Hello I'm developing a Virtual Reality App in Unity and I'm using Steam Virtual Reality, VRTK and HTC VIVE.  Like in PC we Use ""Void OnMouseDown"" to detect clicks on a game object, how we can achieve this in Virtual Reality? Suppose We have Virtual Reality scene with a single Cube. Now what I want is to detect clicks on the objects so how can I achieve this in Virtual Reality? Help Me Please Thanks.
public void OnMouseDown()
{
   //some code    
}

","
You should refer to this tutorial on VRTK. You can also refer the following example in your VRTK package.

VRTK/Examples/005_Controller/BasicObjectGrabbing

You have two parts for this:

You will need to add VRTK_InteractTouch & VRTK_InteractGrab scripts
on your Left & Right Controllers. And setup grab button to your
desired button. By default its set to Grip Press.
After you have setup controller its time to setup the object which you want to make interactable. Select your Cube. Go to Windows. Point to VRTK option. Click Setup Interactable Object. A popup will show up called Setup Object. You can go through the options and make desired changes else you can directly click on setup selected object button. It will add the desired scripts on your object. You can also add manually each of these scripts which are shown in the tutorial if you need to else use the options from windows directly to add intractability.

You also might want to go through this. If you need a grabbed object there is a specific function mentioned in document called GetGrabbedObect.

As a first time developer it will be bit confusing to use VRTK but trust me its one of the most powerful tool to make VR games. Secondly keep on checking the tutorial videos about using VRTK and go through the examples folder. 
"
"
I am new and currently trying to get my Vive Controllers to Pause / Play Unity. So far i Can see my ""hands"" and it does Recognise my triggers, which is all it needs to. 
Does Anyone know how to make it Pause when I press the Trigger and then Start when I press it again ?
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using Valve.VR;

public class Viveinput : MonoBehaviour
{
[SteamVR_DefaultAction(""Squeeze"")]
public SteamVR_Action_Single squeezeAction;
public bool paused;

void Update () {
    if (SteamVR_Input._default.inActions.GrabPinch.GetLastStateUp(SteamVR_Input_Sources.Any))
    {
        print("" Grab Pinch Up"");
    }
    float triggerValue = squeezeAction.GetAxis(SteamVR_Input_Sources.Any);

    if (triggerValue > 00f)
    {
        print(triggerValue);
    }

}
}

This is What I am using atm for connection between controller and Unity.
","
I assume that your video is playing on a VideoPlayer MonoBehaviour :
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using Valve.VR;

public class Viveinput : MonoBehaviour
{
public VideoPlayer video;

[SteamVR_DefaultAction(""Squeeze"")]
public SteamVR_Action_Single squeezeAction;
private bool _triggered = false;

void Update () {
    if (SteamVR_Input._default.inActions.GrabPinch.GetLastStateUp(SteamVR_Input_Sources.Any))
    {
        print("" Grab Pinch Up"");
    }
    float triggerValue = squeezeAction.GetAxis(SteamVR_Input_Sources.Any);

    if (triggerValue > 0f && !_triggered)
    {
        _triggered = true; // This will prevent the following code to be executed each frames when pressing the trigger.
        if(!video.isPlaying) { // You dont need a paused boolean as the videoplayer has a property for that.
            video.Play();
        } else {
            video.Pause();
        }
    } else {
         _triggered = false;
    }    
}
}

You need to drag and drop the VideoPlayer in the editor and it should be it.
"
"
****Update:** I've created a test scene where I've recreated the usage of a canvas with image and text, primitive game objects and the use of two cameras in addition to the Camera Rig, with target textures set to the same render texture. In this state, it worked, however when I installed and upgraded all materials to via the lightweight render pipeline, the render texture turned pink and would not render anything from the cameras. 
Considering this, my next step forward is to remove the lightweight render pipeline via reverting to a previous commit which does not have the lightweight render pipeline.* If you run into the same situation remember if you do not have a previous commit you can revert to, after removing the lightweight render pipeline you will need to create new materials for all your game objects.* 
Problem: In one scene within a VR Project project we are using a world space canvas to display interactable UI. When running through the editor we have no issues, however, when we build the project, all UI canvas's become invisible, though with the use of a laser pointer we can still interact with buttons on the canvas.
I've narrowed the cause down to the use of a specific render texture (only one), which is applied to the target texture of two (2) cameras in the scene. The two cameras are used to provide a live feed to a mesh of the view of a device in the scene.
When I set the two cameras (neither are the main camera in the scene) target texture's to null, that is the only way which I can get the Canvas to appear.
When after running a build I always check the output_log.txt file and have not found any errors.
We are using:
Unity 2018.1.3f1,
VRTK 3.3.0a,
Steam VR w/HTC Vive,
Unity's Lightweight Render Pipeline,
Post Process layer
There is only one canvas in the scene, with all UI objects as children of that object. Our Canvas set up:
Note: I've set the VRTK_UI Canvas component to be inactive to check if that was the cause, and it was not.
[
Camera One:
Note: I've tried clicking the ""Fix now"" under the target texture, with no change or improvement
[
Camera 2:
Note: I've tried clicking the ""Fix now"" under the target texture, with no change or improvement
[
Mesh we are Rendering to:
[
Render Texture:
[
Main Camera:
[MainCamera]6
","
The lightweight render pipeline was the issue, removing it allowed for everything to work as expected/needed.
"
"
I need to call a function when teleported to a particular area. How can I achieve this using VRTK and unity.
","
VRTK_HeadsetCollision

Script Usage: Place the VRTK_HeadsetCollision script on any active scene GameObject.

Other than that it's pretty much a plain trigger collider workflow
"
"
I have the following simple example of initializing entities with components:
<!DOCTYPE html>
<html>
  <head>
    <title>Hello, WebVR! - A-Frame</title>
    <meta name=""description"" content=""Hello, WebVR! - A-Frame"">
    <script src=""https://aframe.io/releases/0.7.0/aframe.min.js""></script>
  </head>
  <script>
    AFRAME.registerComponent('a', {
      dependencies: ['b']
    });
    // Initializes second.
    AFRAME.registerComponent('b', {
      dependencies: ['c']
    });
    // Initializes first.
    AFRAME.registerComponent('c', {});
  </script>
  <body>
    <a-scene>
    </a-scene>
  </body>
  <script>
    sceneEl = document.querySelector('a-scene');
    aEntity = document.createElement('a-entity');
    aEntity.setAttribute('a');
    sceneEl.appendChild(aEntity);
  </script>
</html>

This is from the documentation of Aframe regarding components and dependencies

dependencies: allows for control on ordering of component initialization if a component depends on one or more other components. Component names specified in the dependencies array will be initialized left-to-right before initializing the current component. If the dependency have other dependency components, those other dependency components will be ordered in the same manner.

My question is why is this code not working. The code generates the a-entity as expected but no component is being attached. I would expect to see a, b, and c Attached to my entity. What am I doing wrong?
","
Looks like if you don't supply a value for setAttribute it's ignored. 
Try aEntity.setAttribute('a', ''); instead.
Console should show: <a-entity c="""" b="""" a="""" position="""" rotation="""" scale="""" visible=""""></a-entity>
<!DOCTYPE html>
<html>
  <head>
    <title>Hello, WebVR! - A-Frame</title>
    <meta name=""description"" content=""Hello, WebVR! - A-Frame"">
    <script src=""https://aframe.io/releases/0.7.0/aframe.min.js""></script>
  </head>
  <script>
    AFRAME.registerComponent('a', {
      dependencies: ['b']
    });
    // Initializes second.
    AFRAME.registerComponent('b', {
      dependencies: ['c']
    });
    // Initializes first.
    AFRAME.registerComponent('c', {});
  </script>
  <body>
    <a-scene>
    </a-scene>
  </body>
  <script>
    sceneEl = document.querySelector('a-scene');
    aEntity = document.createElement('a-entity');
    aEntity.setAttribute('a', '');
    sceneEl.appendChild(aEntity);
    console.log(aEntity)
  </script>
</html>

"
"
Is it possible for us to control the loading screen of browser during VR to VR transition ? I am currently using a oculus go (oculus browser) for development. The default loading screen is a void space with a spinner and  it would be nice to have custom content during that transition.
","
It’s not possible. Once you leave a page after navigation the browser is in control of what it’s rendered on screen until the new page loads. 
"
"
I'm working on a VR game, which has some climbing mechanics in it. The player will be able to grab onto edges and I want to use dynamic edge detection instead of what I'm doing right now, which is placing colliders on the edges that I want the player to be able to grab. Doing this takes a long time, and doesn't allow for any procedural mesh generation.
I know that something like this can be achived with shaders, but since they don't return any data, thats not an option. Are there any other ways I could do it? Even a way to mark the edges in the mesh instead of using colliders would be great.
","
Here's how I would do it. This is an opinionated take, not necessarily a best practice. Your mileage may vary.

Forget colliders unless your levels are small. They're too expensive here. I like to use colliders for classic physics rigidbody collisions. I don't find they're always the best match for motive interactions like walking or climbing etc, esp. in Unity, and often using a collider to detect a simple line-plane intersection is overkill.
Analyze your mesh (at runtime, say, during level load) and pull out grabbable edges/faces. Here's one way to do it. These will be navigation waypoints for your climb. Note that you have to define what constitutes a ""grabbable"" edge or face, which to me would mostly depend on the face orientation, which you can get from the face normal.
If your levels are large or complex, further group those faces into a volume tree structure (oct-tree, k-d, something to help you locate edges in the vicinity).
As the player climbs, test for intersections between the player's hand and nearby climbable faces. You may be able to rely on the player's actions here and perform your most time-consuming tests only when he ""clutches"" what he believes to be a climbable edge.

Under this system, climbing is essentially a mini-platformer. The hand must ""jump"" from one platform to another platform, enabling further vertical movement.
"
"
I am very new to unity and am building a VR app for Oculus Go. I want to pick and move the object by pointing the ray from the controller on the object and then picking or releasing it by pressing the trigger button. I want the object to stay fixed at the end of the ray's position rather than coming suddenly onto the controller. I have used this script to create a ray and basically allow the controller to pick it up but this script shits the object to the controller's position and as a result I can only move object in a circle(in 360 degrees). It also does not drop the object correctly, as the objects continue to float.
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class PlayerPointer : MonoBehaviour {

//Returns whatever object is infrount of the controller
private GameObject pointerOver;
[SerializeField]
//Is the object that is currently intractable
private PropBase selectedObject;
//Is the object currently stored in hand, ready to throw.
[SerializeField]
private PickUp inHand;

//This is a refrance to the object we want the pointer to be cast from.
[SerializeField]
public Transform controllerRef;
//This is where we want object we are holding to appear
[SerializeField]
private Transform holdingRef;
//The amount of force we want to throw objects from our hand with.
[SerializeField]
[Range(2,12)]
private float throwForce = 10;

//The script that handles the visuals to show what object is selected
[SerializeField]
private HighlightObject selectVisual;
private LineRenderer line;

void Start () {
    line = GetComponent<LineRenderer> ();
}

void Update () {
    //If a object is currently being held I don't want to select another 
object until it is thrown.
    if (inHand == null) {
        WorldPointer ();
    } else {
        line.SetPosition (0, controllerRef.position);
        line.SetPosition (1, controllerRef.position);
        pointerOver = null;
    }
    //This function handles how you intract with selected objects
    Intract ();
}

//This function handles shooting a raycast into the world from the 
controller to see what can be intracted with.
void WorldPointer(){
    //We set the line visual to start from the controller.
    line.SetPosition (0, controllerRef.position);

    RaycastHit hit;
    //We reset the pointer so things don't stay selected when we are 
pointing at nothing.
    pointerOver = null;

    //This sends a line from the controller directly ahead of it, it returns 
true if it hits something. Using the RaycastHit we can then get information 
back.
    if (Physics.Raycast (controllerRef.position, controllerRef.forward, out 
hit)) {
        //Beacuse raycast is true only when it hits anything, we don't need 
to check if hit is null
        //We set pointerOver to whatever object the raycast hit.
        pointerOver = hit.collider.gameObject;
        //We set the line visual to stop and the point the raycast hit the 
object.
        line.SetPosition (1, hit.point);

        //Here we check if the object we hit has the PropBase component, or 
a child class of its.
        if (pointerOver.GetComponent<PropBase> ()) {
            //We set the object to be highlighted
            selectVisual.NewObject (pointerOver);
        } else {
            selectVisual.ClearObject ();
        }
    } else {
        //If the raycast hits nothing we set the line visual to stop a
little bit infrount of the controller.
        line.SetPosition (1, controllerRef.position + controllerRef.forward 
* 10);
        selectVisual.ClearObject ();
    }

    Debug.DrawRay(controllerRef.position , controllerRef.forward * 
10,Color.grey);
}

void Intract(){
    //We set up the input ""OculusTouchpad"" in the Input manager
    if (Input.GetButtonDown (""Jump"") || OVRInput.GetDown 
(OVRInput.Button.PrimaryTouchpad)) {
        selectVisual.ClearObject ();                
        //Check if you are holding something you can throw first
        if (inHand != null) {
            inHand.Release (controllerRef.forward, throwForce);
            inHand = null;
            //We do this check here to prevent Errors if you have nothing 
selected
        } else if (selectedObject != null) {
            //Check if you can pick up the selected object second
            if (selectedObject.GetComponent<PickUp> ()) {
                //Beacuse PickUp is a child of PropBase, we can ask InHand 
to store selectedObject as PickUp, rather than use GetComponent
                inHand = selectedObject as PickUp;
                inHand.Store (holdingRef);
                //If non of the above were valid then simple call the 
trigger function of the selected object
            } else {
                selectedObject.Trigger ();
            }
        }
        //If you have a object that you need to hold down a button to 
intract with
    } else if (Input.GetButton (""Jump"") && selectedObject != null || 
OVRInput.Get (OVRInput.Button.PrimaryTouchpad) && selectedObject != null) {
        selectedObject.Pulse ();
        //When you are not pressing down the touchpad button, the selected 
object can be updated
    } else if (pointerOver != null) {
        if (pointerOver.GetComponent<PropBase> ()) {
            selectedObject = pointerOver.GetComponent<PropBase> ();
        } else {
            selectedObject = null;
        }
    } else {
        selectedObject = null;
    }

   }

}

And i have attached this script to the objects I want to pick:
public class PickUp : PropBase
{

private Rigidbody rb;

void Start()
{
    rb = GetComponent<Rigidbody>();
}

public virtual void Store(Transform NewParent)
{
    //The following stops the object being effected by physics while it's in 
the players hand
    rb.isKinematic = true;
    //And fixes it to the new parent it is given by the player script to 
follow.
    transform.parent = NewParent;
    //It then resets it's position and rotation to match it's new parent 
object
    transform.localRotation = Quaternion.identity;
    transform.localPosition = Vector3.zero;
}
public virtual void Release(Vector3 ThrowDir, float ThrowForce)
{
    //On Release the object is made to be effected by physics again.
    rb.isKinematic = false;
    //Free itself from following it's parent object
    transform.parent = null;
    //And applies a burst of force for one frame to propel itself away from 
the player.
    rb.AddForce(ThrowDir * ThrowForce, ForceMode.Impulse);
    }
}

What i'd like to see is have the position of the sphere change according to wherever the end of the ray is cast.
I have also attached this script to the player contoller, which allows it to move to a point by pointing to it and pressing the touchpad button.
using System.Collections;
using System.Collections.Generic;
using UnityEngine;

public class ClickToMove : MonoBehaviour
{

private Vector3 targetPos; //This Vector3 will store the position where we 
click to move.

private bool Moving = false; /*This bool keeps track of whether we are in 
the process of moving or not.*/

private GameObject targetInstance;

/*The variables we want to customize. Added info headers to these for the 
Unity Editor.*/

[Header(""Our Go controller object"")]

public GameObject goController;

[Header(""Movement Speed"")]

public float speed = 1;

[Header(""Stop When This Far Away From Target"")]

public float haltDistance = 0;

[Header(""Optional Target Object"")]

public GameObject targetObj;

void Update()

{

    MoveToTarget(); /*Here we simply run our MoveToTarget method in the 
Update method.*/

    //That way we don't clutter up the Update method with too much code.

}

void MoveToTarget() //Here we do the cluttering instead.

{

    var ray = new Ray(goController.transform.position, 
goController.transform.forward); /*Create a ray going from the goController 
position and in the Forward direction of the goController.*/

    RaycastHit hitInfo; //Store info about what the ray hits.

    Physics.Raycast(ray, out hitInfo, 100);

    if (OVRInput.GetUp(OVRInput.Button.PrimaryTouchpad)) /*If we release the 
trigger..*/

    {

        targetPos = hitInfo.point; /*Make our targetPos assume the 
positional value of the hit point.*/

        if (targetObj) /*If we have specified a Target Object to mark where 
we click*/

        //If we didn't, then we don't want to try to instantiate it.

        {

            if (targetInstance) /*If there is already a Target Object in the 
scene.*/

            {

                Destroy(targetInstance); //Destroy it.

            }

            targetInstance = Instantiate(targetObj, targetPos, 
transform.rotation); //Create our Target object at the position we clicked.

        }

        Moving = true; //And finally we set Moving to True.

    }

    if (Moving == true) //Since Moving is now true

    {

        transform.position = Vector3.MoveTowards(transform.position, new 
Vector3(targetPos.x, transform.position.y, targetPos.z), speed * 
Time.deltaTime); /*Transform our x and z position to move towards the 
targetPos.*/

        /*Note that our y position is kept at default transform position 
since we only want to move along the ground plane.*/

    }

    if (Vector3.Distance(transform.position, targetPos) <= haltDistance + 1) 
 /*Check proximity to targetPos. Mainly useful to keep your player from 
  setting a target position right next to say a building and then end up 
  clipping  through half of it.*/

    {

        if (targetInstance) //If we created a Target Object..

        {

            Destroy(targetInstance); //Then we want to destroy it when we 
reach it.

        }

        Moving = false; //Since we have now arrived at our target 
//destination.

    }

}

}

If anyone could point me in a right direction or help me with this, I would greatly appreciate it!
Thanks in advance.
","
Okay, with your updated question its now possible to try and answer.
First off - have you tried not resetting your BaseProp localPosition to the controller's?
Try commenting the line that says 
transform.localPosition = Vector3.zero;

This wil still orient the object and parent it to the controller but will lock it in a position relative to the moment of parenting.
You currently use ""holdingRef"" object as a place where the object appears. You may want to use ""controllerRef"" instead.
To vary distance at which the object appears you can set the object position to: 
controllerRef.position+ distance*controllerRef.forward

As this is the direction in which you fire your raycasts. You can get the hit distance by querying hit.distance.
If for any reason that doesn't work out for you, the very point of the raycast hitting the collider is available within HitInfo, so with hit.point you can extract the hit position and position the object relative to that point. Another very useful attribute of hitinfo is .normal, which enables you to get direction at which the hit happened.
You can pass that info along with your Store method. 
"
"
I'm working on a project that does serverside rendering and thereby streams the images from a Windows 10 server (maximum resolution 1920x1080) to an Android 7 client (maximum resolution 2960x1440). The used framework is the TrinusVr library.
As I have understood, the Trinus framework streams whatever is in the game view to the client. In the Unity Editor I have the pleasure to be able to set any arbitrary resolution; of course when I put a resolution greater than what my screen can do, on the server side I see a scaled down version of the image in the game view. However, on the client it would use exactly this high resolution.
Now for the built game: Of course I tried using
Screen.SetResolution

to set high resolutions in the built game, too. Too high resolutions though will always be clamped down to the maximum resolution of my screen.
So is there any way, to have it like in the Unity Editor: a scaled down image on the server and the high resolution image on the client? 
Does anybody know another way to achieve this using the trinus lib?
Thank you
","
You might be able to use the camera's Target Eye parameter, combined with a higher resolution render texture. But then you'd need at least one more render pass to render the lower-resolution version, which could be too costly for VR.
"
"
I am developing a VR application using Unity 2018.2.8f1 and the TrinusVR lib (a framework that does serverside rendering). When I press play in the editor or build for Linux it works as expected. However, when using the Windows build, only about one frame gets transmitted to the client, then the connection is reset. On the client it shows the errors:

Error streaming video error: java.net.SocketException: Connection reset
Error Error delivering sensor data: java.net.SocketException: Connection reset

Digging into into the logs of the server it seems that this reset is caused by a null reference exception:

Disconnected (Reason: ERROR:ERR_IMG_PROC:Could not deliver frame to client. 
  System.TypeInitializationException: The type initializer for 'ManagedToNativeWrapper' threw an exception. ---> 
  System.NullReferenceException: Object reference not set to an instance of an object
    at System.Drawing.ComIStreamMarshaler+ManagedToNativeWrapper..cctor () [0x00049] in :0`

Does somebody know, what is wrong here? Has anybody seen this problem before?
You can find the complete log files (Windows 10 server and android 7 client) here:
https://transfernow.net/419cp2m92898
Thank you
","
Turns out that it actually is a problem of the .NET version on Windows as the trinus lib is made for an older version. Setting the right parameters in Edit > Project Settings > Player, section Other Settings, resolves:

Scripting Runtime Version: .NET 3.5 Equivalent
Api Compatibility Level: .NET 2.0

"
"
In google's Cardboard API we can use CardboardView.setSettingsButtonEnabled(boolean) method to hide the settings button on the bottom of the screen.

    Now,Google VR API is graded to 1.0. Use GvrView insteads of 'CardboardView', and the settings button is moved to top|right when adding a close button on left|top, but I can't find any GvrView's methods which can hide the two buttons.
    Please help me how to fix it, Thanks
","
In the current version (1.170.0) of the GVR SDK. You can find the buttons and remove them (but only in older versions of Android):
// Settings Button
val settingsButton = gvrView.findViewById<ImageButton>(R.id.ui_settings_button)
settingsButton.visibility = View.GONE

// Back Button
val backButton = gvrView.findViewById<ImageButton>(R.id.ui_back_button)
backButton.visibility = View.GONE

// Alignment Marker
val alignmentMarker = gvrView.findViewById<RelativeLayout>(R.id.ui_alignment_marker)
alignmentMarker.visibility = View.GONE

"
"
I'm working on an Oculus Go app and haven't figured out yet how to sent it to our test users. The app is not published  to Store yet and we'd like to test on a dozen distributed headsets before publishing. So we uploaded a build to the Alpha channel, added a few people, and invitations were accepted. Everything should have work, but no one can find the app nor ""My Preview Apps"" section.
Seems like I missed something. I tried Developer Mode, dug into both Oculus Go and Oculus mobile app, w/o any success.
","
""My Preview Apps"" will be visible at the end of the ""Store"" page in the Oculus mobile app. 
"
"
I've started working on converting a game into Virtual reality. For starting purposes, I've created an empty project and imported Google VR plugin. But when I want to create the build in cellphone, following error occurs:

CommandInvokationFailure: Failed to re-package resources. See the Console for details.
      stderr[
      AndroidManifest.xml:30: error: No resource identifier found for attribute 'enableVrMode' in package 'android'

stdout[
Configurations:
 (default)`

I've searched internet, changed API levels, did everything I could. But it didn't worked. Need your help guys. 
","
The enableVrMode attribute was added in Android N. You will get this error if you are using anything less that this version. 
Download Android SDK Platform-Tools 24 and then Android 7.0 (API 24). You can do this from Android Studio. That should solve your problem.
If this does not solve your problem, note that the latest Google VR SDK includes some API that requires Unity VR Technical preview to work.You can read here for more information.
"
"
I have a 2D video and I would like to move it into 360. I'm aware of the differences and so on and so forth but, I would like to have a 360 video with something like a cinema room and, in the main screen, that 2D video would be displayed.
Is there any suggestion, automatic tool for that, or anything that could be useful? I'm open to use Unity3D, blender or any software related to video editing.
","
The usual approach is to use a computer generated world or a 360 image for the cinema room rather than a 360 video, and display your 'flat' 2D video on a 'screen' or wall in that generated room.
You basically render the video onto a texture that you have set up in Unity. This is supported as standard by Google VR for unity(https://developers.google.com/vr/develop/unity/video-overview):

Streaming Video Support
The GVR SDK for Unity includes an additional GVR video plugin that supports streaming flat and 360° videos in both mono and stereo formats by using the ExoPlayer library to handle decoding and rendering of video, audio, and related streams. ExoPlayer provides all the standard playback framework for both videos embedded in your application and streaming videos, including adaptive streaming support, such as DASH and HLS.

For example the Netflix version, VR Theatre, looks like this - the video plays on the 'screen' in front of the viewer:

If you look on the unity asset store you can find complete home or cinema 360 'images' that you can use in Unity also - for example (at the time of writing): https://assetstore.unity.com/packages/templates/vr-home-cinema-66863
"
"
i have been trying for some time to set an external image over the environment.
This is my index.js:
import React from 'react';
import {
    AppRegistry,
    StyleSheet,
    Text,
    asset,
    View,
    VrButton,
} from 'react-360';

import {Environment} from 'react-360';

// Set the background to a 360 or 180 image
Environment.setBackgroundImage(
    {uri: 'https://devcdn.player360.de/dev/media-768-raw.jpg'}
    // asset('media-768-raw.jpg')
);

export default class Hello360 extends React.Component {
    // Our component will keep track of this state
    state = {
    };


    componentDidMount(){
    }


    render() {
        return (
            <View style={styles.panel}>
                <Text>
                    <Text>Hey</Text>
                </Text>
            </View>
        );
    }
};


const styles = StyleSheet.create({
    panel: {
        // Fill the entire surface
        width: 1000,
        height: 600,
        backgroundColor: 'rgba(255, 255, 255, 0.4)',
        justifyContent: 'center',
        alignItems: 'center',
    }
});

AppRegistry.registerComponent('Hello360', () => Hello360);

React seems to try to load the image but it simply cancled for some reason.
Developer Tools: Network Screenshot
Also the console output gives no indication of a CORS or any other exception:
Console Output Screenshot
The background of my scene is still black. What would be the correct way to include dynamically external images in react-360?
","
It was still the right way to load external images. 
The problem was, that a filter was active in my console output. Therefore I was not able to see the CORS warning in the console. 
It was still a CORS problem on the devcdn.player360.de server. We fixed the CORS headers and are now able to load external images.
"
"
I have a 'pointer enter' event, but i only want the event to trigger if the gaze have been active on the object for n seconds.
public void PointerEnter() {
   // change scene if the gaze point have been active for n seconds.
}

Anyway to achive this ?
Just having a timeout wont do as it will still execute rather the pointer stay locked on the object or not.
","
You can use a boolean variable to keep state of when pointer has entered and exits by setting it to true and false. This boolean variable can then be checked in the Update function. When it is true, start a timer and if it becomes false during the timer, reset the timer to 0. Check when timer is more than the x second then load new scene.
The example below assumes that PointerEnter is called when pointer is pointing and PointerExit when it's no longer pointing. The functions might be different depending on the VR plugin you're using but the rest of the code is the-same.
const float nSecond = 2f;

float timer = 0;
bool entered = false;

public void PointerEnter()
{
    entered = true;
}

public void PointerExit()
{
    entered = false;
}

void Update()
{
    //If pointer is pointing on the object, start the timer
    if (entered)
    {
        //Increment timer
        timer += Time.deltaTime;

        //Load scene if counter has reached the nSecond
        if (timer > nSecond)
        {
            SceneManager.LoadScene(""SceneName"");
        }
    }
    else
    {
        //Reset timer when it's no longer pointing
        timer = 0;
    }
}

"
"
I am using .png rounded corner image with entity tempalate as data thumbnail when I go to virtual mode... rounded corner image appearing with black square box (see screenshot for further understanding ).
<img id=""subway"" crossorigin=""anonymous"" src=""images/new/subway.png"">
<a-entity id=""links"" layout=""type: line; margin: 2.5"" position=""-5 4.6 -4"">
      <a-entity template=""src: #link"" data-src=""#expense-subway"" opacity=""0.5"" transparent=""true"" rotation=""0 30 0"" data-thumb=""#subway"" class=""links""></a-entity>
</a-entity>


","
When an image should display areas as fully opaque and others as fully transparent, but nothing in between at 1%–99% transparency, you should keep the default opacity/transparency values (opacity: 1.0; transparent: false;) on the material component. This is called ""mask"" or ""cutout"" mode. Then, set the alphaTest value as a threshold for which parts of the image are hidden. You'll want to tweak the value a bit, depending on the image, but in this case alphaTest: 0.25 looks reasonable to me.

"
"
am new to webvr and need a little bit of help, so i built a virtual reality room, with little interior details, now i want a user to only move to specific points, for example say a user hovers over a window, than a the camera/view should jump to that window/point not necessary viewing outside just a little bit, this is to avoid users going through walls and objects. So how can one solve this? Thanks in advance.
","
Managed to get it right, thanks to JavaScript, so the camera now change position to that specific point you point on with cursor and than rotates.function windowCameraChange(){var curt = document.getElementById(""MainCamera"");curt.setAttribute('camera', 'zoom', '1.450');curt.setAttribute('rotation','0 -90 0');}
"
"
I used a canvas and Unity UI elements as a HUD. I want HUD to be in front of everything but it's not. 

","
Haven't worked with Cardboard before but I beleive you can fix this with camera culling. Just follow the steps below: 
Canvas:
1.Select the Canvas and change the Layer to UI. The default is UI but make sure that it is still set to UI.
Default/Main Camera:
1.Select the Main Camera that was there already there and change the Clear Flags to Skybox.
2.Change the Culling Mask to Everything. Click on Everything which is a setting of Culling Mask and de-select/uncheck UI.
3.Change Depth to 0.
New Camera:
1.Create a new Camera and name it UI Camera.
2.Select the UI Camera, change the Clear Flags to Depth Only.
3.Change the Culling Mask to UI.
4.Change Depth to 1.
5.If the Canvas you want to display on top has the ""Render Mode"" set to ""World Space"" or ""Screen Space - Camera"" then drag the UI Camera to the Canvas' Render Camera/Event Camera slot. You don't have to do this if the ""Render Mode"" is set to ""Screen Space - Overlay"".
That's it. This will always display the UI in the front. It should on default but its not working for you for some reason.
"
"
I have downloaded ViroCore lib the latest version (virocore-release-v_1_7_2.aar), then I added dependencies below to my build.gradle file
dependencies {
implementation fileTree(dir: 'libs', include: ['*.jar'])
implementation 'com.android.support:appcompat-v7:25.0.0'
implementation 'com.google.android.exoplayer:exoplayer:r2.2.0'
androidTestImplementation('com.android.support.test.espresso:espresso-core:2.2.2', {
    exclude group: 'com.android.support', module: 'support-annotations'
})
implementation 'com.android.support.constraint:constraint-layout:1.0.2'
testImplementation 'junit:junit:4.12'
implementation (name:'virocore-release-v_1_7_2', ext: 'aar')
implementation 'com.google.protobuf.nano:protobuf-javanano:3.0.0-alpha-7'
implementation 'com.amazonaws:aws-android-sdk-core:2.2.+'
implementation 'com.amazonaws:aws-android-sdk-ddb:2.2.+'
implementation 'com.amazonaws:aws-android-sdk-ddb-mapper:2.2.+'
implementation 'com.amazonaws:aws-android-sdk-cognito:2.2.+'
implementation 'com.amazonaws:aws-android-sdk-cognitoidentityprovider:2.2.+'}

and then wrote to my ViroActivity code below:
private ViroView createGVRView() {
    return new ViroViewGVR(this, new ViroViewGVR.StartupListener() {

        @Override
        public void onSuccess() {
            onRendererStart();
        }

        @Override
        public void onFailure(ViroViewGVR.StartupError error, String errorMessage) {
            onRendererFailed(error.toString(), errorMessage);
        }
    }, new Runnable() {
        @Override
        public void run() {
        }
    });

}
But when run the app, I always see next Exception:
E/AndroidRuntime: FATAL EXCEPTION: main
              Process: com.example.virosample.gvr.ovr, PID: 17304
              java.lang.NoClassDefFoundError: Failed resolution of: Lcom/google/vr/ndk/base/GvrLayout;
                  at com.viro.core.ViroViewGVR.init(ViroViewGVR.java:287)
                  at com.viro.core.ViroViewGVR.<init>(ViroViewGVR.java:211)
                  at com.example.panorama.ui.view.ViroActivityVR.createGVRView(ViroActivityVR.java:104)
                  at com.example.panorama.ui.view.ViroActivityVR.onCreate(ViroActivityVR.java:90)
                  ...

Anyone knows how to resolve this exception?
","
I solved this issue. I added to my dependency just viro library, but I must add additionally both sdk-common.aar and core.aar which you can download at the official virocore site.
"
"
I have downloaded ViroCore lib the latest version (virocore-release-v_1_7_2.aar), then I added dependencies below to my build.gradle file
dependencies {
implementation fileTree(dir: 'libs', include: ['*.jar'])
implementation 'com.android.support:appcompat-v7:25.0.0'
implementation 'com.google.android.exoplayer:exoplayer:r2.2.0'
androidTestImplementation('com.android.support.test.espresso:espresso-core:2.2.2', {
    exclude group: 'com.android.support', module: 'support-annotations'
})
implementation 'com.android.support.constraint:constraint-layout:1.0.2'
testImplementation 'junit:junit:4.12'
implementation (name:'virocore-release-v_1_7_2', ext: 'aar')
implementation 'com.google.protobuf.nano:protobuf-javanano:3.0.0-alpha-7'
implementation 'com.amazonaws:aws-android-sdk-core:2.2.+'
implementation 'com.amazonaws:aws-android-sdk-ddb:2.2.+'
implementation 'com.amazonaws:aws-android-sdk-ddb-mapper:2.2.+'
implementation 'com.amazonaws:aws-android-sdk-cognito:2.2.+'
implementation 'com.amazonaws:aws-android-sdk-cognitoidentityprovider:2.2.+'}

and then wrote to my ViroActivity code below:
private ViroView createGVRView() {
    return new ViroViewGVR(this, new ViroViewGVR.StartupListener() {

        @Override
        public void onSuccess() {
            onRendererStart();
        }

        @Override
        public void onFailure(ViroViewGVR.StartupError error, String errorMessage) {
            onRendererFailed(error.toString(), errorMessage);
        }
    }, new Runnable() {
        @Override
        public void run() {
        }
    });

}
But when run the app, I always see next Exception:
E/AndroidRuntime: FATAL EXCEPTION: main
              Process: com.example.virosample.gvr.ovr, PID: 17304
              java.lang.NoClassDefFoundError: Failed resolution of: Lcom/google/vr/ndk/base/GvrLayout;
                  at com.viro.core.ViroViewGVR.init(ViroViewGVR.java:287)
                  at com.viro.core.ViroViewGVR.<init>(ViroViewGVR.java:211)
                  at com.example.panorama.ui.view.ViroActivityVR.createGVRView(ViroActivityVR.java:104)
                  at com.example.panorama.ui.view.ViroActivityVR.onCreate(ViroActivityVR.java:90)
                  ...

Anyone knows how to resolve this exception?
","
I solved this issue. I added to my dependency just viro library, but I must add additionally both sdk-common.aar and core.aar which you can download at the official virocore site.
"
"
I am trying to use Google VR asset for unity and it is pretty easy, I have already built the most of the things in my project.
I have just one issue, I don't find a way to display text to the player...
In my project the user walks around a model, and when he looks on a specific part of it, a window with the proper information pops up.
I have tried using OnGui function, but for no avail...
Do you have any idea how to do that?
","
You have to use a world-space canvas or a 3D Text. You will probably also want some billboarding (the text facing the user).
I often use this helper for this (just drag to a new gameobject):
using UnityEngine;

[RequireComponent(typeof(TextMesh))]
[RequireComponent(typeof(MeshRenderer))]
[ExecuteInEditMode]
public class Billboarded3dText : MonoBehaviour {

    public bool yawOnly = false;
    public float minimalViewDistance = 0.5f;

    private MeshRenderer meshRenderer;
    private TextMesh textMesh;


    private void OnEnable()
    {
        meshRenderer = GetComponent<MeshRenderer>();
        textMesh = GetComponent<TextMesh>();
    }


    private void LateUpdate()
    {
        var cam = Camera.main;
        if (cam == null) return;

        var lookDir = transform.position - cam.transform.position;
        if (yawOnly) lookDir.y = 0;

        if(Vector3.SqrMagnitude(lookDir) < minimalViewDistance)
        {
            meshRenderer.enabled = false;
        }
        else
        {
            meshRenderer.enabled = true;
            transform.rotation = Quaternion.LookRotation(lookDir);
        }
    }


}

"
"
Does anyone know how to create a 360º camera in three.js?
I'm trying to render the entire scene as a 360º panorama like you would with a go pro 360 rig.
I'm trying to recreate a panorama by arranging several screens in a circle and stretch a threejs window across all the screens.
For this I need a very wide window that has a tree.js camera that captures the entire scene in 360º
Is this possible?
","
It is definitely possible, as it was already implemented: https://github.com/spite/THREE.CubemapToEquirectangular
That library will just export snapshots as png, but looking at the code it should be possible to integrate the same method it uses into realtime-rendering if you want to...
"
"
I would like to run Google's ARCore sample demos using a Cardboard headset. 
I've checked off 'Virtual Reality Supported' under the Android Player Settings and have included 'Cardboard' as the VR SDK. 
The resulting images from the camera do become stereoscopic. However, everything is a bit distorted upon putting the phone into an actual headset. I'm using a Pixel 2 fwiw. I haven't seen anything mentioned about ARCore working with tandem with VR, but I don't see why splitting the camera into a stereoscopic view shouldn't work. 
Any ideas on how to fix the distorted view? 
","
The FOV (field of view) of the phone's camera is around 60 degrees (horizontal direction), The VR camera i think is around 90 degrees thus there will be stretching/distortion of the image. see https://github.com/google-ar/arcore-unity-sdk/issues/134 for more info
"
"
I have a earth object that I would like to rotate using animation in A-frame.
I just want the object to rotate forever and I don't know how to do it. 
Below is my code thanks!
    <a-sphere rotation=""45 0 0"" position=""-44.277 50 -80.933"" radius=""30"" src=""images/earth.png"" roughness=""0.6"">

        <a-animation attribute=""rotation""
                     easing=""linear""
                     dur=""10000""
                     to=""0 360 0""
                     repeat=""indefinite"">

        </a-animation>
    </a-sphere>

","
That code is correct. It will take 10,000 ms (10 seconds) to complete 1 rotation, and it will repeat indefinitely (""forever""). I just tested the code and it works. If the earth.png texture isn't appearing, it may be difficult to see the sphere rotation. If you use the same code on an <a-box>, for example, it's much more obvious.
That said, since you have the rotation for the <a-sphere> set to 45 along the x axis and have the <a-animation> set to rotate to 360 over the y axis, it will not rotate in a linear fashion.
This can be remedied by adding a container <a-entity> element and setting the default position and rotation on that entity, removing it from the <a-sphere>, which will now be positioned relative to its container.
Here is demo: https://codepen.io/dansinni/pen/MVgqxd
Note that I had to use a different texture for the Earth.
"
"
We're working on a video player app for the Go. We built a straightforward raycaster script to trigger onClick events when a user points at a UI Button element and pulls the trigger:
bool triggerPulled = OVRInput.GetDown(OVRInput.Button.PrimaryIndexTrigger);
    if (Physics.Raycast(transform.position, transform.forward, out hit, 1000))
    {

        if ( triggerPulled )
        {

            // if we hit a button
            Button button = hit.transform.gameObject.GetComponent<Button>();

            if (button != null)
            {
                button.onClick.Invoke();
            }

        }
....
    }

We'd really like to be able to manipulate UI Sliders with the laser pointer as well as buttons, but aren't clear on whether there are analogous events we can trigger for the appropriate behavior. We can call onValueChanged to alter the value, but that doesn't really give us the sliding behavior we'd like, only lets us set the new value once we know where we're ending up.
Does anybody have good ideas for how to approach this?
","
Oculus Integration has a script called OVRInputModule.cs. I believe this is designed for what you are trying to do. In order to do this there are a few steps. A GIF of the result.

To achieve this, I split the code into three scripts; ControllerInfo, ControllerPointer, and SetUITransformRay.
Controller info is just a class that makes sure that scripts always have the correct information.
using UnityEngine;
using static OVRInput;

public class ControllerInfo : MonoBehaviour {
    [SerializeField]
    private Transform trackingSpace;
    public static Transform TRACKING_SPACE;
    public static Controller CONTROLLER;
    public static GameObject CONTROLLER_DATA_FOR_RAYS;

    private void Start () {
        TRACKING_SPACE = trackingSpace;
    }

    private void Update()
    {
        CONTROLLER = ((GetConnectedControllers() & (Controller.LTrackedRemote | Controller.RTrackedRemote) & Controller.LTrackedRemote) != Controller.None) ? Controller.LTrackedRemote : Controller.RTrackedRemote;
    }
}

ControllerPointer draws a line from the controller. This represents the way the controller is pointing. Add this to a LineRenderer.
using UnityEngine;
using UnityEngine.EventSystems;
using static OVRInput;

[RequireComponent(typeof(LineRenderer))]
public class ControllerPointer : MonoBehaviour
{
    [SerializeField]
    private SetUITransformRay uiRays;
    private LineRenderer pointerLine;
    private GameObject tempPointerVals;

    private void Start()
    {
        tempPointerVals = new GameObject();
        tempPointerVals.transform.parent = transform;
        tempPointerVals.name = ""tempPointerVals"";
        pointerLine = gameObject.GetComponent<LineRenderer>();
        pointerLine.useWorldSpace = true;

        ControllerInfo.CONTROLLER_DATA_FOR_RAYS = tempPointerVals;
        uiRays.SetUIRays();
    }

    private void LateUpdate()
    {
        Quaternion rotation = GetLocalControllerRotation(ControllerInfo.CONTROLLER);
        Vector3 position = GetLocalControllerPosition(ControllerInfo.CONTROLLER);
        Vector3 pointerOrigin = ControllerInfo.TRACKING_SPACE.position + position;
        Vector3 pointerProjectedOrientation = ControllerInfo.TRACKING_SPACE.position + (rotation * Vector3.forward);
        PointerEventData pointerData = new PointerEventData(EventSystem.current);
        Vector3 pointerDrawStart = pointerOrigin - pointerProjectedOrientation * 0.05f;
        Vector3 pointerDrawEnd = pointerOrigin + pointerProjectedOrientation * 500.0f;
        pointerLine.SetPosition(0, pointerDrawStart);
        pointerLine.SetPosition(1, pointerDrawEnd);

        tempPointerVals.transform.position = pointerDrawStart;
        tempPointerVals.transform.rotation = rotation;
    }
}

SetUITransformRay will automatically set the controller for the OVRInputModule ray. It is required as normally you have two controllers in your scene; one for left and one for right. See the full method below for more info on how to set this up. Add this component to your EventSystem that is generated when you add a canvas.
using UnityEngine;
using UnityEngine.EventSystems;

public class SetUITransformRay : MonoBehaviour
{
    [SerializeField]
    private OVRInputModule inputModule;
    [SerializeField]
    private OVRGazePointer gazePointer;

    public void SetUIRays()
    {
        inputModule.rayTransform = ControllerInfo.CONTROLLER_DATA_FOR_RAYS.transform;
        gazePointer.rayTransform = ControllerInfo.CONTROLLER_DATA_FOR_RAYS.transform;
    }
}


Steps to Use
Step 1) Scene Setup
Import the Oculus Integration pack. Drag the OVRCameraRig onto your scene. Drag a OVRTrackedRemote into the left and right hand anchors. Set each remote to left or right depending on the anchor.

Step 2) Set up your canvas and eventsystem.
On the Canvas,

On the EventSystem,

Step 3) Set up correct objects
Create 3 objects; Controller Manager, Controller Pointer, and OVRGazePointer.
For the OVRGazePointer, I just quickly went in the example scene for UI, Oculus\VR\Scenes, and prefabed the OVRGazePointer there.
On Controller Pointer, there is a LineRenderer. This has two points, doesn't matter where. It uses world space. It has a width of 0.005.
Only having two points, and using world space is very important. This is because ControllerPointer the script relies on those two being set like that.

"
"
I Have two absolute xyz positions
first point-> x=""2.08039166666667"" y=""41.4722694444444"" z=""128.71
and
second point->x=""2.08039166666667"" y=""41.4723111111111"" z=""128.79"" 
both positions are absolute from 0 0 0
now if i change the center to first point  how i can get position from second point?
","
To get the relative position of the second point from the first point, you'd subtract the first point from the second point (x2-x1, y2-y1, z2-z1).
"
"
I have this json string:
[
   [
      {
         ""Antibiotic after diagnosis"":[
            ""Azithromycin"",
            ""Ciprofloxacin HCl"",
            ""Ampicillin Sodium""
         ],
         ""City"":[
            ""Tel Aviv"",
            ""Jerusalem""
         ]
      }
   ],
   [
      {
         ""Antibiotic after diagnosis"":""Azithromycin"",
         ""City"":""Tel Aviv""
      },
      {
         ""Antibiotic after diagnosis"":""Ciprofloxacin HCl"",
         ""City"":""Jerusalem""
      }
   ]
]

I deserialized this string:
data = Newtonsoft.Json.JsonConvert.DeserializeObject<List<object>>(""*json str*"");
JParameters = data[0] as JArray;
Debug.Log(JParameters[""Antibiotic after diagnosis""]);

But when i run the code it crashed on the line (Debug.Log(JParameters[""Antibiotic after diagnosis""]);) with the following error:

""ArgumentException: Accessed JArray values with invalid key value: ""Antibiotic after diagnosis"". Int32 array index expected.""

","
The index into JParameters should be an integer as the error states.  What you want is:
JParameters[0][""Antibiotic after diagnosis""]
The above code selects the first element, and then selects the value for the dictionary key ""Antibiotic after diagnosis"".
I recommend reading w3schools JSON to properly understand how JSON works.
"
"
im currently trying to run an app with daydream compatibility. Everything works fine except when im trying to switch to the cardboard mode, and i get the following error message ""This Cardboard application is not compatible with the Daydream headsets""
According to some post ive found online, it could be a manifest issue ( https://github.com/Samsung/GearVRf/issues/1618 and https://github.com/googlevr/gvr-android-sdk/issues/295 ) but my app properly declare the DAYDREAM intent filters, here's my manifest ( my problem occurs while in the PlayerActivity )
<?xml version=""1.0"" encoding=""utf-8""?>


<!-- Required GLES 2 -->
<uses-feature android:glEsVersion=""0x00020002"" android:required=""true"" />

<!-- Required by the app to stream video. -->
<uses-permission android:name=""android.permission.INTERNET"" />

<!-- Make accelerometer and gyroscope hard requirements for good head tracking. -->
<uses-feature android:name=""android.hardware.sensor.accelerometer"" android:required=""true""/>
<uses-feature android:name=""android.hardware.sensor.gyroscope"" android:required=""true""/>

<!-- Indicates use of Android's VR-mode, available only on Android N+. -->
<uses-feature android:name=""android.software.vr.mode"" android:required=""true""/>
<!-- Indicates use of VR features that are available only on Daydream-ready devices. -->
<uses-feature android:name=""android.hardware.vr.high_performance"" android:required=""true""/>

<application
    android:allowBackup=""true""
    android:icon=""@mipmap/ic_sphereplay""
    android:label=""@string/app_name""
    android:theme=""@style/AppTheme""
    android:largeHeap=""true"">

    <activity
        android:name="".MainActivity""
        android:configChanges=""orientation|keyboardHidden|screenSize""
        android:theme=""@style/SpherePlayMaterialTheme""
        android:label=""@string/app_name"" >
        <!-- Cardboard -->
        <intent-filter>
            <action android:name=""android.intent.action.MAIN"" />
            <category android:name=""android.intent.category.LAUNCHER"" />
        </intent-filter>
    </activity>

    <activity
        android:name="".PlayerActivity""
        android:screenOrientation=""landscape""
        android:theme=""@style/SpherePlayMaterialTheme""
        android:resizeableActivity=""false""
        android:configChanges=""density|keyboardHidden|navigation|orientation|screenSize|uiMode""
        android:launchMode=""singleTask"">

        <intent-filter>
            <category android:name=""com.google.intent.category.CARDBOARD"" />
            <action android:name=""android.intent.action.VIEW""/>
            <category android:name=""android.intent.category.DEFAULT""/>
            <category android:name=""android.intent.category.BROWSABLE""/>
            <data android:scheme=""spmedia""/>
        </intent-filter>
    </activity>

    <activity
        android:name="".DaydreamActivity""
        android:screenOrientation=""landscape""
        android:theme=""@style/SpherePlayMaterialTheme""
        android:resizeableActivity=""false""
        android:configChanges=""density|keyboardHidden|navigation|orientation|screenSize|uiMode""
        android:enableVrMode=""@string/gvr_vr_mode_component""
        android:launchMode=""singleTask"">

        <!-- The VR icon to be used in Daydream Home comes in two parts:
            a foreground icon and a background icon.  -->
        <meta-data
            android:name=""com.google.android.vr.icon""
            android:resource=""@drawable/vr_icon_fg"" />
        <meta-data
            android:name=""com.google.android.vr.icon_background""
            android:resource=""@drawable/vr_icon_bg"" />

        <intent-filter>
            <action android:name=""android.intent.action.MAIN"" />
            <category android:name=""com.google.intent.category.DAYDREAM"" />
        </intent-filter>
    </activity>

</application>


Any idea what to do in order to be able to use the cardboard mode?
","
I am not a programmer, but I think I may know of a workaround. I have a zte blade max model number z982 from MetroPCS Awesome phone by the way. First thing I did was install CardBoard on it, was disappointed that it didn't want to use my bluetooth gamepad. So I tryed to get the DayDream app, but google wouldn't let me download it to my cheap phone so I don't know if it could use my bluetooth gamepad or not. but I did see that I could download Google VR Services app, I hoped that would let me use my bluetooth gamepad in cardboard, no luck. But I did find out that I could setup google cardboard to use ether it self or Google VR Services in the settings.
For your problem I think you need to go into cardboard, then demo games. When the screen comes up tap on the * button in the middle, bottom of the screen, that takes you to the settings screen you should see a popup that asks you which service you want to use. select cardboard and one time. I believe this workaround will work for you.
"
"
I am new to Unity and Oculus. I have a bunch of images (their path and other information are loaded from a JSON file) which I am trying to render in a VR room. And want to give the user an experience that he can move these images within that room using Oculus Touch.
I placed an Empty Object which has a script that iterates through JSON and creates a prefab object (which has Rigid body,Box Collider and OVRGrabbable -so that it can be grabbed in VR, components). Moreover this prefab object has a script which is responsible to load images in a Sprite.
What is working?
Images are getting rendered and can be grabbed and moved. 
What is not working as desired?
I followed this tutorial, and as shown here angles of the cube are persisted pretty fine. But when an image is grabbed and rotated, it doesn't persist it's side angles. As shown in following image:

Question
Is there any way that I could fix it? I tried to look for it online but as I am new to Unity I am not quite sure what exactly I am missing.
Any clues would be really appreciated.
","
I think the problem lies in your hierarchy. Your Images game object (the parent of your DisplayImage) has a scale of (1.88,1,1). Trying to rotate child objects whose parent object have non-uniform scale in Unity gives you funky results. Try resetting the scale of your Images object to (1,1,1) and see if that helps.

This behaviour occurs because of the nature of the ""parent/child"" relationship. The child's coordinate system is relative to it's parents. We can think of it as the parent defining a coordinate system for our children. This is what we call local space. So, when we perform any linear transformations on the parent, it's like we're performing them on the whole parent coordinate system that our children are using to define their orientation. So translating the parent will move all of its children with it, because our whole local coordinate space has moved with the parent. Note that our children will still have the same localPosition, but their global position will have changed.
The same logic applies to scaling and rotation. Rotating the parent essentially rotates the whole coordinate space that our child is using around the center point of the parent (which would be the point (0,0,0) in local space). So all children would then be rotated as if they were an extension of the parent object.
In our situation we've scaled our parent, thus scaling the whole coordinate system we use to define our child object. This means that anything using the parent's coordinate space will also be scaled according to the parent's scale. In our situation, the parent was scaled by (1.88,1,1). So everything attached to the parent was also scaled along the parent's X-Axis by 1.88, resulting in the weird effect seen in your screenshots. Despite rotating our child object, it's still scaled along the parent's X-axis.
(Link to the official documentation on this.)
The solution to this is to apply your linear transformations as deep in the hierarchy as possible. Here, instead of scaling the parent, scale the child. If the parent object needs to be scaled, or its scale changes on the fly, another solution would be to remove the child from the parent/child hierarchy and manipulate its transform based on the old parent's transform in a script. In this case you could set the unlinked child's position and rotation to the parent's position and rotation, but ignore the scale.
"
"
when i enter the Vr-Mode on mobile you get an cursor to interact. Now my Problem the cursor is disorted. When i move the lenses of my vr-glasses i can get the cursor to be one but then my scene isn´t right anymore. Here my camera with my cursor:
  <!--------camera---------->    
<a-entity rotation=""0 90 0"">
  <a-camera user-height=""0"" wasd-controls-enabled=""false"" look-controls>
    <a-cursor id=""curseid"" visible=""false"" opacity=""0"" fuse=""true"" fusetimeout=""4000""
      position=""0 0 -0.1""
      raycaster=""objects: .clickable""
      geometry=""primitive: ring;
      radiusInner: 0.002;
      radiusOuter: 0.003""
      material=""color: red; shader: flat"">
      <a-animation attribute=""scale""
                      to=""3 3 3""
                      dur=""2000""
                      begin=""cursor-fusing""
                      fill=""backwards""
                      easing=""linear"">
      </a-animation>
      <a-animation attribute=""material.opacity"" begin=""fade"" to=""0""></a-animation>
      <a-animation attribute=""material.opacity"" begin=""fade_cursor"" dur=""2000"" from=""0"" to=""1""></a-animation>
   </a-cursor>
    <a-entity id=""redcircle"" visible=""false"" position=""0 0 -0.1"" 
            geometry=""primitive: ring;
            radiusInner: 0.007;
            radiusOuter: 0.0077""
            material=""color: red; opacity: 0; shader: flat"">
     <a-animation attribute=""material.opacity"" begin=""fade"" to=""0""></a-animation>
    <a-animation attribute=""material.opacity"" begin=""fade_circle"" dur=""2000"" from=""0"" to=""0.25""></a-animation></a-entity>
 </a-camera>  
</a-entity>  

","
cursor is way to close on the camera i set it on z: -1 and now its fine
"
"
I have added the gradle files. Please review them and help me fix the problem. I am having a problem in gradle sync as I copied this project from github
I followed every single step.
Thanks
    // Top-level build file where you can add configuration options common      to all sub-projects/modules.enter code here

    buildscript {
        repositories {
            jcenter()
        }
        dependencies {
            classpath 'com.android.tools.build:gradle:2.2.3'

            // NOTE: Do not place your application dependencies here; they  belong
            // in the individual module build.gradle files
        }
    }

    allprojects {
        repositories {
            jcenter()
        }
    }

    task clean(type: Delete) {
        delete rootProject.buildDir
    }

    task copy_cardboard_images() {
        doFirst {
            exec {
                if (System.properties['os.name'].toLowerCase().contains('windows')) {
                    // windows
                    commandLine 'cmd', '/c', 'adb', 'pull', '/mnt/shell/emulated/0/DCIM/CardboardCamera'
                } else {
                    // linux
                    commandLine 'adb', 'pull', '/mnt/shell/emulated/0/DCIM/CardboardCamera'
                }
            }
        }
        doLast {
            copy {
                from 'CardboardCamera'
                into 'website/images'
            }
        }
    }

    task remove_cardboard_images() {
        doFirst {
            exec {
                if (System.properties['os.name'].toLowerCase().contains('windows')) {
                    // windows
                    commandLine 'cmd', '/c', 'adb', 'shell', 'rm',  '-r', '/mnt/shell/emulated/0/DCIM/CardboardCamera'
                } else {
                    // linux
                    commandLine 'adb', 'shell', 'rm',  '-r', '/mnt/shell/emulated/0/DCIM/CardboardCamera'
                }
            }
        }
    }

    task move_cardboard_images() {
        dependsOn copy_cardboard_images
        dependsOn remove_cardboard_images
    }



Build.gradle : app module 


apply plugin: 'com.android.application'

android {
    compileSdkVersion 23
    buildToolsVersion ""23.0.2""

    defaultConfig {
        applicationId ""com.google.devrel.vrviewapp""
        minSdkVersion 19
        targetSdkVersion 23
        versionCode 1
        versionName ""1.0""
    }
    buildTypes {
        release {
            minifyEnabled false
            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
        }
    }
}

dependencies {
    compile fileTree(dir: 'libs', include: ['*.jar'])
    compile 'com.android.support:appcompat-v7:23.3.0'
    compile 'com.android.support:design:23.3.0'
    compile project(':gvr-android-sdk/libraries:common')
    compile project(':gvr-android-sdk/libraries:commonwidget')
    compile project(':gvr-android-sdk/libraries:panowidget')
    compile project(':gvr-android-sdk/libraries:videowidget')
}


Settings.gradle : 

This code belongs to settings.gradle file

include ':app'
include ':app'
include ':gvr-android-sdk/libraries:audio'
include ':gvr-android-sdk/libraries:base'
include ':gvr-android-sdk/libraries:common'
include ':gvr-android-sdk/libraries:commonwidget'
include ':gvr-android-sdk/libraries:panowidget'
include ':gvr-android-sdk/libraries:videowidget'

","
Have you checked all the dependencies of your Android project, located in the Android folder?
Inside there is the settings.gradle file. In this file every dependency of your project will be checked.
Example :
include 'your dependencies'

project(':your dependencies').projectDir = new File('path dependencies')

"
"
I am using the latest google vr plugin for unity and latest unity version but it seems that the split screen and the head movement doesn't work?
I have read some of article that if you build it will automatically show in the mobile device but in my case it doesn't any advice on this one? 
Here are my settings in unity. Thanks in advance

","
I make an answer because a comment would be too long:
If the HelloVR.unity scene is not working on your mobile-phone, your phone might be not supported? Is it running in Editor?
In the Editor you can not see split screen. But you can use Instant Preview - here you can find a documentation. I think that is the best way to check if split screen is working.
In your scene you have to find GvrEventSystem: here you can enable VR Mode Only: your app will always render in split-screen on mobile. (But not in Editor mode!) If this is not check you can switch between 2D and split-screen via VR settings.
How I made it run:

Clone GVR repo from here
Open downloaded repo with Unity
In your BuildSettings switch to Android and check Development Build
Open HelloVR.unity scene
enable VR Mode Only in GvrEventSystem
Under DemoInputManager I set GVR Emulated Platform Type to Cardboard
Under GvrControllerMain set Emulator Connection Mode to USB
Enable developer options on your phone
Connect your phone and check connection via AndroidStudio or Android Device Manager - if your phone is not listed here, something went wrong and you have to install additional drivers for your phone
Hit run: screen should be displayed in split-screen mode on your phone

And again: if your phone displays the scene, but no splitscreen or gyro movement is detected your phone is probably not supported. You can check for errors using the Android Device Manager
"
"
With the Unity 5.1 and Oculus SDK 0.6 and the new built-in Virtual Reality Supported check-box, I cannot seem to change the camera's position in VR.
Of course I can change the camera's Tranform Position component numbers (i.e. x and y and z) but when I run in play mode, it becomes obvious that Oculus' seeing camera has not moved.
How do I move/change the Oculus' perspective, to adjust how the Unity world is seen through the 2 lenses?
","
Add a parent GameObject to the camera and move that one, don't modify the VR camera.
"
"
I'm developing a game with THREEjs and webvr-boilerplate. I'm struggling a bit with how to properly render a HUD (score, distance, powerups etc) that always stays at the top of the scene. I've tried to have a plane (with a texture that's brought in from a hidden canvas element) but positioning it in space proves difficult since I can't match the right depth.
Any clues please? :)
","
Well, you shouldn't have a classic HUD, VR doesn't work like that. 
You're searching for something called diegetic or spatial UI - that is the scores and other icons are rendered as geometry in scene space in a fixed position or distance (this one is called spatial UI). For best results, draw the information on some game object mimicking real displays, for example a fuel gauge on the dashboard of a car or visible remaining bullets on a gun (this one is called diegetic UI).
Unity has made a nice page describing these concepts.
"
"
I've been trying to project a 360 video inside a Sphere with flipped normals for Google Cardboard VR. The Video works fine, only it is inverted horizontally, which is noticeable only when there is some text on the screen. I've included a screen shot of video and a UI.Text element in front of it to compare it with. 

I've tried to invert the view of the camera through projectionMatrix but then it just ends up in blank space. Screenshot : 
I can't figure out a way to make the video project the right way. Please help!
","
Flipping the normals on a sphere is insufficient, you also need to reverse the U part of the UV coordinates (that is, change all the values U such that they are 1-U).  A sphere is set up so that the outside renders text correctly from right to left. When you flip the normals ""right"" is still on the right from the outside...meaning that it's on the left when viewed from the inside.
You will either need to manually edit the UV coordinates yourself or get a premade inverted sphere off the asset store (IIRC there are two that are available for free).
"
"
I upgraded my new Google Cardboard Unity game to the newer SDK, but now the head tracking is inversed:
The camera is upside down. Even the demo scene is the same way.
Has anyone been having the same issue? Thanks in advance!
","
It's a bug in v0.4.10.  You can work around it in a few different ways:

Turn off Native Distortion Correction
Go back to v0.4.9
Use Unity 5

"
"
Essentially, I'm trying to move an object between two ""hands"" that are currently fully functional. I can pick objects up and manipulate them both within the room space and rotationally.
However, the issue I'm having is transferring the object between the controllers.
I can hand over the object to the other hand, however if I let go of the trigger on the original hand, the object then becomes Non-Kinematic and drops back to the floor.
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using UnityEngine.XR;

public class HandGrab : MonoBehaviour
{
    // Variable for Controller button register
    public string buttonName;

    // Allows objects to be transferred between hands
    public HandGrab secondaryHandRef;

    // Determines the referenced object type
    public XRNode nodeType;

    // This affects the distance of object grab from controller
    public Vector3 objectGrabOffset;

    // Default grab distance relative to the controller, (Where the collider sphere is) This can be changed to affect the visuals and where the items get manipulated from
    public float grabDistance = 0.1f;

    // Force at which an object is thrown
    public float throwingMultiplier = 1.5f;

    // To determine if object has been grabbed or cant be grabbed
    public string grabName = ""Grabbing"";

    // Stores the values of the currently grabbed object
    public Transform CurrentGrabObject
    {
        get { return _currentlyHeldObject; }
        set { _currentlyHeldObject = value; }
    }

    // Does as it says and stores the last frame and is used to work out velocity calculations
    private Vector3 _lastFramePosition;

    // Determines the transform values for the currently selected objects
    private Transform _currentlyHeldObject;

    // bolean to determine if an object is currently being held
    private bool _isHeld;

    // *Hopefully gets the roation of the controllers and matches that to the roation of the item*
    // *private Transform _objectRotation;*

    // Use this for initialization
    void Start()
    {
        // Clearing all variables back to zero
        _currentlyHeldObject = null;
        _lastFramePosition = transform.position;
        _isHeld = false;

        // Determining the tracking space available
        XRDevice.SetTrackingSpaceType(TrackingSpaceType.RoomScale);
    }

    // Update is called once per frame

    void Update()
    {
        // rotation and position tracking. Updates every frame, should be 90fps

        transform.localPosition = InputTracking.GetLocalPosition(nodeType);
        transform.localRotation = InputTracking.GetLocalRotation(nodeType);

        // Check the object in hand variable. If hand is empty, check to see if we can pick anything up (Object highlight posibilites)

        if (_currentlyHeldObject == null)
        {
            // Check to see if there are any colliders in the proximity of hands

            Collider[] colliders = Physics.OverlapSphere(transform.position, grabDistance);
            if (colliders.Length > 0)
            {
                // If there is a collision, then pickup the object if we press the grab button and give it the tag ""Grabbing""

                if (Input.GetAxis(buttonName) >= 0.5F && colliders[0].transform.CompareTag(grabName))
                {
                    // Now we set to grabbing to true

                    if (_isHeld)
                    {
                        return;
                    }
                    _isHeld = true;

                    // Sets the current object to the object we have just picked up and binds it as a child object (this should allow all rotation to mimic the hands)

                    colliders[0].transform.SetParent(transform);

                    // If there is no RigidBody attached to the current object, then assign it a RigidBody variable

                    if (colliders[0].GetComponent<Rigidbody>() == null)
                    {
                        colliders[0].gameObject.AddComponent<Rigidbody>();
                    }

                    // Disable all physics on the grabbed object until it is released (May need to change if we want to enable objects to be swung and hit other ""live objects"")

                    colliders[0].GetComponent<Rigidbody>().isKinematic = true;

                    // Save the current object for a reference later

                    _currentlyHeldObject = colliders[0].transform;

                    // If other hand grabs the object, then release the object to be able to be re-parented above

                    if (secondaryHandRef.CurrentGrabObject != null)
                    {
                        secondaryHandRef.CurrentGrabObject = null;
                    }
                }
            }
        }
        else
        // We must now have an object in our hand at this point. We now update its current position to the current hand poistion + the offset definded above

        {
            //If we release the grab button, we drop the object here
            if (Input.GetAxis(buttonName) < 0.1f)
            {

                // Return the value of the object back to Non-Kinematic and re-enable physics
                Rigidbody _objectRB = _currentlyHeldObject.GetComponent<Rigidbody>();
                _objectRB.isKinematic = false;
                _objectRB.collisionDetectionMode = CollisionDetectionMode.Continuous;

                // Fun stuff starts here, we now calculate the hands current velocity
                Vector3 velocity = (transform.position - _lastFramePosition) / Time.deltaTime;

                // We now set that object velocity to the current velocity of the hand (I may adjust this to improve physics simulation)
                _objectRB.velocity = velocity * throwingMultiplier;

                // Kill the reference and revert the hands back to being empty and remove its parent
                _currentlyHeldObject.SetParent(null);
                _currentlyHeldObject = null;
            }
        }

        //Release the grab function
        if (Input.GetAxis(buttonName) < 0.5f && _isHeld)
        {
            _isHeld = false;
        }

        // Store the current position of the hand for velocity calculation in the upcoming frame
        _lastFramePosition = transform.position;

        //Tying off all loose ends here
    }
}

","
In your update function:
if (_currentlyHeldObject != null && !_currentlyHeldObject.GetComponent<Rigidbody>().isKinematic) {
    _currentlyHeldObject.GetComponent<Rigidbody>().isKinematic = true;
}

"
"
I am using https://cdn.rawgit.com/donmccurdy/aframe-extras/v3.8.6/dist/aframe-extras.min.js aframe extra version 3.8.6 with aframe version 0.6.0 , on mouse clicked cursor is disappearing, need to pressed esc key everytime to get cursor back.
can someone please help me into this issue.
","
It's intentional, so you won't have to move around constantly clicking around.
You can disable it quite easily by switching
look-controls=""pointerLockEnabled: true""

to
look-controls=""pointerLockEnabled: false""

I see your version also has the pointerLockEnabled property.

Nonetheless, i would recommend switching to the newest a-frame build + a-frame extras, since there was a ton of polishing and bug-fixes.

Glitch with pointerLockEnabled: true 
Glitch with pointerLockEnabled: false 
source here.
"
"
i have an 360 degree cinemapicture and i try to make that when you click on something the picture blend from one light cinemapicture to an dark cinemapicture so it looks like the light goes off. Some idea how i code that in a js-function? 
     <a-scene>
          <a-assets>
            <img id=""cinema"" src=""cinema360degree.png"">
            <img id=""cinema_dark"" src=""cinema360degree_dark.png"">
            <img id=""button"" src=""playbutton.png"">
          </a-assets>
          <a-image id=""playit"" src=""#button"" width=""4"" height=""4"" position=""-28 5 15"" rotation=""0 90 0"" onclick=""startmovie()""></a-image>
          <a-sky src=""#cinema""></a-sky>
    <script type=""text/javascript"">
    function startmovie() {
     //blend cinema into cinemadark
     // i got this but it only change it. Its not blending:
 //document.getElementById('skyid').setAttribute('src', '#cinema_dark')
    }
    </script>
        </a-scene>

","
Id make two spheres, one with the ""day"" texture, the other with the ""night"" texture:
<a-sphere id=""day"" radius=""100"" material=""side: back""></a-sphere>
<a-sphere id=""night"" radius=""101"" material=""side: back""></a-sphere>

And use the animation component, changing the day's opacity to 0:
Js:
AFRAME.registerComponent(""foo"", {
  init: function() {
    document.querySelector(""#day"").addEventListener(""click"", (e) => {
      this.el.emit(""fadeout"")
    })
  }
})

Html:
<a-sphere id=""one"" foo animation=""property: material.opacity; to: 0; 
delay: 500; startEvents: fadeout;""></a-sphere>


Check it out here.
"
"
I am using A-Frame for VR and I am looking for a way to pin a second entity on the top of all other A-Frame entities like the black diamond in the image below:

I have 2 main challenges:

place automatically the diamond: I am able to place it relatively to its parent as a child but I need to manage myself the size of the parent to be sure the child is not inside the parent
rotated the diamong in order to get it perpendicular to the user view at every moment.

To be honest, I would like to recreate a indicator visible on top of each entity on the screen like in watch dog 2 for example (in the example the diamond is white on the top of the car). 

I don't see any easy to do it with A-Frame and I would like to know if I need to code it by myself or if there is something that could help me.
Thank you in advance
","
I'll try to focus on the so called main challenges:
2) Rotating the indicator. You can use Ngo Kevin's look-at component, which rotates an entity to face any given entity.
Example:
  <a-box position=""-1 0.5 -3"" look-at=""#player"" color=""#4CC3D9""></a-box>
  <a-camera id=""player"" position=""0 1.6 0""></a-camera>

Live fiddle here.

1) Placing above the entity. I'd use bounding boxes to determine the height. Then create your diamond, place the it about 0.2 above the objects height. and set any other attributes (material, src, text)
Example:
AFRAME.registerComponent(""foo"", {
 init: function() {
    var object = this.el.getObject3D('mesh');
    var bbox = new THREE.Box3().setFromObject(object);
    var position = {x:0, y: bbox.max.y + 0.2, z:0}
    var diamond = document.createElement(""a-box"")
    diamond.setAttribute(""color"", ""green"")
    diamond.setAttribute(""width"", ""0.1"")
    diamond.setAttribute(""height"", ""0.1"")
    diamond.setAttribute(""depth"", ""0.1"")
    diamond.setAttribute(""position"", position)
    diamond.setAttribute(""look-at"", ""#player"")
    this.el.appendChild(diamond)
  }
})


<a-box foo></a-box>

live fiddle here
"
"
I am looking to create a macOS 10.13 application that tests for virtual reality support. What would be the best way to test a Mac for VR support, considering the CPU, GPU, and connectivity requirements?
Also, given a MTLDevice, is there  a way to check for VR support using the Metal API?
I have tried to check the default system Metal device for macOS GPUFamily1_v3 support, but that does not completely answer the question of whether a device supports VR on macOS. The code below is what I use to test support for the Metal feature set.
let defaultDevice = MTLCreateSystemDefaultDevice()
    print(defaultDevice?.supportsFeatureSet(.macOS_GPUFamily1_v3))

","
There is no such thing as ""Metal VR Support"". There is no special capability or GPU-level feature required to render for VR. Furthermore, there is no such thing as a ""spec good enough for VR"" as it relies completely on the resolution and frame rate of the particular headset used, as well as your application.
You can query the IOService layer to get GPU model and specs, but you will have to extrapolate the capabilities for yourself based on your personal requirements.
"
"
I need help with deployment of IBM Watson voice recognition on mobile from unity3D.
I'm working on a VR app for the Google cardboard. The idea is to cause events in the app to trigger based on voice commands. For instance, if I say ""Hello"", that word should trigger an action in the app.
Initially, it worked perfectly on HTC Vive and Oculus Rift. 
Now, I ported the same speech recognition code to mobile, and it has refused to work. 
What to do? I decided to use IBM Watson STT(Speech To Text) and Conversation service to achieve the result I wanted using Watson SDK for Unity and IBM Speech Sandbox. It finally began to work on play mode in unity editor. I.e the app works perfectly, recognizing my voice or any other voice input and triggering the relative in-app action.
The issue now is this, once I build to an Android phone, connect to the internet (internet connection required for Watson to send data between my app and the cloud), turn on microphone option from app permission in app settings, it doesn't recognize my voice input or that of any other person. But in the editor, it works fine. I've tried all I could, done all the debugging I know, voice recognition is still not working on Cardboard.
Thanks for your time.
","
Android builds are not able to connect to TLS 1.2 endpoints. Please change your endpoints to 
conversation
https://gateway-tls10.watsonplatform.net/conversation/api

speech to text
https://stream-tls10.watsonplatform.net/speech-to-text/api

"
"
I am currently using the OSVR HMD and I'm trying to figure out how to use the IR tracker that comes with the VR headset. In this article, it is suggested to use the osvr_server_config.VideoBasedHMDTracker.sample.json configuration file but it seems that it as been removed from the osvr core snapshot.
What is the current configuration file to enable the IR tracker ?
","
Late reply, but:
With the latest core (at least), all that's needed is to launch osvr_server.exe.
I had some connection problems with mine and actually had to cut away some plastic around the usb-port.
"
"
Any tips how to make AR with some big objects? Like building, car or kitchen?
I already used it Vuforia. But Vuforia is only for small object. 
","
I guess you're looking for something like ARCore for Android or ARKit for iOS. 
"
"
I am trying to install Cardboard SDK on Ubuntu 14.04 and it say it needs ""Unity"", but the download of ""Unity"" seems to be only for Windows and Mac. Anyone has tried this on Ubuntu?
","
Unity3d has already released a beta Linux version.
It is not perfect, but stable enough. If you want to develop a cardboard app on Ubuntu, you need it.       
"
"
Is possible to add fade in or fade out animations on a sky in aframe?
How can I add animations when the user hovers their cursor over the ball?
In this example when you hover your mouse, the background will change but w/o animations.


AFRAME.registerComponent('set-sky', {
  schema: {
    default: ''
  },
  init() {
    const sky = document.querySelector('a-sky');
    this.el.addEventListener('click', () => {
      sky.setAttribute('src', this.data);
    });
  }
});
<script src=""https://rawgit.com/aframevr/aframe/b813db0614ac2b518e547105e810b5b6eccfe1c8/dist/aframe.min.js""></script>
<a-scene>
  <a-camera position=""0 2 4"">
    <a-cursor color=""#4CC3D9"" fuse=""true"" timeout=""10""></a-cursor>
  </a-camera>

  <a-sphere color=""#F44336"" radius=""1"" position=""-4 2 0"" set-sky=""https://c3.staticflickr.com/2/1475/26239222850_cabde81c39_k.jpg""></a-sphere>

  <a-sphere color=""#FFEB3B"" radius=""1"" position=""4 2 0"" set-sky=""https://c2.staticflickr.com/2/1688/25044226823_53c979f8a1_k.jpg""></a-sphere>

  <a-sky></a-sky>
</a-scene>



","
I wouldn't use CSS as demonstrated in the answer above. A-Frame generally doesn't use CSS, but relies on its own display components and properties.
You can fade by animating the material.opacity of the <a-sky> primitive. This can be done either by using the A-Frame core <a-animation>, or the aframe-animation-component, which seems to be becoming the standard.
If you use the aframe-animation-component, which I recommend, you could set up your <a-sky> like this:
 <a-sky src=""#sky-1""
     animation__fadein=""startEvents: fadein; property: material.opacity; from: 0; to: 1; dur: 1000;""
     animation__fadeout=""startEvents: fadeout; property: material.opacity; from: 1; to: 0; dur: 1000;""></a-sky>

Then within your custom component, you would trigger the startEvents for each animation using emit (e.g., sky.emit('fadein')).
I forked your code and made a few changes:

Updated A-Frame version to the official release
Uploaded sky images to a CDN to avoid CORS issues
Defined aforementioned images as assets for better loading
Set a default image for <a-sky>
Added the aframe-animation-component
Converted ES6 features such as const and arrow functions to var and function, respectively, for better compatibility (to rule out any misbehaving)
Used setTimeout in your custom set-sky component to time the animations. Although, you may want to depend on events instead, which can get more complicated with multiple listeners. I just wanted to give you a rudimentary example. 

Here is the modified demo: https://codepen.io/dansinni/pen/gzbpWy
There's actually also an example right on the A-Frame site that does pretty much what you're looking to do: https://aframe.io/examples/showcase/360-image-gallery/
"
"
The question is already quite direct and short: 

Can the Hololens be used as a virtual reality glasses?

Sorry beforehand if the question is clear for those who have tried them out, but I had not yet the chance. 
From what I read I know that they have been designed to be a very good augmented reality tool. This approach is clear for everybody.
Just thinking that may be applications where you simply don't want the user to have any spatial contact with the reality for some moments, or others where you want the user to forget in the complete experience about were s-he is, then a complete environment should be shown as we are used to with the virtual reality glasses.
How are the Hololens ready for this? I think there are two key sub-questions that may be answered for this:
How solid are the holograms?
Does the screen where holograms can be placed covers the complete view?
","
1) The holograms quality is defined by the following specs: 
 - Holographic Resolution: 2.3M total light points 
 - Holographic Density: 2.5k light points per radian 
It is worth to say that Microsoft holograms disappear under a certain distance indicated here in 0.85m
Personal note: in the past I worked also on Google Project Tango and I can tell you from these personal experiences that the stability of Microsoft holograms is absolutely superior. Also, the holograms are kept once the device is turned off, so if you place something and you reboot the device you will find them again where you left them, without the need to restart from scratch
2) Absolutely not: ""[The field of view] amounts to the size of a monitor in front of you – equivalent to 15 inches"" as stated here. And it will not be addressed as reported also here. So if the holograms size exceeds this space they will be shown partially [i.e. cut]. Moreover the surrounding environment is always visible because the device purpose is interacting with the real environment adding another layer on top
"
"
To be totally clear. This question is about SteamVR dashboard overlays specifically, not regular overlays.
I have been playing around with the C++ SteamVR SDK and working on some overlay application prototypes lately. Something I have not managed to do so far is to get a dashboard overlay to show up. The error I get when I call CreateDashboardOverlay is VROverlayError_PermissionDenied. I'm guessing that this is because I need to be authenticated with a SteamVR developer account, which I don't currently have. Can anyone verify that? There doesn't seem to be any (public) documentation on this at all beyond what's in openvr.h and the openvr github docs page, which is somewhat sparse.
I'm also guessing that any dashboard overlay application would need to be distributed through the official Steam store, but again I can't find anything official on that. I suspect that Valve would require this since otherwise any old malware that happens to be running on the system could easily create an official-looking dashboard overlay.
Note again that I am referring specifically to dashboard overlays. I can get regular overlays to show up just fine.
There are a few repos on github with implementations of steamvr overlays (https://github.com/Hotrian/OpenVRDesktopDisplayPortal for example), but I have yet to find one that is actually creating a dashboard overlay.
Any info or links to documentation I'm somehow missing would be greatly appreciated. I'm starting to think I might be missing something obvious.
Thanks
Edit for clarity:
My questions are: Am I getting the permission denied error when calling CreateDashboardOverlay because I need to satisfy some kind of authentication requirement such as having a steam dev account? And do SteamVR dashboard overlay apps need to be distributed via an official channel?
","
On further review it appears I was misinterpreting my own debug output and reading a bit too much into it because the conclusions sort of made sense in my mind.
The CreateDashboardOverlay call was working fine. Later on in my code I was calling ShowOverlay, which of course is not allowed for dashboard overlays (They are shown by opening them via the SteamVR dashboard UI).
My dashboard overlay is working fine after all.
To summarize, the answer to both of my questions is no. No Steam developer status is needed to create a dashboard overlay and SteamVR dashboard overlay apps do not need to be distributed through any kind of official channel.
"
"
What does a VR Ready computer mean technically? 
Does an industry standard exist that specifies a VR Ready computer has to be able to render X number of triangles twice (left and right eye) at a frame rate F at a resolution R, where X, F and R are defined some industry standard?
","
Oculus has quite strict guidelines on what are the minimal requirements. It will not work or at least will constantly nag you if your machine isn't up to spec.
Vive has minimum specs, but they're not so orthodox in enforcing them as Oculus is.
OSVR has no such thing as far as I know - and it works almost everywhere.
That said, the stereoscopic display and rotation measurement actually isn't that costly in terms of computing power¹, the specs above are arbitrary and provided as a baseline for content makers. For example VR video can work on really old machines, provided that middleware (Oculus drivers mostly) doesn't forbid you to.
¹ positional tracking is moderately expensive in case of camera based trackers like Oculus or OSVR. Still, it mostly eats up USB bandwidth and not that much of the CPU. Vive has no such issue.
"
"
Does any know or can possibly point me to some instructions or a github repository on how I can create a script where I have an object and in GoogleVr (Cardboard) if I was to gaze over an object, a tooltip would appear? 

If anyone is familiar, in the Cardboard Demos under Under Arctic Journey > Learn, when you click on the fox, a tooltip appears to showcase that item along with like a brief description on it. I want to have something similar (maybe even the same thing) except just having a gaze over will automatically show it. Is this possible? 
I want to have this done on multiple objects in my project so I want it created so I can easily substitute out text and whatnot. 
","
Have a script with a reference to a World Space Canvas (WSC). The WSC will be your tooltip and be activated when you hover over the object and disabled when you don't.
You can set images and texts of the WSC through the inspector or through code if you make a reference to them.
The script should also always have its rotation set to face the player.
You can use the SetActive(bool) method to show or hide the WSC.

The UI system makes it easy to create UI that is positioned in the world among other 2D or 3D objects in the Scene.
Start by creating a UI element (such as an Image) if you don’t already have one in your scene by using GameObject > UI > Image. This will also create a Canvas for you.
  Set the Canvas to World Space
Select your Canvas and change the Render Mode to World Space.
Now your Canvas is already positioned in the World and can be seen by all cameras if they are pointed at it, but it is probably huge compared to other objects in your Scene. We’ll get back to that.

https://docs.unity3d.com/Manual/HOWTO-UIWorldSpace.html
"
"
Is there a way to fix the view of Google Web VR View so it doesn't go up and down. Currently I have an image with a floor inside a room. When you view it with an iPhone, it automatically points to the floor if you hold it normally in your hands and you would have no idea there was anything to look at above. No one would lift their phone and go ""Ahhhhhhh""
Any ideas?
","
Sounds like you want the is_yaw_only option? https://developers.google.com/vr/develop/web/vrview-web#vr_view
"
"
i need to know how i can add entity on position cursor using aFrame
on click i want to add entity far from camera 2 meter away or something like that 
or i need to know how can i get position on click event (x,y,z) 
but the problem here i what to add entity on 360 photo and i using a-sky tag and i tried to get position using js but i Failed it's working on entity only


this.el.addEventListener('click', function (evt) {
		 console.log('I was clicked at: ', evt.detail.intersection.point);



for Example see this photo 
","
The sky is very far away so the raycaster won't reach (default far value is 1000`).
You could do something like:
<a-camera>
  <a-entity id=""clickPosition"" position=""0 0 -100""></a-entity>
</a-camera>

And then you can get the position to add entity via:
document.getElementById('clickPosition').getWorldPosition();

"
"
What I want is a world space canvas that I can scroll using hand movements. I can get the worldPosition of the controllers and find the actual movement but I cannot figure out how to enable the text to scroll.
What I am trying at the moment is to use a scrollrect+ a hidden scrollbar and change the value of the scrollbar but this doesn't work.
I have also tried changing the world position of the textobject and while I managed to make the text to move accordingly I cannot figure out how to find the text size, but I think this is like reinventing the scrollrect.
","
For anyone checking this now, I ld propose to use the OVR Input module with its scroller components provided in the oculus framework. I 'm leaving my old answer below.

Old Answer, if you dont want to use OVRInput Module
After some more search I stumbled upon an answer in the unity forums,
I m posting it here for future reference.
Here you just scroll with the assigned axis as setup in the project settings, if you want to scroll with hand movements just calculate the difference between controller position, for oculus touch I could get mine through changing a bit their code from here https://developer.oculus.com/blog/adding-gear-vr-controller-support-to-the-unity-vr-samples/
public class MoveScrollRect : ScrollRect, IMoveHandler, IPointerClickHandler
{
private const float speedMultiplier = 0.01f;
public float xSpeed = 0;
public float ySpeed = 0;
private float hPos, vPos;


void IMoveHandler.OnMove(AxisEventData e)
{
    xSpeed += e.moveVector.x * (Mathf.Abs(xSpeed) + 0.1f);
    ySpeed += e.moveVector.y * (Mathf.Abs(ySpeed) + 0.1f);
}

void Update()
{

    ySpeed = Input.GetAxis(""VerticalScroller"");

    hPos = horizontalNormalizedPosition + xSpeed * speedMultiplier;
    vPos = verticalNormalizedPosition + ySpeed * speedMultiplier;

    xSpeed = Mathf.Lerp(xSpeed, 0, 0.1f);
    ySpeed = Mathf.Lerp(ySpeed, 0, 0.1f);

    if (movementType == MovementType.Clamped)
    {
        hPos = Mathf.Clamp01(hPos);
        vPos = Mathf.Clamp01(vPos);
    }

    normalizedPosition = new Vector2(hPos, vPos);
}

public void OnPointerClick(PointerEventData e)
{
    EventSystem.current.SetSelectedGameObject(gameObject);
}

public override void OnBeginDrag(PointerEventData eventData)
{
    EventSystem.current.SetSelectedGameObject(gameObject);
    base.OnBeginDrag(eventData);
}
}

"
"
I'm attempting to map a video as texture to a primitive cylinder for a VR project by using Scenekit: an SKVideoNode embedded in an SKScene as a texture for a SceneKit SCNTube object, and I just can't get video to display as a still image would. PLayground code below should generate moving video mapped to cylinder, but the mapping does not work:
EDIT: ADDED SINGLE LINE AT END OF LISTING TO FIX. CODE BELOW SHOULD WORK
import UIKit
import SceneKit      // for 3D mapping
import SpriteKit     // for SKVideoNode
import QuartzCore    // for basic animation
import XCPlayground  // for live preview
import AVFoundation  // for video playback engine

// create a scene view with an empty scene
var sceneView = SCNView(frame: CGRect(x: 0, y: 0, width: 300, height: 300))
var scene = SCNScene()
sceneView.scene = scene


// start a live preview of that view
XCPShowView(""The Scene View"", view: sceneView)

// default lighting
sceneView.autoenablesDefaultLighting = true

// a geometry object
var tube = SCNTube(innerRadius: 1.99, outerRadius: 2, height: 3)
var tubeNode = SCNNode(geometry: tube)
scene.rootNode.addChildNode(tubeNode)

// video scene


let urlStr = NSBundle.mainBundle().pathForResource(""sample"", ofType: ""mp4"")
let url = NSURL(fileURLWithPath: urlStr!)

let asset = AVURLAsset(URL: url, options: nil)
let playerItem = AVPlayerItem(asset: asset)
let player = AVPlayer(playerItem: playerItem)
let videoNode = SKVideoNode(AVPlayer: player)
let spritescene = SKScene(size: CGSize(width: 1211, height: 431))



videoNode.size.width=spritescene.size.width
videoNode.size.height=spritescene.size.height
spritescene.addChild(videoNode)


// configure the geometry object

var myImage = UIImage.init(named: ""BandImage.jpeg"")

tube.firstMaterial?.diffuse.contents  = spritescene


// set a rotation axis (no angle) to be able to
// use a nicer keypath below and avoid needing
// to wrap it in an NSValue
tubeNode.rotation = SCNVector4(x: 0.0, y: 1.0, z: 0.0, w: 0.0)

// animate the rotation of the torus
var spin = CABasicAnimation(keyPath: ""rotation.w"") // only animate the angle
spin.toValue = 2.0*M_PI
spin.duration = 3
spin.repeatCount = HUGE // for infinity
tubeNode.addAnimation(spin, forKey: ""spin around"")

// starts the video, solving the issue
    sceneView.playing = true

","
I've posted my code (and some sample panoramic content) on github for anyone who wants to have a working sample code or is interested in collaborating on an opensource panoramic video player:
https://github.com/jglasse/OSVR 
As it turns out, it looks like simulator (and playgrounds) doesn't support this feature. Moving the above code to a project and running on a device, I finally have it working. 
So the moral of the story is - if you're using SKVideoNodes as textures for Scenekit, use an actual device for testing. 
"
"
I have been working on this VR website for my project for sometime. It used to be able to allow user to view left/right side of the virtual world by moving your phone to the left/right. However, just recently, it suddenly did not work for android phone. I have tested on iphone/ipad, still works as per normal. 
But for some reason, i tried using note8 , s7 it doesnt work anymore. 
Would really appreciate if you can identify whats the problem, and how can I fix it? Thank you very much.
","
Try using firefox on android, or the built-in samsung internet app.
If everything works as expected, it means you've stumbled on a bug in the recent chrome build, described here, which should be fixed in the next build.
Also using the latest a-frame master build should solve the issue for now.
"
"
I'm Vincent, a French student. For a school project, my group and I are looking to make a 360 app on iOS. The idea is that the user will use the app with a cardboard, and live a 360 experience. Noone in my group never did this, so here I am, to get so good advices on which techs should we use to make our project live.
https://i.sstatic.net/GL38c.jpg (Illustration of the user using the app)
We find out that ArKit and Unity using the Google SDK VR are the two main techs to make what we want to, but the thing is that we're a little scared about choosing the wrong one. ArKit seems fine, but this is in 3D. We're more looking to project 2D image around the image and he can move the head to navigate between them. Unity, on its side, seems too much, and I not sure that the scripting / animating render well.
So what do you guys think ? ArKit ? Unity ? Maybe another thing ? I surely forget something, so please ask for more details :). Thanks !
","
You only need Unity+GoogleVR plugin for that,
ARKit/Vuforia is not needed, since you are not doing Augmented Reality.
For the panoramic image:
Use Equirectangular images, can assign it as a cubemap in unity,
or use shader Skybox/Panoramic (which support 360 or 180 degree image types)
Moving between other images:
GoogleVR has example scene on how to click objects,
then its just a matter of loading new panorama image there.
You can also play 360 videos on that panorama sphere,
using unity video player or some of the video player from asset store (they can handle youtube url parsing for you)
"
"
I'm trying to create a simple native app similiar to Passthrough app, I'm using the Passthrough.java class, but when I call the startCameraPreview method nothing happens. 
My objective is to activate the rear camera of Galaxy Note 4 and show the preview using the Gear VR.
The camera preview seems to have started, there's no log errors, but all that I can get is a black screen and a recurring message in logcat ""W/QCamera2HWI: processMetaCallback: lls not enabled, no ops here"". 
Someone knows the meaning of that message?
EDIT:
I've found a useful framework created by Samsung, http://www.gearvrf.org/ through this I've got the camera working. 
","
I've found a useful framework created by Samsung, http://www.gearvrf.org/ through this I've got the camera working.
Inside the Gear VR Framework there are several samples, one of those it's called scene-objects. I take inspiration using that sample, anyway the flow is simple: Declare your android Camera object and pass it to a GVRCameraSceneObject 
"
"
I am using player prefab from SteamVR Plugin. Whenever player touches an object with a Collider on it, camera sees through it. I am using 0.01 as minimum clipping value. I have also added Rigidbody and Collider on camera.
","
I also do not have code currently but here’s a concept of how to approach the problem:

Keep a trigger collider on the camera.
Write a script that detects when that collider is within another collider (your walls). There is a method for checking if colliders are intersecting/overlapping
If there is an intersect/overlap, fade the camera to black. If there is not, fade back.

"
"
This builds upon a question I recently asked here:
Unity3D Leap Motion - Put hand in static pose (SetTransform causes Hand to spin crazy)
However, the code suggested in the only response given does not work. I have been using the 'SetTransform' method and while it does allow me to move the hand to my desired position, the rotation is crazy. The hand continuously spins and despite spending the best part of four days on it, I can't find the solution.
In short all I am trying to do it set the hand to a fixed pose (a fist for example) but have it move and rotate with the live hand data. I created a method (detailed in previous question) that would manually recalculate the positions of the joints to put the hand in the pose as the SetTransform was causing this crazy rotation. However, I still ended up with crazy rotation from having to transform the hand to rotate it, so I have switched back to the SetTransform method for ease.
posedHand = PoseManager.LoadHandPose(mhd.LeftHand, int.Parse(""2""), transform);
Hand h = new Hand();
if(posedHand != null)
{
    h.CopyFrom(posedHand);                               
    h.SetTransform(LiveData.LeftHand.PalmPosition.ToVector3(), LiveData.LeftHand.Rotation.ToQuaternion());
}

All I really want is a method that I can pass two hand objects into (one being the current 'live' hand, the other being the desired pose) and get a hand object returned which I can then render.
Update
As requested here are images of what I am currently getting and what I want to achieve.
Current:

Target:

The target image would display the fixed pose, which in this example is a fist at the current position and rotation of the live hand. Thus meaning I could have my hand 'open' but onscreen I would see a fist moving around. As you can see from the 'current' the SetTransform is giving me the correct pose and position but the rotation is freaking out.
","
This question has been answere elsewhere. Unity3D Leap Motion - Put hand in static pose (Pose done just can't rotate) 
Essentially, i had to create a function that took in the pose hand as a parameter which then used Hands.GetHand(Chiralty) to get the position and rotation from the live data which was then used to 'SetTransform' the posed hand to the new position and rotation
public static Hand TransformHandToLivePosition(Chirality handType, Hand poseHand)
    {
        Hand sourceHand = Hands.Get(handType);
        Hand mimicHand = null;

        if (poseHand == null && sourceHand != null && sourceHand.Fingers.Count > 0)
        {
            //poseHand = PoseManager.LoadHandPose(sourceHand, 2, this.transform, true);
        }

        if (poseHand != null && sourceHand != null)
        {
            // Copy data from the tracked hand into the mimic hand.
            if (mimicHand == null) { mimicHand = new Hand(); }

            mimicHand.CopyFrom(poseHand); //copy the stored pose in the mimic hand
            mimicHand.Arm.CopyFrom(poseHand.Arm); // copy the stored pose's arm into the mimic hand

            // Use the rotation from the live data
            var handRotation = sourceHand.Rotation.ToQuaternion();

            // Transform the copied hand so that it's centered on the current hands position and matches it's rotation.
            mimicHand.SetTransform(sourceHand.PalmPosition.ToVector3(), handRotation);
        }
        return mimicHand;
    }

"
"
I want in an aframe game continuous moving to the viewport of the camera. I used the code from here: 
In my desktop browser (not vr mode) it works pretty well, but when I am switching to the vr-mode the moving doesn't work... 
Here is an exampe of what i have:
    <!DOCTYPE html>
<html>

<head>
    <meta charset=""utf-8"">
    <title>360° Image Browser</title>
    <script src=""https://aframe.io/releases/0.7.1/aframe.min.js""></script>
    <script src=""https://unpkg.com/[email protected]/dist/aframe-extras.ocean.min.js""></script>


</head>

<body>
    <a-scene>
        <!-- sky + ocean -->
        <a-sky radius=""100"" src=""http://2.bp.blogspot.com/-ELqPrSgPbGU/Vcnw54n7Q1I/AAAAAAAAGdk/rcfkvjlMNqI/s1600/PANO_360Test09.jpg""
            position=""0 -6 0 ""></a-sky>
        <a-ocean width=""200"" depth=""200"" density=""200"" position=""0 0 0""></a-ocean>

        <!-- camera + cursor. -->
        <a-camera id=""camera"" position=""0 20 80"" fly wasd-controls-enabled=""false"" look>
            <a-cursor id=""cursor"" color=""black""></a-cursor>
        </a-camera>
    </a-scene>
    <script>
        // document.querySelector(""a-scene"").enterVR();
        AFRAME.registerComponent(""fly"", {
            schema: {
                stepFactor: { type: ""number"", default: 0.1 },
                isFlying: { type: ""boolean"", default: true }
            },
            tick: function () {
                if (this.data.isFlying) {
                    this.el.components.camera.camera.parent.position.add(
                        this.el.components.camera.camera
                            .getWorldDirection()
                            .multiplyScalar(this.data.stepFactor)
                    );
                }
            }
        });
    </script>

</body>

</html>

you can turn on the vr mode in the script by removing the commit
Anyone an idea? 
","
I'm not entirely sure what happens when you switch to VR mode (in regards to the camera element), but trying setAttribute seems to make it work:
https://glitch.com/edit/#!/a-frame-move-camera
tick: function () {
    if (this.data.isFlying) {
        let newP = this.el.components.camera.camera.parent.position.add(this.el.components.camera.camera.getWorldDirection().multiplyScalar(this.data.stepFactor));
        this.el.setAttribute('position', newP)
    }
}

"
"
I have a problem that main camera position doesn't change after CharacterController.SimpleMove() called. The task is to create scene where camera moves.
I have Main Camera game object with Character Controller and Script attached.
The issue is that nothing in vrCamera position changed after SimpleMove() called. 
My question is what is wrong in this code. I suggest something wrong with binding between MainCamera object and CharacterController component, but I have spend a lot of time investigating and nothing working found.

using UnityEngine;

[RequireComponent(typeof(CharacterController))]
public class VRLookWalk : MonoBehaviour {

    public Transform vrCamera;

    public float toggleAngle = 30.0f;
    public float speed = 3.0f;
    public bool moveForwad;
    private CharacterController cc;

  // Use this for initialization
  void Start () {
        cc = vrCamera.GetComponent<CharacterController>();
  }

  // Update is called once per frame
  void Update () {
        if (vrCamera.eulerAngles.x >= toggleAngle && vrCamera.eulerAngles.x < 90.0f)
        {
            Vector3 forward = vrCamera.TransformDirection(Vector3.forward);
            cc.SimpleMove(forward * speed);
        }
    }
}

","
You can't move the VR Camera, it's the SDK that determine the mainCamera position.
In order to move your camera you can just make a new GameObject as a parent of your mainCamera then move the parent GameObject
"
"
I'm developing a lot these days with A-Frame, but more specifically targeting mobile. One thing I noticed is that the touch controls for mobile only allow for horizontal camera rotation movement(left/right). I realize the gyroscope is to be a huge factor in camera rotation movement, but it would be nice to have full control of the camera using touch (horizontal and vertical). The default setting for the camera only allows horizontal rotation movement. I haven't seen were to implement certain properties to the camera entity, or to write some script. Is there a simple boolean property I can set on an entity, or am I completely off base?
","
I didn't see any properties to do this on the built in look-controls component. So the next step would be to create your own version and modify the touchmove function.
I've created a starter project to help you jump in:
https://glitch.com/edit/#!/a-frame-touch-look-controls
Instead of using the native look-controls I've created a mod called touch-look-controls.
Inside the touchmove function I've added a few lines so that it affects both x and y axis:
onTouchMove: function (evt) {
  var canvas = this.el.sceneEl.canvas;
  var deltaX, deltaY;
  var pitchObject = this.pitchObject;
  var yawObject = this.yawObject;

  if (!this.touchStarted || !this.data.touchEnabled) { return; }

  deltaY = 2 * Math.PI * (evt.touches[0].pageX - this.touchStart.x) / canvas.clientWidth;
  deltaX = 2 * Math.PI * (evt.touches[0].pageY - this.touchStart.y) / canvas.clientHeight;

  // Allow touch orientaion to both x and y
  yawObject.rotation.y -= deltaY * 0.5;
  pitchObject.rotation.x -= deltaX * 0.5;
  pitchObject.rotation.x = Math.max(-PI_2, Math.min(PI_2, pitchObject.rotation.x));
  this.touchStart = {
    x: evt.touches[0].pageX,
    y: evt.touches[0].pageY
  };
},

PS: I didn't thoroughly test how this affects VR mode, plus it looks like the gyroscope is set off when using the component (a bug from copy pasting the look-controls) so just keep that in mind.
PPS: you can see the native look-controls code here:
https://github.com/aframevr/aframe/blob/master/src/components/look-controls.js
"
"
I am trying to add a on click handler that will toggle the light attribute on an entity when a separate entity is clicked. However nothing is happening when I click the separate entity, and I have no errors in my console. 
Here is my component:
    AFRAME.registerComponent(""lightsOut"", {
    schema: {
    target : { type : ""selector""}
    },
    init: function(){
        this.el.addEventListener(""click"",function(){
            this.data.target.removeAttribute(""light"");
        }.bind(this));
    }
    })

And here are the two entities:
<a-entity id=""streetLamp""
obj-model=""obj:./models/streetlamp/StreetLamp.obj;mtl:./models/streetlamp/StreetLamp.mtl""
material=""color:black"" 
scale=""0.3 0.6 0.5"" 
position=""-7.138 -1.499 -11.711"" 
lightsOut=""target:#streetLight"">
</a-entity>

<a-entity id=""streetLight"" 
position=""-4.961 6.205 -11.962"" 
rotation=""-67.208 -112.987 87.548"" 
scale=""0.657 0.612 0.718"" 
light=""distance:15;decay:2;intensity:5a;angle:90;color:#ff8000;groundColor:#ffffff;penumbra:0.5;castShadow:true;type:point""
></a-entity>

When I click streetLamp, I would like to remove or hide streetLight. I am using cursor controls. 
","
You can't have a uppercase letter in the component name, a-frame will try to make it lowercase, (...) in the end it won't work. 
Try naming it lightsout or lights_out. Throw in a console.log() when clicked, to be sure it's properly working.

Also You need to refresh the raycaster manually, using raycaster.refreshObjects(), it's a bug, which is fixed by december 2017 in the master build.
"
"
I've been blown away by aframe, and I want to start walking around the scenes I design.
I've paired the cheap bluetooth controller that came with my headset (blitzwolf VR), but that doesn't work with aframe (or google cardboard). A bluetooth keyboard doesn't work either.
What should I be using to test movement on mobile VR? Thanks!
","
I'm going to take a guess here and say that it's possible that you haven't enabled the Gamepad API for your browser, so although your phone is connecting to and responding to your gamepad, your browser can't.  I'm reasonably sure that the latest version of Chrome on mobile, (and Chrome in general), has the Gamepad API disabled by default, so you'll have to chase after it.
Go to chrome://flags in your browser then look for the entry that says ""Gamepad Extensions"" and flip the entry from ""Default"" to ""Enabled"".  From there, you should be good to go.
"
"
Does any one know how to control the camera view with only a single joystick? Currently im using a joystick from the switch.
Heres my current code.
 public class MoveCamera : MonoBehaviour 
{
public float speed = 1.0f;
Camera cameraMovement;

void Update () 
{    

    float xRot = speed * Input.GetAxis(""JoyStickX"");
    float yRot = speed * Input.GetAxis(""JoyStickY"");

    transform.Rotate(xRot, yRot, 0.0f);
}
}

","

In the editor, create an empty game object called ""Pivot""
Make your camera a child of Pivot.
Move the camera away from Pivot, whatever distance you need.
Now attach a script to Pivot. 

Something like this :
 void Update(){
     float speed = 3.0f;
     float xRot = speed * Input.GetAxis(""Vertical"");
     float yRot = speed * Input.GetAxis(""Horizontal"");
     transform.Rotate(xRot, yRot, 0.0f);
  }


Assign your camera to the camera var in the script attached to Pivot
Now you do the messing around of the rotation

"
"
i am working on an a-frame website but i just now discovered that for some reason the camera height goes up by alot when the user presses the VR button (bottom right).
can someone explain why this happens?
i am running the website on
iphone 6
ios 11
firefox
link to the website so you can see for yourself:
https://boerenkool-met-worst.firebaseapp.com/
links to images:
https://ibb.co/cTwJK6
https://ibb.co/nEaCe6
","
Could it be something with the combo of the userHeight property mixed with the position property on the camera? Maybe try to wrap the camera entity and set the userHeight to 0.
Something like this:
<a-entity position=""0 1.6 2"">
    <a-camera id=""camera"" user-height=""0"">
        <a-cursor id=""cursor"">
        </a-cursor>
    </a-camera>
</a-entity>

"
"
i am working on an a-frame website but i just now discovered that for some reason the camera height goes up by alot when the user presses the VR button (bottom right).
can someone explain why this happens?
i am running the website on
iphone 6
ios 11
firefox
link to the website so you can see for yourself:
https://boerenkool-met-worst.firebaseapp.com/
links to images:
https://ibb.co/cTwJK6
https://ibb.co/nEaCe6
","
Could it be something with the combo of the userHeight property mixed with the position property on the camera? Maybe try to wrap the camera entity and set the userHeight to 0.
Something like this:
<a-entity position=""0 1.6 2"">
    <a-camera id=""camera"" user-height=""0"">
        <a-cursor id=""cursor"">
        </a-cursor>
    </a-camera>
</a-entity>

"
"
Anyone else getting this error when they try to build their scene with Photon voice? Seems to be some issue with the opus_egpv.dll file.

I've tried it both in my project, attempting to build my main scene, and in a completely empty project with only Photon Voice, attempting to build their demo voice scene. 
In both cases I'm unable to finish the build unless I delete the opus_egpv.dll file from the x86_64 folder. 
When I do so, I'm able to run the game successfully for the first user. However, the game will crash for the second user as they try to join in. The second user's game crashes by freezing completely, requiring that I force exit it (this does not affect the first user).
I've tried this with both Unity 2017.3.0f3 and Unity 2017.2.0f3 to the same effect. I'm using the [currently] most recent version of Photon Voice, ver. 1.13 (released Nov 22, 2017). I am making a multiplayer VR game.
","

All files under ""Assets\Plugins\x86"" should have platforms ""Standalone"" and ""Editor"" only and CPU: ""X86"". 
All files under ""Assets\Plugins\x86_64"" should have platforms ""Standalone"" and ""Editor"" only and CPU: ""X86_64"".

More information can be found on the answers to your question on Photon engine forum.
"
"
So I'm struggling to find a solution to play/stop/pause sound on ""click"" i.e. when focusing with a black dot with A-Frame sound entity... What I would like to have is a plain, shape or whatever with a play/pause image on it, which would trigger audio when focused. Did anyone encounter something similar perhaps?
<audio id=""sound"" crossorigin=""anonymous"" preload=""auto"" src=""some-audio-file.mp3""></audio>

... would trigger something like sound=""on: click; src: #sound""

","
try making a custom component
AFRAME.registerComponent('audiohandler', {
  init:function() {
     let playing = false;
     let audio = document.querySelector(""#audio"");
     this.el.addEventListener('click', () => {
          if(!playing) {
              audio.play();
           } else {
              audio.pause();
              audio.currentTime = 0;
           }
           playing = !playing;
     });
  }
})

and use it within Your 'button""
<a-box audiohandler> </a-box>

You can check all media methods, properties etc here.
You can check this button here. 
"
"
I am trying to figure out the OpenVR APIs capabilities and its exact role in VR contents development. 
So far i understood that games can be developed using OpenVR in order to be flexible on the kind of hardware used. 
At this point I have a couple of questions that come to my mind: 

if one doesn't use OpenVR to develop a game in (let's say) unity, what else can he/she use? does the Steam plugin for unity use OpenVR?
also, if I develop an OpenVR driver for a new kind of controller for VR games, what kind of games would be compatible with my new controller?

Thanks a lot
","
SteamVR is at this point synonymous with OpenVR. It is the only known server that implements the API. So yes, the SteamVR plugin will use OpenVR.
If you want to develop a VR controller, then I guess it would work in most games as long as you map it with the same axis. For example, here is the mapping that Unity3d uses. You can make a vastly different controller, but don't count on it working with software that was not designed with it in mind.
Remember, that the controller API has not only tracking and axis, but also it's 3d model that can be shown in VR. Quite a few (most?) apps don't use the API provided model, but have their own built in. This means people will most likely still see your controller in VR as the standard Vive wand. Try to model the grip at the same shape so at least that doesn't break immersion.
"
"
I am developing a game for Gear VR in Unity 5.6.1p1 with Oculus Utils 1.9.0. It is on technical review now on developer console. However I keep getting entitlement error even after adding it to the project. 
Here is their explanation:

It appears that your app does not support entitlement checks to prevent unauthorized use of your content. Documentation on how to add entitlement checks can be found here: https://developer.oculus.com/documentation/platform/latest/concepts/pgsg-get-started-with-sdk/

And here is my entitlement code:
public class PlatformManager : MonoBehaviour
{
    public static bool entitled = false;
    private static PlatformManager s_instance;
    private ulong m_myID;

    private string m_myOculusID;

    void Awake()
    {
        if(s_instance != null)
        {
            Destroy(gameObject);
            return;
        }
        s_instance = this;
        DontDestroyOnLoad(gameObject);
        Core.Initialize();
    }

    private void Start()
    {
        entitled = false;
        Entitlements.IsUserEntitledToApplication().OnComplete(IsEntitledCallback);
    }

    void IsEntitledCallback(Message msg)
    {
        if(msg.IsError)
        {
            entitled = false;
            TerminateWithError(msg);
            return;
        }
        entitled = true;
        Users.GetLoggedInUser().OnComplete(GetLoggedInUserCallback);
    }

    public static void TerminateWithError(Message msg)
    {
        Debug.Log(""Error: "" + msg.GetError().Message);
        UnityEngine.Application.Quit();
    }

    void GetLoggedInUserCallback(Message<User> msg)
    {
        if(msg.IsError)
        {
            TerminateWithError(msg);
            return;
        }

        m_myID = msg.Data.ID;
        m_myOculusID = msg.Data.OculusID;
    }
}

I am not doing anything with ID after entitling. Should I do something? Is there a mistake in my code? I do get true value after entitling.
","
My app has passed entitlement check and techical review. The code was good, but I had to add a message to let the user know that he is not entitled, and then exit the app.
"
"
It's difficult for me to provide a video since the scene view and game view don't show what I'm talking about, this is just on the headset's display.
My terrain trees rotate visually. When I say visually, it just looks like the perspective is being changed on the trees (the base doesn't rotate at all, just the tops). This only occurs with VR headset movement. The trees I've placed as regular objects don't have this rotation bug.
There's no code to show either since I haven't even coded anything yet.
Using SteamVR, Unity, and a Vive.
","
I did some reading and stumbled over this post:
https://forum.unity3d.com/threads/using-tree-gameobjects-vs-unitys-built-in-system.183224/
It explains that the unity generated trees are displayed as billboards to reduce rendering cost I would assume.
Billboards are generally know to look weird in VR, since their rotation algorithm messes up in some way. You can override it for normal billboards, but I don't think the terrain generated tree is changeable, unless you want to dig very deep.
Hope the post I linked and looking into unity billboards will help you understand.
"
"
I have one of these VR headsets that you insert your phone into and you can use VR apps. The headset also has a feature for AR, meaning I can take out a part of it and the phone`s camera can record what I am looking at and overlap AR elements on the image. The problem is none of the AR tutorials I read show how to do the split screen style VR apps usually have.
","
Its called Cardboard SDK but appears that Vuforia integrated it in its core: 
https://library.vuforia.com/articles/Solution/Integrating-Cardboard-to-the-ARVR-Sample
If using other AR libraries (EasyAR maybe) Cardboard SDK might still be usefull.
"
"
I can use a MovieTexture to play a video file in the Unity Editor, but trying to build to Android causes a compiler error.  A little digging shows that Android does not support MovieTexture, only full screen video playout.
But I've seen apps on the Samsung Gear VR that play a video non-full screen with various 3d assets around (such as a virtual theater).  Clearly non-full screen video is possible.
How is this accomplished? I've seen references to 360 video tutorials, but nothing like what I want to figure out.  Do I HAVE to use the native sdk for this functionality?  If so, are there any good tutorials?
","
EasyMovieTexture is an excellent option, i've tried a bunch of other ones and they don't hold a candle to it.
You can find it in the assets store, it's about 40 dollars if i remember correctly.
"
"
I am developing a VR App but in VR app Designing and other VRVideoview and Images not showing view properly in which it shows only properly with one eye but not showing with two eyes please give me some solution on that.
I am testing it with Samsung Gear VR.
","
For Showing video and image with two eyes with the proper view I have just added following code and its work fine for me in VR mode.
 videoWidgetView.setDisplayMode(VrWidgetView.DisplayMode.FULLSCREEN_STEREO);

"
"
I can use a MovieTexture to play a video file in the Unity Editor, but trying to build to Android causes a compiler error.  A little digging shows that Android does not support MovieTexture, only full screen video playout.
But I've seen apps on the Samsung Gear VR that play a video non-full screen with various 3d assets around (such as a virtual theater).  Clearly non-full screen video is possible.
How is this accomplished? I've seen references to 360 video tutorials, but nothing like what I want to figure out.  Do I HAVE to use the native sdk for this functionality?  If so, are there any good tutorials?
","
EasyMovieTexture is an excellent option, i've tried a bunch of other ones and they don't hold a candle to it.
You can find it in the assets store, it's about 40 dollars if i remember correctly.
"
"
I am developing a VR App but in VR app Designing and other VRVideoview and Images not showing view properly in which it shows only properly with one eye but not showing with two eyes please give me some solution on that.
I am testing it with Samsung Gear VR.
","
For Showing video and image with two eyes with the proper view I have just added following code and its work fine for me in VR mode.
 videoWidgetView.setDisplayMode(VrWidgetView.DisplayMode.FULLSCREEN_STEREO);

"
"
I used code (installing it wasn't working for me) from simple-raycaster to display pointer on my VR screen, it works but with some problem - 'behind' me there is square space where this pointer disappears and didn't work. Any idea how can I fix that?
","
I ran into an issue similar to what you describe.  I had several Text elements placed at various locations.  I had each wrapped in a VrButton with a onEnter and onExit calling functions to set timeouts gaze select an item.  I was seeing the raycaster disappear in a section of space directly behind me.  I eventually figured out that if I remove the VrButton wrapper and placed the onEnter and onExit directly on the Text element it solved the problem.  Hope this helps.
"
"
At first the texture works fine and the video plays as expected, but when VR is entered via VRDisplay.requestPresent it stops. Why is this and how to fix it?
","
The VR display has its own render loop. Usually needsUpdate is automatically set to true on every animation frame by three.js, but this is only true for the default display.
To fix this, get the VR display from the vrdisplayconnect event and create your own update loop. E.g.
let display = e.display;

let displayUpdateLoop = () =>
{
    // May get a warning if getFrameData is not called.
    let frameData = new VRFrameData();
    display.getFrameData(frameData);

    videoTexture.needsUpdate = true;

    // Stop loop if no longer presenting.
    if (display.isPresenting)
        display.requestAnimationFrame(displayUpdateLoop);
}
display.requestAnimationFrame(displayUpdateLoop);

"
"
When I try to initialize the platform:
Oculus.Platform.Core.Initialize();
I get an error:

UnityException: Oculus Platform failed to initialize.

What could be the reason?
Device: GearVR and S8+;
Env: Unity v2017.1.1f1, Oculus Utilities v1.19.0, OVRPlugin v1.19.0, SDK v1.1.9.0.
","
In my case, the problem was with Oculus Runtime.
To find the problem need to use OculusLogGatherer. This util can gather all logs and perform auto analysis. If u see in auto analysis something like this:

[Service_2017-11-15_14.59.51.txt] 15/11 14:59:54.088 {!ERROR!} [OAF
  ERROR] ......\package\PackageManager.cpp(187) : Package does not
  exist: oculus-runtime (1971015) [Service_2017-11-15_14.59.51.txt]
  15/11 14:59:54.103 {!ERROR!} [OAF ERROR]
  ......\package\PackageManager.cpp(187) : Package does not exist:
  oculus-runtime (1971015) [Service_2017-11-15_14.59.51.txt] 15/11
  15:00:09.078 {!ERROR!} [MobileConfigWrapper] Could not find
  mobileconfig metadata at C:\Program
  Files\Oculus\Support\oculus-runtime\mobileconfig\MobileConfigMetadata.json

This means that you need to install\reinstall a Oculus Runtime.
"
"
I'm working on a project to showcase active RFID tracking of multiple devices in real-time. I've been looking at React VR as a possible way of showing users in remote locations the real-time positioning of the devices within a static, predefined space. 
Essentially, I need to know if it is possible to use React VR with socket.io for pushing the real-time locations (x,y,z co-ords) to the frontend?
","
I've had success using Socket.io with React-VR to receive real-time Bitcoin transaction updates.
package.json
socket.io-client: ""^2.0.4""

Inside Redux store to load and store real-time data from coincap:
...
import io from 'socket.io-client';
...
export function loadTransactionsIntoState() {
  return function thunk(dispatch) {
    let socket = io.connect('http://socket.coincap.io', { jsonp: false })
    socket.on('trades', (tradeMsg) => {
      if (tradeMsg.coin == 'BTC') dispatch(addNewTransaction(tradeMsg.trade.data))
    })
  }
}

I originally had this connection statement inside a React-VR component which worked just as well.
"
"
I've set up a Physics Raycast Event System for the Google Cardboard in Unity, which works if I use a cube or any of the pre-set 3D objects. When I import my own object from 3DS Max, however, the Raycast doesn't seem to detect the object. I've checked, my object does have a collider on it, and the layer is set at default. I'm using the GVR API, and the GVR Reticle Pointer. 


","
The problem is that the Collider is not even aligned on that imported Object. The center value of the Collider should be 0,0,0 so reset that. After that, click the ""Edit Collider"" button, go to the Scene View and modify the Collider until it matches the shape/size of your imported 3D model.
See the image I am talking about below for more info:

"
"
How can I see the game on my glasses when pressing play in the editor?
I'm using GearVR. USB Cable is plugged to my headset.
","
You have to build and deploy the app on your phone and when it runs then mount it in headset. You can not use unity remote for VR apps. This is because GearVR headset takes priority when you mount the phone. VR apps which have GearVR sdk enabled should default ""insert your phone in headset"" when you run it. An app can either be VR or Non-VR and UnityRemote is Non-VR App. Can only be used as container for Non-VR games.
"
"
I am getting the issue: The app does not use the Daydream controller properly
The app allows users to use head gaze to position the cursor and interact with menu UI's ""Play"" button. 
Here is my class that sets the GvrPointerInputModule.Pointer to either use the GVRLaserPointer (for daydream) or the GvrReticlePointer (for cardboard):
public class InputModuleSelector : MonoBehaviour {
    bool DaydreamControllerConntected = false;
    public GvrBasePointer DayDreamController;
    public GvrBasePointer CardboardController;

    private void Start() {
        refreshControllers();
    }

    public static bool IsDayDreamMode() {
        return VRSettings.loadedDeviceName != ""cardboard"";
    }

    private void refreshControllers() {
        DaydreamControllerConntected = IsDayDreamMode();
        DayDreamController.gameObject.SetActive(DaydreamControllerConntected);
        CardboardController.gameObject.SetActive(!DaydreamControllerConntected);

        GvrPointerInputModule.Pointer = DaydreamControllerConntected ? DayDreamController : CardboardController;
    }
}

If we use the daydream, we will use the laser, otherwise we will use the reticle. 
How can I get my app to pass? Are we allowed to submit an app that can be used for both daydream and cardboard? Let me know if you need more information.
I am not sure if I should include a link... but my app is active on the playstore: https://play.google.com/store/apps/details?id=com.fungamefuntime.warehouse
","
I was able to figure and correct the issue with the help from Google Support. The main Camera rotates with the player's gaze direction. This means that the GvrControllerPointer depends on the main camera. This violated the requirements because you moved the laser when you also moved your head. Hierarchy View:
     Player
         Main Camera
             GvrReticlePointer
             GvrControllerPointer

When I switched to this structure, the problem was solved. The GvrControllerPointer is now independent of the Main Camera and the player's gaze.
     Player
         GvrControllerPointer
         Main Camera
         GvrReticlePointer

"
"
When starting VR tool on mobile and watching directly ahead, I would like to have the view to show the whole model in the center of the screen. The view should be at a slight angle, so I could see the whole building floor. Currently it is directly ahead, which leaves you with a view where you cannot see the whole model. How could I achieve this?
For example, in this Autodesk example, the model is in the center when you enter VR.
http://viewervr.herokuapp.com/
Current code, with what I am trying to adjust the camera position
document.getElementById(""toolbar-vrTool"").addEventListener(""click"", function () {
   let _navapi = viewer.navigation;
   let _camera = _navapi.getCamera();

   let xValue = viewer.getCamera().position.x;
   let yValue = viewer.getCamera().position.y;
   let zValue = viewer.getCamera().position.z;

   zValue = zValue * 0.5;
   yValue = (zValue * 0.7071) * -1;

   _camera.position.set(xValue, yValue, zValue);
});

Current view

View I would like to have

","
There is a function named fitToView() which will do exactly what you want. But you need to wait for the geometry to be fully loaded before using it. I also added a call to setHomeViewFrom() in the example below to reset the Home position to the fitToView() position result for later navigation.
oViewer.addEventListener (Autodesk.Viewing.GEOMETRY_LOADED_EVENT, onViewerGeometryLoaded) ;


function onViewerGeometryLoaded () {
    oViewer.removeEventListener (Autodesk.Viewing.GEOMETRY_LOADED_EVENT, onViewerGeometryLoaded) ;
    oViewer.fitToView (true) ;
    setTimeout (function () { oViewer.autocam.setHomeViewFrom (oViewer.navigation.getCamera ()) ; }, 1000) ;
}

"
"
I have included the following line to show a simple timer in the top left corner of a scene, which works of course, but when I tick the Virtual Reality Supported check-box and put on an Oculus Rift, it disappears.
void OnGUI()
{
    GUI.Label(new Rect(10, 10, 100, 20), Time.time.ToString());
}

What am I missing? What should I do additionally to resolve this?
","
OnGUI() does not work in VR. Instead use world space canvas UI.
I did the following for the Gear-VR.
Add a canvas (or other UI elements containing ""Canvas"" component) to your scene. Set render mode to World Space. This can be found on the render mode drop down list for the UI Canvas object:

I ended up going for an 800 x 600 canvas.
For the timer itself I used Time.deltaTime. 
Here is my whole PlayerController script:
void Start ()
{
 timeLeft = 5;
 rb = GetComponent<Rigidbody>();
 count = 0;
 winText.text = """";
 SetCountText ();
}

void Update() {
 if (gameOver) {
    if (Input.GetMouseButtonDown(0)) {
        Application.LoadLevel(0);
    }
} else {
    timeLeft -= Time.deltaTime;
    timerText.text = timeLeft.ToString(""0.00"");
    if (timeLeft < 0) {
        winner = false;
        GameOver(winner);
    }
 }
}
void GameOver(bool winner) {
 gameOver = true;
 timerText.text = ""-- --"";
 string tryAgainString = ""Tap the touch pad to try again."";
 if (!winner) { // case A
    winText.text = ""Time's up.\n"" + tryAgainString;
 }
 if (winner) { // case B
    winText.text = ""Well played!\n"" + tryAgainString;
 }
}

void FixedUpdate ()
{
 float moveHorizontal = Input.GetAxis (""Mouse X"");
 float moveVertical = Input.GetAxis (""Mouse Y""); 
 Vector3 movement = new Vector3 (moveHorizontal, 0.0f, moveVertical);    
 rb.AddForce (movement * speed);
}
void OnTriggerEnter(Collider other) 
{
 if (other.gameObject.CompareTag ( ""Pick Up"")){
    other.gameObject.SetActive (false);
    count = count + 1;
    SetCountText ();
    if (!gameOver) {
        timeLeft += 3;
    }
 }
}   
void SetCountText ()
{
 if (!gameOver) {
    countText.text = ""Count: "" + count.ToString ();
 }
 if (count >= 12) {
    winner = true;
    GameOver(winner);
 }
}

"
"
I'm trying to make a Gear VR application with the Gear VR controller and the Oculus SDK.
I got the GazePointerRing to work with the controller prefab. There is a reticle in my app visible that i can move around with the Gear VR controller. It detects the cubes that I've placed in the scene.
What I want to do now is point the reticle at a cube and hold a button on the controller, so that the cube will stick to my controller model and can move around until I let go of the button. I've been searching in the OVR Physics Raycaster script how i can call upon the raycast hit and input it in a button command in an if statement. But i can't find a way to call upon the raycast hit with an object.
This is the Oculus code in the OVR Physics Raycaster script:
using System.Collections.Generic;

namespace UnityEngine.EventSystems
{
    /// <summary>
    /// Simple event system using physics raycasts.
    /// </summary>
    [RequireComponent(typeof(OVRCameraRig))]
    public class OVRPhysicsRaycaster : BaseRaycaster
    {
        /// <summary>
        /// Const to use for clarity when no event mask is set
        /// </summary>
        protected const int kNoEventMaskSet = -1;


    /// <summary>
    /// Layer mask used to filter events. Always combined with the camera's culling mask if a camera is used.
    /// </summary>
    [SerializeField]
    public LayerMask m_EventMask = kNoEventMaskSet;

    protected OVRPhysicsRaycaster()
    { }

    public override Camera eventCamera
    {
        get
        {
            return GetComponent<OVRCameraRig>().leftEyeCamera;
        }
    }

    /// <summary>
    /// Depth used to determine the order of event processing.
    /// </summary>
    public virtual int depth
    {
        get { return (eventCamera != null) ? (int)eventCamera.depth : 0xFFFFFF; }
    }

    /// <summary>
    /// Event mask used to determine which objects will receive events.
    /// </summary>
    public int finalEventMask
    {
        get { return (eventCamera != null) ? eventCamera.cullingMask & m_EventMask : kNoEventMaskSet; }
    }

    /// <summary>
    /// Layer mask used to filter events. Always combined with the camera's culling mask if a camera is used.
    /// </summary>
    public LayerMask eventMask
    {
        get { return m_EventMask; }
        set { m_EventMask = value; }
    }


    /// <summary>
    /// Perform a raycast using the worldSpaceRay in eventData.
    /// </summary>
    /// <param name=""eventData""></param>
    /// <param name=""resultAppendList""></param>
    public override void Raycast(PointerEventData eventData, List<RaycastResult> resultAppendList)
    {
        // This function is closely based on PhysicsRaycaster.Raycast

        if (eventCamera == null)
            return;

        OVRRayPointerEventData rayPointerEventData = eventData as OVRRayPointerEventData;
        if (rayPointerEventData == null)
            return;

        var ray = rayPointerEventData.worldSpaceRay;

        float dist = eventCamera.farClipPlane - eventCamera.nearClipPlane;

        var hits = Physics.RaycastAll(ray, dist, finalEventMask);

        if (hits.Length > 1)
            System.Array.Sort(hits, (r1, r2) => r1.distance.CompareTo(r2.distance));

        if (hits.Length != 0)
        {
            for (int b = 0, bmax = hits.Length; b < bmax; ++b)
            {
                var result = new RaycastResult
                {
                    gameObject = hits[b].collider.gameObject,
                    module = this,
                    distance = hits[b].distance,
                    index = resultAppendList.Count,
                    worldPosition = hits[0].point,
                    worldNormal = hits[0].normal,
                };
                resultAppendList.Add(result);
            }
        }
    }

    /// <summary>
    ///  Perform a Spherecast using the worldSpaceRay in eventData.
    /// </summary>
    /// <param name=""eventData""></param>
    /// <param name=""resultAppendList""></param>
    /// <param name=""radius"">Radius of the sphere</param>
    public void Spherecast(PointerEventData eventData, List<RaycastResult> resultAppendList, float radius)
    {
        if (eventCamera == null)
            return;

        OVRRayPointerEventData rayPointerEventData = eventData as OVRRayPointerEventData;
        if (rayPointerEventData == null)
            return;

        var ray = rayPointerEventData.worldSpaceRay;

        float dist = eventCamera.farClipPlane - eventCamera.nearClipPlane;

        var hits = Physics.SphereCastAll(ray, radius, dist, finalEventMask);

        if (hits.Length > 1)
            System.Array.Sort(hits, (r1, r2) => r1.distance.CompareTo(r2.distance));

        if (hits.Length != 0)
        {
            for (int b = 0, bmax = hits.Length; b < bmax; ++b)
            {
                var result = new RaycastResult
                {
                    gameObject = hits[b].collider.gameObject,
                    module = this,
                    distance = hits[b].distance,
                    index = resultAppendList.Count,
                    worldPosition = hits[0].point,
                    worldNormal = hits[0].normal,
                };
                resultAppendList.Add(result);
            }
        }
    }
    /// <summary>
    /// Get screen position of this world position as seen by the event camera of this OVRPhysicsRaycaster
    /// </summary>
    /// <param name=""worldPosition""></param>
    /// <returns></returns>
    public Vector2 GetScreenPos(Vector3 worldPosition)
    {
        // In future versions of Uinty RaycastResult will contain screenPosition so this will not be necessary
        return eventCamera.WorldToScreenPoint(worldPosition);
    }
}
}

","
prerequisite: Ensure to have OVR Manager in your scene, it is a singleton and required for the GearVR controller (OVRInput class) to work.
My usual approach is to raycast a ray from the controller anchor position going forward and check if it hits a desired object
public class SampleScript : MonoBehaviour
{
    public Transform anchorPos;
    public GameObject detectionLineObject; // a gameObject with a line renderer

    private RaycastHit _hitInfo;
    private LineRenderer _detectionLine;

    void Start()
    {
        GameObject line = Instantiate(detectionLineObject);
        _detectionLine = line.GetComponent<LineRenderer>();
    }

    void Update()
    {
        DetectionManager();
    }

    void DetectionManager()
    {
        // check if controller is actually connected
        if (!OVRInput.IsControllerConnected(OVRInput.Controller.RTrackedRemote) || !OVRInput.IsControllerConnected(OVRInput.Controller.LTrackedRemote))
        {
            return;
        }
        // launch a ray from the OVRCameraRig's Anchor Right
        if (Physics.Raycast(anchorPos.position, anchorPos.forward, out _hitInfo))
        {
            // set our line points
            _detectionLine.SetPosition(0, anchorPos.position);
            _detectionLine.SetPosition(1, _hitInfo.point);

            MyComponent target = _hitInfo.collider.gameObject.GetComponent<MyComponent>();

            if (target != null)
            {
                // Do you stuff here
                target.StartInteraction();
            }
        }
        else
        {

            // point our line to infinity
            _detectionLine.SetPosition(0, anchorPos.position);
            _detectionLine.SetPosition(1, anchorPos.forward * 500.0f);
        }
    }
}

"
"
I'm currently using Oculus Rift + Touch in A-Frame (0.6.1). I haven't found any online forum on where to ask. I've tried using Firefox 55 (Which comes with WebVR), but it doesn't detect the Rift. 
I can use Firefox Nightly, but it's very unstable, and it comes with different problems. My most recent one, is that the whole screen goes blank whenever I try to enter VR mode. The display is seen in the headset, and the app is able to work flawlessly. But I can't go back to Firefox and check the devtools (which is annoying).
How can I troubleshoot the Oculus Rift so I can connect it to a stable version of Firefox, such as 55? 
Tried checking this:
WebVR supported, but no VRDisplays found, suddnly
But it didn't help.
","
Check about:config in Firefox and make sure the VR flags are enabled (specifically the Oculus one). Sometimes it may be disabled for some reason.
"
"
After importing GoogleVRForUnity I can't find the VR SDKs Cardboard and Daydream in Player Settings -> Other -> Virtual Reality SDKs.

","
Cardboard SDK and DayDream are available on the beta only for the moment
"
"
I'm working on a game that support both normal playing and VR using SceneKit. When playing normally I have a single SCNView, one camera which also corresponds to the view audio listener. When switching to VR mode I split the screen into two SCNView both playing the same scene from two different camera with the audio listener positioned between them, no problem here.
In my game I need positional audio so I use SCNAudioSuorce to play the sound effect using a SCNAction but since the same scene is playing in both view how can I mute one of them and have the audio only came from the other? I've searched the web and the documentation but found nothing about this.
iOS 11 Update: after testing my app on iOS 11 and trying to play a sound while in VR mode the app crashes on the rendering queue thread at __UpdateAudioTransform because I play that sound, removing it from the action (which includes other nodes transformations) makes the game run fine.
Update 1: Thanks to @rickster advice I tried implementing a SCNTechnique and following Apple documentation I got this
SCNTechnique(dictionary: [
    ""passes"" : [
        ""leftView"": [
            ""draw"": ""DRAW_SCENE"",
            ""pointOfView"": stereoCameraLeftName,
            ""viewport"": ""(0, 0, 400, 400)""
        ],
        ""rightView"": [
            ""draw"": ""DRAW_SCENE"",
            ""pointOfView"": stereoCameraRightName,
            ""viewport"": ""(200, 0, 400, 400)""
        ]
    ],
    ""sequence"": [ ""leftView"", ""rightView"" ]
    ])

where stereoCameraLeftName and stereoCameraRightName are defined elsewhere as strings and used as the names of the nodes containing the corresponding cameras and the viewports dimensions are just for testing. When I apply this technique to the scene it doesn't work and depending on how the scene pointOfView is set I either see the full scene filled with the background colour (pointOfView set to the camera used for non VR mode, no more linked to the scene graph) or the full size rendered using one of the two cameras (pointOfView set to nil), what am I missing?
Update 2: after some trail and error I found that the correct way to define a simple pass for a custom viewport is like this, with w and h defined as CGFloat:
let technique = SCNTechnique(dictionary: [
    ""passes"" : [
        ""leftView"": [
            ""outputs"": [ ""color"": ""COLOR"" ],
            ""draw"": ""DRAW_SCENE"",
            ""colorStates"": [
                ""clear"": true,
                ""clearColor"": ""sceneBackground""
            ],
            ""pointOfView"": stereoCameraLeftName,
            ""viewport"": ""0 0 \(w) \(h)""
        ],
        ""rightView"": [
            ""outputs"": [ ""color"": ""COLOR"" ],
            ""draw"": ""DRAW_SCENE"",
            ""colorStates"": [
                ""clear"": false
            ],
            ""pointOfView"": stereoCameraRightName,
            ""viewport"": ""\(w) 0 \(w) \(h)"",
            ""blendStates"": [
                ""colorOp"": ""add"",
                ""alphaOp"": ""add""
            ]
        ]
    ],
    ""sequence"": [ ""leftView"", ""rightView"" ]
])

But there is a problem: when the second pass is rendered the left side of the screen remain black (clear colour) and the right side is rendered correctly, if I exclude this pass leaving only the first one the left side is correctly rendered and the right side appear with the scene background colour as expected. What's strange is that if instead of testing on my iPhone 6s running iOS 11 I test on a simulator (iOS 11) all seems to work fine, I'm sure I'm missing something in combining the two passes.
Update 2.1: removing blendStates from the second pass gives the same results.
Update 3: trying to recreate a MWE to recreate the problem I've found that my approach is correct, I'll try to investigate other parts of my projects to find where's the problem.
Final update: the problems seems to be caused by antialiasing being active on the view, disabling it make the SCNTechnique works fine, seems like a bug of iOS 11 and I've already filed a bug report.
","
This is one of a number of issues with trying to render the same scene in multiple views. (Others include performance and concurrency.) Instead, it's probably a better choice to use only a single view, and use SCNTechnique to render the view's scene twice in it.
When you configure a technique, you can set the pointOfView separately for each draw pass — that'd probably be handy if you're looking for stereo separation.
You can also set the viewport for each pass. If you're doing some sort of stereo rendering, you'd use this to make one render pass cover one half of the view, and the other render pass cover the other half.
"
"
Just downloaded the latest Google Cardboard SDK for Unity from here:
https://developers.google.com/vr/unity/download
All the tutorials I have seen (including Google's own) talk about dragging the GvrViewerMain prefab into the Hierachy.  However, I have searched all the google VR assets included in the download and GvrViewerMain is not present.  Anyone know where I can download this from or which Prefab to use instead?
","
According to the Google VR SDK for Unity v1.50 release note, this version and any other version above it removed GvrViewerMain. 
It has been replaced with GvrEditorEmulator.
You can find this in GoogleVR/Prefabs/GvrEditorEmulator.prefab.
"
"
i saw people project using double image on a sky
[https://codepen.io/captDaylight/full/PNaVmR/][code]

when u open that you can see 2 sphere, and default background is white / without image. then if you move your cursor to sphere you can see background changed from white to image,
nah im just confuse how to set default image for white background.
i mean when i open that i can saw default image from my setting, only default image when i open that first time, other still same. can anyone help me? im really tired to tryng set default image.
","
Set the src: <a-sky src=""myimg.jpg"">
"
"
I'm using the VR Samples from Unity https://www.assetstore.unity3d.com/en/#!/content/51519 with the scene 'Shooter 360' (though I've tried the other scenes). 
I'm on a Mac and in XR settings I have both 'Mock HMD - HTC' and 'Oculus SDK' supported. 
When I press Play, I'm unable to move the cursor or interact with the GUI at all. Any recommendations?

","
By default it is the SDK you use that will rotate the camera at runtime (Cardboard, Oculus, HTC Vive, ...).
You can use a script that will rotate the camera while in edit mode. Here is an example, just assign this script to your camera and press Left Ctrl while moving the mouse in the editor window:
public class EditorCameraController : MonoBehaviour 
{    
    public float sensitivityX = 15F;
    public float sensitivityY = 15F;
    public float minimumX = -360F;
    public float maximumX = 360F;
    public float minimumY = -60F;
    public float maximumY = 60F;
    private float rotationX = 0F;
    private float rotationY = 0F;
    private Quaternion originalRotation;

    void Start()
    {
        originalRotation = transform.localRotation;
    }

    void Update () 
    {
        if (Input.GetKey(KeyCode.LeftControl))
        {
            rotationX += Input.GetAxis(""Mouse X"") * sensitivityX;
            rotationY += Input.GetAxis(""Mouse Y"") * sensitivityY;
            rotationX = ClampAngle(rotationX, minimumX, maximumX);
            rotationY = ClampAngle(rotationY, minimumY, maximumY);
            Quaternion xQuaternion = Quaternion.AngleAxis(rotationX, Vector3.up);
            Quaternion yQuaternion = Quaternion.AngleAxis(rotationY, -Vector3.right);
            transform.localRotation = originalRotation * xQuaternion * yQuaternion;
        }
    }

    public static float ClampAngle(float angle, float min, float max)
    {
        if (angle < -360F)
         angle += 360F;
        if (angle > 360F)
         angle -= 360F;
        return Mathf.Clamp(angle, min, max);
    }
}

"
"
I'm new to VR development,  I am a bit confused what's the differnce and the relationship between Cardboard Sdk and Oculus Sdk, if I want to develop an App which can play 360 VR video or photos, then which one is better I should to choose?
","
By Oculus SDK, I assume you mean the mobile SDK for GearVR since you mention cardboard.  If you're talking about the SDK for PC, then the question is Oculus vs SteamVR vs OpenVR vs Morpheus :)

The major choice for which to develop for I think probably comes down to what your timeline and audience is.
GearVR is the best quality device out there right now and it is SIGNIFICANTLY more polished than cardboard, and it requires specific expensive hardware (Note 4 or S6, soon the Note 5).  It has a store that people are buying things off of (even if it's not much yet).  But since GearVR apps in development need to be signed, you will only have an audience if you can commit to at least a demo that will be accepted on the Samsung store. (the alternative is to have every user use the developer-signing system, which means you'll get tens of people instead of thousands to see it probably)
Cardboard is a very short-term experience. There are no head-straps on cardboard for a reason - it's intended to be something you hold up for only a minute or two at a time.  Most of the audience is people interested in tech demos, but many more people will be able to try out your app.  Google is working on stuff behind the scenes, so there may be more meat on it in the future - a non-cardboard VR device I've heard rumors of, and they're pushing cardboard pretty hard for classroom experiences.  And in a couple years, every phone MIGHT have sensors good enough to give a GearVR-level experience.
Both SDKs will give you the basic two-eye 3d stereo rendering framework.  Oculus' is a little more fleshed out with some built in scene loading (it converts from FBX format which is made by MODO, which is expensive) and a UI library (I'm not very happy with it though).
Either way, most of the work you do will likely be independent of the SDK you use, so I don't think you'll be boxing yourself in whichever one you choose.
"
"
I would like to make a VR game in unity. I have that basics such as a door, drawer, gun and a object picker upper script. I would like to make a base builder/editor but I have absolutely no clue on how to get how to get where the controller is pointing to (like a laser pointer)
The project is in c#
","
You have to make a raycast from your controller to the object.
https://docs.unity3d.com/ScriptReference/Physics.Raycast.html
To simplify your work, I would recommend you to use the VRTK Toolkit, which you can simply add as assert to your unity project: https://vrtoolkit.readme.io/
The toolkit hat a lot of functions, like laserpoint, grapping etc.
I hope this helps you!
EDIT:
Here a very good example for a laser pointer from the VRTK Toolkit:
using UnityEngine;
using System.Collections;

public class ViveCursor : MonoBehaviour
{
    public enum AxisType
    {
        XAxis,
        ZAxis
    }

    public Color color;
    public float thickness = 0.002f;
    public AxisType facingAxis = AxisType.XAxis;
    public float length = 100f;
    public bool showCursor = true;

    GameObject holder;
    GameObject pointer;
    GameObject cursor;

    Vector3 cursorScale = new Vector3(0.05f, 0.05f, 0.05f);
    float contactDistance = 0f;
    Transform contactTarget = null;

    void SetPointerTransform(float setLength, float setThicknes)
    {
        //if the additional decimal isn't added then the beam position glitches
        float beamPosition = setLength / (2 + 0.00001f);

        if (facingAxis == AxisType.XAxis)
        {
            pointer.transform.localScale = new Vector3(setLength, setThicknes, setThicknes);
            pointer.transform.localPosition = new Vector3(beamPosition, 0f, 0f);
            if (showCursor)
            {
                cursor.transform.localPosition = new Vector3(setLength - cursor.transform.localScale.x, 0f, 0f);
            }
        }
        else
        {
            pointer.transform.localScale = new Vector3(setThicknes, setThicknes, setLength);
            pointer.transform.localPosition = new Vector3(0f, 0f, beamPosition);

            if (showCursor)
            {
                cursor.transform.localPosition = new Vector3(0f, 0f, setLength - cursor.transform.localScale.z);
            }
        }
    }

    // Use this for initialization
    void Start()
    {
        Material newMaterial = new Material(Shader.Find(""Unlit/Color""));
        newMaterial.SetColor(""_Color"", color);

        holder = new GameObject();
        holder.transform.parent = this.transform;
        holder.transform.localPosition = Vector3.zero;

        pointer = GameObject.CreatePrimitive(PrimitiveType.Cube);
        pointer.transform.parent = holder.transform;
        pointer.GetComponent<MeshRenderer>().material = newMaterial;

        pointer.GetComponent<BoxCollider>().isTrigger = true;
        pointer.AddComponent<Rigidbody>().isKinematic = true;
        pointer.layer = 2;

        if (showCursor)
        {
            cursor = GameObject.CreatePrimitive(PrimitiveType.Sphere);
            cursor.transform.parent = holder.transform;
            cursor.GetComponent<MeshRenderer>().material = newMaterial;
            cursor.transform.localScale = cursorScale;

            cursor.GetComponent<SphereCollider>().isTrigger = true;
            cursor.AddComponent<Rigidbody>().isKinematic = true;
            cursor.layer = 2;
        }

        SetPointerTransform(length, thickness);
    }

    float GetBeamLength(bool bHit, RaycastHit hit)
    {
        float actualLength = length;

        //reset if beam not hitting or hitting new target
        if (!bHit || (contactTarget && contactTarget != hit.transform))
        {
            contactDistance = 0f;
            contactTarget = null;
        }

        //check if beam has hit a new target
        if (bHit)
        {
            if (hit.distance <= 0)
            {

            }
            contactDistance = hit.distance;
            contactTarget = hit.transform;
        }

        //adjust beam length if something is blocking it
        if (bHit && contactDistance < length)
        {
            actualLength = contactDistance;
        }

        if (actualLength <= 0)
        {
            actualLength = length;
        }

        return actualLength; ;
    }

    void Update()
    {
        Ray raycast = new Ray(transform.position, transform.forward);

        RaycastHit hitObject;
        bool rayHit = Physics.Raycast(raycast, out hitObject);

        float beamLength = GetBeamLength(rayHit, hitObject);
        SetPointerTransform(beamLength, thickness);
    }
}

"
"
I am trying to make a VR game with google cardboard in unity. However we can not find a way to display score text right in front of the player. However when I add 2D text it is only on one side and therefore on one side of the eye and getting the position right for 2 texts is hard. If I use 3D text and set in front of the players position I think it will go into the wall if a player hits one. Is their any way to display a score on google cardboard / Unity VR.
","
You can either use native Unity Canvas UI or Googles hack to render OnGUI calls onto a texture.
I would definately recommend Canvas as that is the way Unity is working on their UI features, and it has much better layout capability.
To use canvas, Right click in the hierarchy and add UI->Text. You will automatically get a canvas. The important part is set the canvas to world-space (not screen space overlay). Then drag the canvas game object so it is a child of the Google Cardboard Main head object. Scale it down (like x:0.001,y:0.001,z=0.001) because by default it will be massive. To avoid going through walls position it about 0.5m in front of the camera - within any collider you may have.
"
"
I'm trying to create a webvr application where one can go in and then be able to navigate to different VR worlds/scenes. I've learned that I cannot have more than one scene in aframe, so I was wondering if it's possible to load HTML files to create the different scenes.
Basically, an index.html that would load its content from main.html file (for example).
Perhaps this can be done with javascript? Not quite sure how to make it work in aframe.
","
Actually loading external html data is a bit complex task.
Although You can switch between scenes defined in Your main index.html file making them visible = true/false, loading content from elsewhere can be achieved in multiple ways, I'll present two of them, even though many frameworks enable loading external html data:
JS/JQuery
I've managed to make a glitch, where the scene elements are loaded from a scene1.html file: https://glitch.com/edit/#!/jungle-aardvark?path=index.html:14:34
 I tried to find the most optimal way, but only this one using jquery managed to give me some results.
Basically I used a XMLHttpRequest();
var xhr= new XMLHttpRequest();
xhr.open('GET', 'scene1.html', true);
xhr.onreadystatechange= function() {
  if (this.readyState!==4) return;
  if (this.status!==200) return; // or whatever error handling you want
  document.querySelector('body').innerHTML= this.responseText;
};
xhr.send();

Frameworks, like ANGULAR
Angular enables You switching entire html+css+typescript 'packs', and swap them via routing, or just switching the visible html elements.
I've done some simple experiments on plunker. If you check it out, there is a couple things responsible for routing in the app.ts:
You need to import routing and Your scenes:
import { aScene } from './a-scene.comp.ts';
import { aScenePhoto } from './a-scene.photo.comp.ts';
import { mainPanel } from './mainpanel.comp.ts';
import {Routes,RouterModule} from'@angular/router';

then You can define the paths
const routes: Routes = [
{path: 'simple',component:aScene},
{path: 'photo',component:aScenePhoto},
{path: '',component:mainPanel},
{path: 'photo/:photoAsset',component:aScenePhoto},

And declare routing, and the scenes:
imports: [ BrowserModule,
          FormsModule,
          routing],
declarations: [ App,
              aScene,
              aScenePhoto,
              mainPanel,
              ],

Check it out on my linked plunker. The above elements are responsible for swapping the scenes.
In the end it's not easy do describe it in few words, if You want to use external frameworks, it would require some research about that framework.
I'm quite unfamiliar with react.js, so I won't get into it, yet i guess there are some rules for swapping 'chunks' of code, to achieve the same.
"
"
I want to create an animation on a single property (eg: scale) of a single entity (eg: a-circle) which would run one after another, using K-Frame. Is this possible? 
For example, this would be a simple case: 
<a-scene>
  <a-circle 
    position=""0 1.25 -5"" 
    color=""#F55"" 
    radius=""1"" 
    scale="".1 .1 1""
    animation__scale=""property: scale; dir: normal; dur: 500; easing: easeInSine; loop: true; to: 1 1 1""
  >
  </a-circle>
</a-scene>

I want to, say, scale from .1 .1 1 to .5 .5 1 for the first 500ms, then stay at .5 .5 1 for another 500ms, and go back to .1 .1 1 in the next 200ms, and then loop through this entire process over and over infinitely. 
Is this even possible?
","
You can daisy-chain the animations within a component checking which animation ended and starting the next one. In Your case that would be 
animation1 ended -> wait 500ms and start animation2 -> animation2 ended -> wait 200 ms and start animation1.
 
You can achieve this, by simply listening for the animation__[ID]-complete event, and emit another one.

If You just chained them, You could just map multiple listeners on the start event list.

However You want to give them some custom wait periods, which can be included within the mapped functions, or done manually:
  el.addEventListener('animation__scale1-complete', function() {
    setTimeout(function(){
    el.emit('secondAnimation');
    },500);
  })
   el.addEventListener('animation__scale2-complete', function() {
    setTimeout(function(){
    el.emit('firstAnimation');
    },200);
  })

working fiddle here: https://jsfiddle.net/gftruj/2qoz8b75/2/  Please note, that i trigger the first animation also on the loaded event in case it fires before its loaded.
"
"
I can't get my Gear VR touchpad to work - I'm just trying to move the camera position on touch. I've tried both pieces of code below:
public Camera cam;
void Update()
{
    if (Input.GetMouseButton (0)) 
    {
        cam.transform.position = new Vector3(-100f, -100f, -100f);
    }
}

and 
void Start () 
{
    OVRTouchpad.Create();
    OVRTouchpad.TouchHandler += HandleTouchHandler;         
}

void HandleTouchHandler (object sender, System.EventArgs e)
{
    OVRTouchpad.TouchArgs touchArgs = (OVRTouchpad.TouchArgs)e;
    if(touchArgs.TouchType == OVRTouchpad.TouchEvent.SingleTap)
    {
        cam.transform.position = new Vector3(-100f, -100f, -100f);
    }
}

My script is attached to the OVRPlayerController
","
You can't move the VR Camera, it's the SDK that determine the Camera position.
In order to move your camera you can just make a new GameObject as a parent of your Cam then move the parent GameObject (here ParentCamera):
public GameObject ParentCamera;

void Update()
{
    if (Input.GetMouseButton (0)) 
    {
        cam.transform.position = new Vector3(-100f, -100f, -100f);
    }
}

"
"
I want to use from OSVR plugin, but I don't know how it exactly works.
In OpenVR I have a framebuffer for each eye and when I write something on that buffers then I see them in glasses and I'm using from HTC Vive.
But now I don't know where those buffers are and how I can change VR eyes content, and I installed OSVR server and OSVR Vive plugin correctly but even this simple example doesn't work correctly, and I don't see anything in VR: 
#include <osvr/ClientKit/ClientKit.h>
#include <osvr/ClientKit/Display.h>
#include ""SDL2Helpers.h""
#include ""OpenGLCube.h""

#include <SDL.h>
#include <SDL_opengl.h>

#include <iostream>

static auto const WIDTH = 1920;
static auto const HEIGHT = 1080;

// Forward declarations of rendering functions defined below.
void render(osvr::clientkit::DisplayConfig &disp);
void renderScene();

int main(int argc, char *argv[]) {
    namespace SDL = osvr::SDL2;

    // Open SDL
    SDL::Lib lib;

    // Use OpenGL 2.1
    SDL_GL_SetAttribute(SDL_GL_CONTEXT_MAJOR_VERSION, 2);
    SDL_GL_SetAttribute(SDL_GL_CONTEXT_MINOR_VERSION, 1);

    // Create a window
    auto window = SDL::createWindow(""OSVR"", SDL_WINDOWPOS_UNDEFINED,
                                    SDL_WINDOWPOS_UNDEFINED, WIDTH, HEIGHT,
                                    SDL_WINDOW_OPENGL | SDL_WINDOW_SHOWN);
    if (!window) {
        std::cerr << ""Could not create window: "" << SDL_GetError() << std::endl;
        return -1;
    }

    // Create an OpenGL context and make it current.
    SDL::GLContext glctx(window.get());

    // Turn on V-SYNC
    SDL_GL_SetSwapInterval(1);

    // Start OSVR and get OSVR display config
    osvr::clientkit::ClientContext ctx(""com.osvr.example.SDLOpenGL"");
    osvr::clientkit::DisplayConfig display(ctx);
    if (!display.valid()) {
        std::cerr << ""\nCould not get display config (server probably not ""
                     ""running or not behaving), exiting.""
                  << std::endl;
        return -1;
    }

    std::cout << ""Waiting for the display to fully start up, including ""
                 ""receiving initial pose update...""
              << std::endl;
    while (!display.checkStartup()) {
        ctx.update();
    }
    std::cout << ""OK, display startup status is good!"" << std::endl;

    // Event handler
    SDL_Event e;
#ifndef __ANDROID__ // Don't want to pop up the on-screen keyboard
    SDL::TextInput textinput;
#endif
    bool quit = false;
    while (!quit) {
        // Handle all queued events
        while (SDL_PollEvent(&e)) {
            switch (e.type) {
            case SDL_QUIT:
                // Handle some system-wide quit event
                quit = true;
                break;
            case SDL_KEYDOWN:
                if (SDL_SCANCODE_ESCAPE == e.key.keysym.scancode) {
                    // Handle pressing ESC
                    quit = true;
                }
                break;
            }
            if (e.type == SDL_QUIT) {
                quit = true;
            }
        }

        // Update OSVR
        ctx.update();

        // Render
        render(display);

        // Swap buffers
        SDL_GL_SwapWindow(window.get());
    }

    return 0;
}

/// @brief A simple dummy ""draw"" function - note that drawing occurs in ""room
/// space"" by default. (that is, in this example, the modelview matrix when this
/// function is called is initialized such that it transforms from world space
/// to view space)
void renderScene() { draw_cube(1.0); }

/// @brief The ""wrapper"" for rendering to a device described by OSVR.
///
/// This function will set up viewport, initialize view and projection matrices
/// to current values, then call `renderScene()` as needed (e.g. once for each
/// eye, for a simple HMD.)
void render(osvr::clientkit::DisplayConfig &disp) {

    // Clear the screen to black and clear depth
    glClearColor(0, 0, 0, 1.0f);
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);

    /// For each viewer, eye combination...
    disp.forEachEye([](osvr::clientkit::Eye eye) {

        /// Try retrieving the view matrix (based on eye pose) from OSVR
        double viewMat[OSVR_MATRIX_SIZE];
        eye.getViewMatrix(OSVR_MATRIX_COLMAJOR | OSVR_MATRIX_COLVECTORS,
                          viewMat);
        /// Initialize the ModelView transform with the view matrix we
        /// received
        glMatrixMode(GL_MODELVIEW);
        glLoadIdentity();
        glMultMatrixd(viewMat);

        /// For each display surface seen by the given eye of the given
        /// viewer...
        eye.forEachSurface([](osvr::clientkit::Surface surface) {
            auto viewport = surface.getRelativeViewport();
            glViewport(static_cast<GLint>(viewport.left),
                       static_cast<GLint>(viewport.bottom),
                       static_cast<GLsizei>(viewport.width),
                       static_cast<GLsizei>(viewport.height));

            /// Set the OpenGL projection matrix based on the one we
            /// computed.
            double zNear = 0.1;
            double zFar = 100;
            double projMat[OSVR_MATRIX_SIZE];
            surface.getProjectionMatrix(
                zNear, zFar, OSVR_MATRIX_COLMAJOR | OSVR_MATRIX_COLVECTORS |
                                 OSVR_MATRIX_SIGNEDZ | OSVR_MATRIX_RHINPUT,
                projMat);

            glMatrixMode(GL_PROJECTION);
            glLoadIdentity();
            glMultMatrixd(projMat);

            /// Set the matrix mode to ModelView, so render code doesn't
            /// mess with the projection matrix on accident.
            glMatrixMode(GL_MODELVIEW);

            /// Call out to render our scene.
            renderScene();
        });
    });

    /// Successfully completed a frame render.
} 

Anybody knows how it works?
","
Instead of the display config API, use osvrRenderManager to get render info and present frames to the HMD. The display config API is a lower level API, and doesn't handle things like window placement for extended mode rendering on the Vive or direct mode rendering, or adjustments to projection based on render target scaling factors. This is typically handled by the RenderManager API.
https://github.com/sensics/osvr-rendermanager
"
"
Calling
GvrViewer.Instance.Recenter ();`

only recenters the view horizontally, i need it also vertically.
I'm using the latest unity beta and GVR 0.8`
","
You can recenter by making your VR camera object a child of another object (let's call it parent) and rotating the parent in the opposite direction of the rotation of your VR camera. This simple script will do (attach to parent):
public class Recenterer:MonoBehaviour 
{

    public Transform VRcam;  // drag the child VR cam here in the inspector

    public void Recenter()
    {
      transform.localRotation = Quaternion.Inverse(VRcam.rotation);
    }

}

"
"
how is it possible to switch between the camera on desktop and VR?
i wanna use  the mouse-cursor on desktop and when i enter VR  it should switch to another camera with cursor-fusing.
here is my code:
<a-entity id=""cam-desktop"" camera=""userHeight: 1.6; zoom:1 "" look-controls mouse-cursor>
</a-entity>


<a-entity id=""cam-vr"" camera=""userHeight: 1.6; zoom:1 "" look-controls>
    <a-animation begin=""cursor-fusing"" delay="" 3000"" attribute=""camera.zoom"" from=""1"" to=""4"" dur=""1000""></a-animation>
  <a-animation begin=""click"" delay=""500"" attribute=""camera.zoom"" from=""4"" to=""1"" dur=""1000""></a-animation>
    <a-entity cursor=""fuse: true; fuseTimeout:4000"" geometry=""primitive: ring; radiusInner: 0.02; radiusOuter: 0.03; thetaLength: 360; thetaStart: 0"" rotation=""0 0 90"" position=""0 0 -1"" material=""color: black; side: double; shader: flat"">
    <a-animation begin=""cursor-fusing"" attribute=""geometry.thetaLength"" from=""360"" to=""0"" easing=""linear"" dur=""3000""></a-animation>
    <a-animation begin=""mouseleave"" attribute=""geometry.thetaLength"" from=""360"" to=""360"" dur=""0""></a-animation>
    </entity>
</a-entity>

thank you very much!
","
You can create a component, which listenes to the 'enter-vr' and 'exit-vr' events, and set the active camera accordingly:
AFRAME.registerComponent('scenelistener',{
   init:function(){
      let vrcam = document.querySelector(""#cam-vr"");
      let dcam = document.querySelector(""#cam-desktop"");
      let  vrmode = false;
      this.el.sceneEl.addEventListener('enter-vr',function(){
           if(!vrmode){
               dcam.setAttribute('active',false);
               vrcam.setAttribute('active',true);
               vrmode!=vrmode;
           }
      }
      this.el.sceneEl.addEventListener('exit-vr',function(){
           if(vrmode){
               dcam.setAttribute('active',true);
               vrcam.setAttribute('active',false);
               vrmode!=vrmode;
           }
      }
  }
});

Actually, You can just paste the listeners anywhere, moreover You could just listen to any event and do stuff accordingly to the received event, I just wanted to show You the concept.Im not sure what happens if 2+ camera's are active, so i made them false on change.
UPDATE
I did not see that the cameras are entities, not primitives, so You have to set the attribute like this: setAttribute('camera','active',true)
I have some issues with the cursor, so i make it visible and invisible, depending on the vr state.
Working fiddle ( at least the camera switch ) here.
"
"
It's should be simple but I can't find any proper tutorial/sample for this.
I imported the OVR package to my Unity scene,
and put the OVRGearVrController in the middle of it.
I can see the controller in my scene, but that's it.
How or where should I connect it to react to the real controller that it's in my hand?
","

Import the latest OculusUtilities package to your project
Replace the default Camera with OVRCameraRig
Add GearVrController under the RightHandAnchor

"
"
I have the following code:
...
camera = new THREE.PerspectiveCamera(75, screenRatio, 1, 10000 );
camera.position.z = -10; // position.set(0, 0, -10) also not working.
controls = new THREE.VRControls( camera );
effect = new THREE.VREffect( renderer );
effect.setSize( window.innerWidth, window.innerHeight );
...

VRControls are working in sync with the accelerometer, but I can't change the cameras position. It seems stuck in the origin point (0,0,0). It was working just fine before applying VRControls and VREffect.
","
Found the solution inside Sechelt demo from Mozilla VR Team demos. I'll put here a code snippet as reference for other VR beginners.
Adding the camera to a group instead of updating the camera position directly is the way to move the camera.
var scene, renderer, cameraRatio, camera, controls, effect, dolly;

function init() {
    scene = new THREE.Scene();

    renderer = new THREE.WebGLRenderer({ antialias: true });
    renderer.setSize( window.innerWidth, window.innerHeight );
    document.body.appendChild( renderer.domElement );

    cameraRatio = window.innerWidth / window.innerHeight;
    camera = new THREE.PerspectiveCamera( 75, cameraRatio, 1, 1000 );       

    controls = new THREE.VRControls( camera );
    effect = new THREE.VREffect( renderer );
    effect.setSize( window.innerWidth, window.innerHeight );

    // This helps move the camera
    dolly = new THREE.Group();
    dolly.position.set( 0, 0, 0 );
    scene.add( dolly );
    dolly.add( camera );

    ...
    // Of course, there should be lights, objects, etc
}

function animate() {
    dolly.position.x += 0.1;
    controls.update();
    effect.render( scene, camera );
}

init();
animate();

"
"
I am making an application in that i am rendering an 360 image and i want to add an object or cross hair in front of camera i.e(billboard concept).
So i have tired alot of thing from stack overflow itself but i didn't get any idea how i can do that below is my code which i am trying to do.
Vector3 position;
    float[] newPosition4 = new float[4];
    position = obj.getPosition();
    float[] posVec4 = {0, 0, -8, 0.0f};
    float[] HeadViewMatrix_inv = new float[16];
    Matrix4 HeadViewMatrix4 = new Matrix4();
    HeadViewMatrix4.setAll(getCurrentCamera().getViewMatrix());
    HeadViewMatrix4 = HeadViewMatrix4.inverse();

    //Copy matrix to HeadViewMatrix array,
    HeadViewMatrix4.toFloatArray(HeadViewMatrix_inv);

    //Translation of viewMatrix 
    Matrix.multiplyMV(newPosition4, 0, HeadViewMatrix_inv, 0, posVec4, 0);
    obj.setPosition(newPosition4[0], newPosition4[1], newPosition4[2]);

So any one can help me out in this.
","
If I understand what you are asking for this is pretty much the simplest thing to do. You need to construct a new matrix or simply remove the current one (use identity).
With identity by drawing a line from A(-0.5, 0.0, 0.0) to B(0.5, 0.0, 0.0) will draw a straight vertical line in the middle of your screen. which will take half of your screen size centered in the middle.
I suggest you use orthographical projection matrix with screen coordinates and place your object in the middle as you would do another view (width/2, height/2) with a constant size.
But make sure your depth buffer is disabled when you draw this element if you want to keep it always visible.
If for some reason you want to keep the object in front of you using frustum matrix and possibly even keep the depth test then you simply need to keep the same projection matrix (frustum part) and set the view matrix to identity (the one from the camera). With these being set all you need to do is place the object in front of absolute zero (.0f, .0f, distanceToObject).
"
"
I have the following code:
...
camera = new THREE.PerspectiveCamera(75, screenRatio, 1, 10000 );
camera.position.z = -10; // position.set(0, 0, -10) also not working.
controls = new THREE.VRControls( camera );
effect = new THREE.VREffect( renderer );
effect.setSize( window.innerWidth, window.innerHeight );
...

VRControls are working in sync with the accelerometer, but I can't change the cameras position. It seems stuck in the origin point (0,0,0). It was working just fine before applying VRControls and VREffect.
","
Found the solution inside Sechelt demo from Mozilla VR Team demos. I'll put here a code snippet as reference for other VR beginners.
Adding the camera to a group instead of updating the camera position directly is the way to move the camera.
var scene, renderer, cameraRatio, camera, controls, effect, dolly;

function init() {
    scene = new THREE.Scene();

    renderer = new THREE.WebGLRenderer({ antialias: true });
    renderer.setSize( window.innerWidth, window.innerHeight );
    document.body.appendChild( renderer.domElement );

    cameraRatio = window.innerWidth / window.innerHeight;
    camera = new THREE.PerspectiveCamera( 75, cameraRatio, 1, 1000 );       

    controls = new THREE.VRControls( camera );
    effect = new THREE.VREffect( renderer );
    effect.setSize( window.innerWidth, window.innerHeight );

    // This helps move the camera
    dolly = new THREE.Group();
    dolly.position.set( 0, 0, 0 );
    scene.add( dolly );
    dolly.add( camera );

    ...
    // Of course, there should be lights, objects, etc
}

function animate() {
    dolly.position.x += 0.1;
    controls.update();
    effect.render( scene, camera );
}

init();
animate();

"
"
I am making an application in that i am rendering an 360 image and i want to add an object or cross hair in front of camera i.e(billboard concept).
So i have tired alot of thing from stack overflow itself but i didn't get any idea how i can do that below is my code which i am trying to do.
Vector3 position;
    float[] newPosition4 = new float[4];
    position = obj.getPosition();
    float[] posVec4 = {0, 0, -8, 0.0f};
    float[] HeadViewMatrix_inv = new float[16];
    Matrix4 HeadViewMatrix4 = new Matrix4();
    HeadViewMatrix4.setAll(getCurrentCamera().getViewMatrix());
    HeadViewMatrix4 = HeadViewMatrix4.inverse();

    //Copy matrix to HeadViewMatrix array,
    HeadViewMatrix4.toFloatArray(HeadViewMatrix_inv);

    //Translation of viewMatrix 
    Matrix.multiplyMV(newPosition4, 0, HeadViewMatrix_inv, 0, posVec4, 0);
    obj.setPosition(newPosition4[0], newPosition4[1], newPosition4[2]);

So any one can help me out in this.
","
If I understand what you are asking for this is pretty much the simplest thing to do. You need to construct a new matrix or simply remove the current one (use identity).
With identity by drawing a line from A(-0.5, 0.0, 0.0) to B(0.5, 0.0, 0.0) will draw a straight vertical line in the middle of your screen. which will take half of your screen size centered in the middle.
I suggest you use orthographical projection matrix with screen coordinates and place your object in the middle as you would do another view (width/2, height/2) with a constant size.
But make sure your depth buffer is disabled when you draw this element if you want to keep it always visible.
If for some reason you want to keep the object in front of you using frustum matrix and possibly even keep the depth test then you simply need to keep the same projection matrix (frustum part) and set the view matrix to identity (the one from the camera). With these being set all you need to do is place the object in front of absolute zero (.0f, .0f, distanceToObject).
"
"
Hi I'm looking for info about registering a custom display as ""Direct Mode"" compatible.
I have been going over the ""VR Headset Developers"" SDK and see example code for getting existing Direct Mode displays but no docs for registering one in the system as ""Direct Mode"" compatible. How do the NVidia drivers / ""VR Headset Developers"" API know what is a ""Direct Mode"" display?
Is this done in the EDID? The Oculus drivers seem to be using the EDID to find a ""Rift DK2"" display and then telling the system (NVidia drivers) that display is ""Direct Mode"" compatible... but as to how is the question I'm failing to find info on.
Any help or direction on this matter would be much appreciated.
If there is a better forum to be asking this on please direct me to that location.
","
Ok I found the documentation for it as it was in the ""release notes"". Odd place for documentation.
The DOCS for this should really be added in the ""NVIDIA Direct Mode Programming Guide.pdf"" file and NOT just the ""Direct Mode Release Notes.txt"".
"
"
In A-Frame FAQ, it is written that:

Link traversal support was introduced in the WebVR 1.0 API and A-Frame
  0.3.0. There is a link component that can trigger navigation and built-in support for knowing whether the user is navigating from
  another WebVR experience.

However, it seems there is no entry in A-Frame Docs about this link component. 
How could one use such a component to trigger link traversal ?
Thank you.
","
Oops, that is a mistake. Built-in link traversal and documentation has not yet been introduced as we improve link traversal experiences in the WebVR specification and the browsers themselves.
"
"
I am trying to implement 360 Video Viewer in my project but I am getting an error for the line:
mVrVideoView.loadVideoFromAsset(""sea.mp4"", options);

This is the error
Method loadVideoFromAsset must be called from the UI thread, currently inferred 
thread is worker

Following is my code:
package com.example.jal.jp;

import android.os.AsyncTask;
import android.os.Bundle;
import android.support.design.widget.FloatingActionButton;
import android.support.design.widget.Snackbar;
import android.support.v7.app.AppCompatActivity;
import android.support.v7.widget.Toolbar;
import android.view.View;
import android.widget.Button;
import android.widget.SeekBar;

import com.google.vr.sdk.widgets.video.VrVideoEventListener;
import com.google.vr.sdk.widgets.video.VrVideoView;

import java.io.IOException;

public abstract class VR_Video extends AppCompatActivity implements SeekBar.OnSeekBarChangeListener {

    private VrVideoView mVrVideoView;
    private SeekBar mSeekBar;
    private Button mVolumeButton;

    private boolean mIsPaused;
    private boolean mIsMuted;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_vr__video);

        initViews();

    }
    public void onPlayPausePressed() {

    }

    public void onVolumeToggleClicked() {

    }

    @Override
    public void onProgressChanged(SeekBar seekBar, int i, boolean b) {

    }

    private void initViews() {
        mVrVideoView = (VrVideoView) findViewById(R.id.video_view);
        mSeekBar = (SeekBar) findViewById(R.id.seek_bar);
        mVolumeButton = (Button) findViewById(R.id.btn_volume);
        mVrVideoView.setEventListener(new ActivityEventListener());
        mSeekBar.setOnSeekBarChangeListener(this);
        mVolumeButton.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                onVolumeToggleClicked();
            }
        });
    }
    class VideoLoaderTask extends AsyncTask<Void, Void, Boolean> {

        @Override
        protected Boolean doInBackground(Void... voids) {
            try {
                VrVideoView.Options options = new VrVideoView.Options();
                options.inputType = VrVideoView.Options.TYPE_MONO;
                mVrVideoView.loadVideoFromAsset(""sea.mp4"", options);
            } catch( IOException e ) {
                //Handle exception
            }

            return true;
        }
    }
    public void playPause() {

    }
    @Override
    protected void onPause() {
        super.onPause();
        mVrVideoView.pauseRendering();
        mIsPaused = true;
    }

    @Override
    protected void onResume() {
        super.onResume();
        mVrVideoView.resumeRendering();
        mIsPaused = false;
    }

    @Override
    protected void onDestroy() {
        mVrVideoView.shutdown();
        super.onDestroy();
    }
    private class ActivityEventListener extends VrVideoEventListener {
        @Override
        public void onLoadSuccess() {
            super.onLoadSuccess();
        }

        @Override
        public void onLoadError(String errorMessage) {
            super.onLoadError(errorMessage);
        }

        @Override
        public void onClick() {
            super.onClick();
        }

        @Override
        public void onNewFrame() {
            super.onNewFrame();
        }

        @Override
        public void onCompletion() {
            super.onCompletion();
        }
    }
}

Please help. I tried my best but couldn't fix. 
","
Remove AsyncTask Implementation And Call Required Methods From UI Thread
Use Below Code : 
private void initViews() {
    mVrVideoView = (VrVideoView) findViewById(R.id.video_view);
    mSeekBar = (SeekBar) findViewById(R.id.seek_bar);
    mVolumeButton = (Button) findViewById(R.id.btn_volume);
    mVrVideoView.setEventListener(new ActivityEventListener());
    try {
            VrVideoView.Options options = new VrVideoView.Options();
            options.inputType = VrVideoView.Options.TYPE_MONO;
            mVrVideoView.loadVideoFromAsset(""sea.mp4"", options);
        } catch( IOException e ) {
            //Handle exception
        }
    mSeekBar.setOnSeekBarChangeListener(this);
    mVolumeButton.setOnClickListener(new View.OnClickListener() {
        @Override
        public void onClick(View view) {
            onVolumeToggleClicked();
        }
    });
}

"
"
Other questions like this one indicate that Unity tries to enforce a metric system, meaning that one unit should roughly equate to one meter. However, in VR specifically absolute size really does matter, because you may want things to be a certain size relative to you.
TLDR: is 1 unit in game space a meter in real space? If not, does there exist some realistic approximation?
","
I confirmed what mattnewport said in the comments: 1 distance unit in Unity is one meter.
"
"

I am building a simple 360 Video Viewer, but it keep crashing. 
Below is the code for choices page, where when the user clicks on view video, the 360 Video should open up.
 package com.example.jal.jp;
import android.content.Intent;
    import android.os.Bundle;
    import android.support.design.widget.FloatingActionButton;
    import android.support.design.widget.Snackbar;
    import android.support.v7.app.AppCompatActivity;
    import android.support.v7.widget.Toolbar;
    import android.view.View;
    import android.widget.Button;

    public class Choices extends AppCompatActivity {

        public Button first_button;
        //public Button second_button;

        public void init(){
            first_button = (Button)findViewById(R.id.video);
           // second_button = (Button)findViewById(R.id.video);
            first_button.setOnClickListener(new View.OnClickListener() {
                @Override
                public void onClick(View v) {
                    Intent view = new Intent(Choices.this,VR_Video.class);
                    startActivity(view);
                }
            });
           /** second_button.setOnClickListener(new View.OnClickListener() {
                @Override
                public void onClick(View view) {
                    Intent view2 = new Intent(Choices.this,VR_Video.class);
                    startActivity(view2);

                }
            });*/
        }

        @Override
        protected void onCreate(Bundle savedInstanceState) {
            super.onCreate(savedInstanceState);
            setContentView(R.layout.activity_choices);
            init();

        }

    }

Below is the code for the Video Viewer file.
package com.example.jal.jp;

import android.os.AsyncTask;
import android.os.Bundle;
import android.support.design.widget.FloatingActionButton;
import android.support.design.widget.Snackbar;
import android.support.v7.app.AppCompatActivity;
import android.support.v7.widget.Toolbar;
import android.view.View;
import android.widget.Button;
import android.widget.SeekBar;

import com.google.vr.sdk.widgets.video.VrVideoEventListener;
import com.google.vr.sdk.widgets.video.VrVideoView;

import java.io.IOException;

public abstract class VR_Video extends AppCompatActivity implements SeekBar.OnSeekBarChangeListener {

    private static final String STATE_PROGRESS = ""state_progress"";
    private static final String STATE_DURATION = ""state_duration"";
    private VrVideoView mVrVideoView;
    private SeekBar mSeekBar;
    private Button mVolumeButton;

    private boolean mIsPaused;
    private boolean mIsMuted;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_vr__video);

        initViews();

    }
    @Override
    protected void onSaveInstanceState(Bundle outState) {
        outState.putLong(STATE_PROGRESS, mVrVideoView.getCurrentPosition());
        outState.putLong(STATE_DURATION, mVrVideoView.getDuration());

        super.onSaveInstanceState(outState);
    }

    @Override
    protected void onRestoreInstanceState(Bundle savedInstanceState) {
        super.onRestoreInstanceState(savedInstanceState);

        long progress = savedInstanceState.getLong(STATE_PROGRESS);

        mVrVideoView.seekTo(progress);
        mSeekBar.setMax((int) savedInstanceState.getLong(STATE_DURATION));
        mSeekBar.setProgress((int) progress);
    }
    public void onPlayPausePressed() {

    }

    public void onVolumeToggleClicked() {
        mIsMuted = !mIsMuted;
        mVrVideoView.setVolume(mIsMuted ? 0.0f : 1.0f);

    }


    @Override
    public void onProgressChanged(SeekBar seekBar, int progress, boolean fromUser) {
        if( fromUser ) {
            mVrVideoView.seekTo(progress);
        }
    }
    private void initViews() {
        mVrVideoView = (VrVideoView) findViewById(R.id.video_view);
        mSeekBar = (SeekBar) findViewById(R.id.seek_bar);
        mVolumeButton = (Button) findViewById(R.id.btn_volume);
        mVrVideoView.setEventListener(new ActivityEventListener());
        //try { VrVideoView.Options options = new VrVideoView.Options(); options.inputType = VrVideoView.Options.TYPE_MONO;
          //  mVrVideoView.loadVideoFromAsset(""sea.mp4"", options); } catch( IOException e ) { //Handle exception }
        mSeekBar.setOnSeekBarChangeListener(this);
        mVolumeButton.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                onVolumeToggleClicked();
            }
        });
    }


    class VideoLoaderTask extends AsyncTask<Void, Void, Boolean> {

        @Override
        protected Boolean doInBackground(Void... voids) {
            runOnUiThread(new Runnable() {
                @Override
                public void run() {
                    try {
                        // VrVideoView.Options options = new VrVideoView.Options();
                        //options.inputType = VrVideoView.Options.TYPE_MONO;
                        VrVideoView.Options options = new VrVideoView.Options(); options.inputType = VrVideoView.Options.TYPE_MONO;
                        mVrVideoView.loadVideoFromAsset(""sea.mp4"", options);
                    } catch(IOException e) {
                        //Handle exception
                    }
                    //your code here


                }
            });


            return true;
        }

        }

    public void playPause() {
        if( mIsPaused ) {
            mVrVideoView.playVideo();
        } else {
            mVrVideoView.pauseVideo();
        }

        mIsPaused = !mIsPaused;

    }
    @Override
    protected void onPause() {
        super.onPause();
        mVrVideoView.pauseRendering();
        mIsPaused = true;
    }

    @Override
    protected void onResume() {
        super.onResume();
        mVrVideoView.resumeRendering();
        mIsPaused = false;
    }

    @Override
    protected void onDestroy() {
        mVrVideoView.shutdown();
        super.onDestroy();
    }
    private class ActivityEventListener extends VrVideoEventListener {
        @Override
        public void onLoadSuccess() {
            super.onLoadSuccess();
            mSeekBar.setMax((int) mVrVideoView.getDuration());
            mIsPaused = false;


        }

        @Override
        public void onLoadError(String errorMessage) {
            super.onLoadError(errorMessage);
        }

        @Override
        public void onClick() {
            super.onClick();
            playPause();
        }

        @Override
        public void onNewFrame() {
            super.onNewFrame();
            mSeekBar.setProgress((int) mVrVideoView.getCurrentPosition());

        }

        @Override
        public void onCompletion() {
            super.onCompletion();
            mVrVideoView.seekTo(0);

        }

    }
}

Code for Content_choices.
<?xml version=""1.0"" encoding=""utf-8""?>
<RelativeLayout xmlns:android=""http://schemas.android.com/apk/res/android""
    xmlns:app=""http://schemas.android.com/apk/res-auto""
    xmlns:tools=""http://schemas.android.com/tools""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    android:paddingBottom=""@dimen/activity_vertical_margin""
    android:paddingLeft=""@dimen/activity_horizontal_margin""
    android:paddingRight=""@dimen/activity_horizontal_margin""
    android:paddingTop=""@dimen/activity_vertical_margin""
    app:layout_behavior=""@string/appbar_scrolling_view_behavior""
    tools:context=""com.example.jal.jp.Choices""
    tools:showIn=""@layout/activity_choices"">

    <Button
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:text=""Play VR Video""
        android:id=""@+id/video""
        android:layout_marginTop=""102dp""
        android:layout_alignParentTop=""true""
        android:layout_alignLeft=""@+id/survey""
        android:layout_alignStart=""@+id/survey""
        android:background=""#403e97""
        android:layout_alignRight=""@+id/survey""
        android:layout_alignEnd=""@+id/survey""
        android:textColor=""#be3e3e"" />

    <Button
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:text=""Take the Survey""
        android:id=""@+id/survey""
        android:layout_marginTop=""105dp""
        android:layout_below=""@+id/video""
        android:layout_centerHorizontal=""true""
        android:allowUndo=""true""
        android:background=""#2845a6""
        android:textColor=""#8f2626"" />

    <Button
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:text=""360 View""
        android:id=""@+id/view""
        android:layout_marginTop=""89dp""
        android:layout_below=""@+id/survey""
        android:layout_alignLeft=""@+id/video""
        android:layout_alignStart=""@+id/video""
        android:background=""#223e80""
        android:layout_alignRight=""@+id/video""
        android:layout_alignEnd=""@+id/video""
        android:textColor=""#bf1b1b"" />
</RelativeLayout>

Code for activity_vr_video.xml
<?xml version=""1.0"" encoding=""utf-8""?>
<LinearLayout xmlns:android=""http://schemas.android.com/apk/res/android""
    android:layout_width=""match_parent""
    android:layout_height=""match_parent""
    android:orientation=""vertical"">

    <com.google.vr.sdk.widgets.video.VrVideoView
        android:id=""@+id/video_view""
        android:layout_width=""match_parent""
        android:layout_height=""250dp""/>

    <SeekBar
        android:id=""@+id/seek_bar""
        android:layout_height=""32dp""
        android:layout_width=""match_parent""
        style=""?android:attr/progressBarStyleHorizontal""/>

    <Button
        android:id=""@+id/btn_volume""
        android:layout_width=""wrap_content""
        android:layout_height=""wrap_content""
        android:text=""Volume Toggle""/>

</LinearLayout>

Crash Log:
06-27 00:03:17.626 1529-1548/? I/ActivityManager: Displayed com.example.jal.jp/.Choices: +387ms
06-27 00:03:17.716 2776-2789/? E/Surface: getSlotFromBufferLocked: unknown buffer: 0xae9f0230
06-27 00:03:18.259 1609-1609/? D/skia: --- SkImageDecoder::Factory returned null
06-27 00:03:22.295 1529-1541/? I/ActivityManager: START u0 {cmp=com.example.jal.jp/.VR_Video} from uid 10058 on display 0
06-27 00:03:22.349 2776-2776/? D/AndroidRuntime: Shutting down VM
06-27 00:03:22.350 2776-2776/? E/AndroidRuntime: FATAL EXCEPTION: main
                                                 Process: com.example.jal.jp, PID: 2776
                                                 java.lang.RuntimeException: Unable to instantiate activity ComponentInfo{com.example.jal.jp/com.example.jal.jp.VR_Video}: java.lang.InstantiationException: java.lang.Class<com.example.jal.jp.VR_Video> cannot be instantiated
                                                     at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2327)
                                                     at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2476)
                                                     at android.app.ActivityThread.-wrap11(ActivityThread.java)
                                                     at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1344)
                                                     at android.os.Handler.dispatchMessage(Handler.java:102)
                                                     at android.os.Looper.loop(Looper.java:148)
                                                     at android.app.ActivityThread.main(ActivityThread.java:5417)
                                                     at java.lang.reflect.Method.invoke(Native Method)
                                                     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:726)
                                                     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:616)
                                                  Caused by: java.lang.InstantiationException: java.lang.Class<com.example.jal.jp.VR_Video> cannot be instantiated
                                                     at java.lang.Class.newInstance(Native Method)
                                                     at android.app.Instrumentation.newActivity(Instrumentation.java:1067)
                                                     at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2317)
                                                     at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2476) 
                                                     at android.app.ActivityThread.-wrap11(ActivityThread.java) 
                                                     at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1344) 
                                                     at android.os.Handler.dispatchMessage(Handler.java:102) 
                                                     at android.os.Looper.loop(Looper.java:148) 
                                                     at android.app.ActivityThread.main(ActivityThread.java:5417) 
                                                     at java.lang.reflect.Method.invoke(Native Method) 
                                                     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:726) 
                                                     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:616) 
06-27 00:03:22.372 1529-1905/? W/ActivityManager:   Force finishing activity com.example.jal.jp/.VR_Video
06-27 00:03:22.384 1529-1905/? W/ActivityManager:   Force finishing activity com.example.jal.jp/.Choices
06-27 00:03:22.483 1529-2747/? I/OpenGLRenderer: Initialized EGL, version 1.4
06-27 00:03:22.772 1529-2747/? W/EGL_emulation: eglSurfaceAttrib not implemented
06-27 00:03:22.772 1529-2747/? W/OpenGLRenderer: Failed to set EGL_SWAP_BEHAVIOR on surface 0x9b1d1ba0, error=EGL_SUCCESS
06-27 00:03:22.886 1529-1543/? W/ActivityManager: Activity pause timeout for ActivityRecord{a0649b u0 com.example.jal.jp/.VR_Video t26 f}
06-27 00:03:24.063 1609-1609/? D/skia: --- SkImageDecoder::Factory returned null
06-27 00:03:32.309 1529-1543/? W/ActivityManager: Launch timeout has expired, giving up wake lock!
06-27 00:03:32.390 1529-1543/? W/ActivityManager: Activity destroy timeout for ActivityRecord{3e989dd u0 com.example.jal.jp/.Choices t26 f}
06-27 00:03:42.336 1529-1543/? W/ActivityManager: Activity destroy timeout for ActivityRecord{a0649b u0 com.example.jal.jp/.VR_Video t26 f}

","
Your problem is in this line:
Intent view = new Intent(Choices.this,VR_Video.class);

You got the error because you try to start the VR_Video activity, but this is an abstract class. Delete the abstract keyword in your VR_Video class and it will be working.
And implement your interface methods:
@Override
public void onProgressChanged(SeekBar seekBar, int i, boolean b) {

}

@Override
public void onStartTrackingTouch(SeekBar seekBar) {

}

@Override
public void onStopTrackingTouch(SeekBar seekBar) {

}

"
"
I use Three.js to render and move (my orbitControl changes camera.position) in a small scene.
Now I have an oculus rift. So I added VRControls and VREffect.
There is no problem to move the head.
But I can no more move in the scene because VRControls override the camera parameters :  
object.quaternion.copy( state.orientation ); // object is the camera

I thought that it was easy to correct : I have only to update the camera instead of overriding it :
object.quaternion.copy(stateOrientationQuat.multiply(currentCameraQuat));

But it does not work : it renders a moving flicking scene. VRControls and orbitControl seem to fight...
Could you tell me what is to do to integrate VRControls in an existing project ? If you have the update code (I don't really know quaternions...) it would very help.
Thanks
","
Edit: See my other answer for a better method.

You can combine both controls by creating a VRControls instance that acts on a fake camera and then apply the transform on top of the orbit controls:
Relevant snippet:
var orbitControls = new THREE.OrbitControls(camera);

// Store the position of the VR HMD in a dummy camera.
var fakeCamera = new THREE.Object3D();
var vrControls = new THREE.VRControls(fakeCamera);

...

var render = function() {
  requestAnimationFrame(render);

  orbitControls.update();
  vrControls.update();

  // Temporarily save the orbited camera position
  var orbitPos = camera.position.clone();

  // Apply the VR HMD camera position and rotation
  // on top of the orbited camera.
  var rotatedPosition = fakeCamera.position.applyQuaternion(
    camera.quaternion);
  camera.position.add(rotatedPosition);
  camera.quaternion.multiply(fakeCamera.quaternion);

  vrEffect.render(scene, camera);

  // Restore the orbit position, so that the OrbitControls can
  // pickup where it left off.
  camera.position.copy(orbitPos);
};

Full example:


var renderer = new THREE.WebGLRenderer( { antialias: true } );
renderer.setSize(window.innerWidth, window.innerHeight);
document.body.appendChild(renderer.domElement);

var vrEffect = new THREE.VREffect(renderer, function () {});

var camera = new THREE.PerspectiveCamera(
  75, window.innerWidth / window.innerHeight, 0.1, 1000);

var orbitControls = new THREE.OrbitControls(camera);

// Store the position of the VR HMD in a dummy camera.
var fakeCamera = new THREE.Object3D();
var vrControls = new THREE.VRControls(fakeCamera);

var scene;
var createScene = function () {
  scene = new THREE.Scene();

  scene.add(new THREE.PointLight());

  var cube = new THREE.Mesh(
    new THREE.BoxGeometry(1, 1, 1),
    new THREE.MeshLambertMaterial({
      color: 'green'
    })
  );
  cube.position.set(-1, -2, -5);
  scene.add(cube);
  orbitControls.target = cube.position;

  for (var i = 0; i < 10; i++) {
    cube = new THREE.Mesh(
      new THREE.BoxGeometry(1, 1, 1),
      new THREE.MeshLambertMaterial()
    );
    cube.position.set(
      (Math.random() - 0.5) * 20,
      (Math.random() - 0.5) * 20,
      (Math.random() - 0.5) * 20
    );
    scene.add(cube);
  }
};
createScene();

var render = function() {
  requestAnimationFrame(render);
  
  orbitControls.update();
  vrControls.update();
  
  // Temporarily save the orbited camera position
  var orbitPos = camera.position.clone();
  
  // Apply the VR HMD camera position and rotation
  // on top of the orbited camera.
  var rotatedPosition = fakeCamera.position.applyQuaternion(
    camera.quaternion);
  camera.position.add(rotatedPosition);
  camera.quaternion.multiply(fakeCamera.quaternion);
  
  vrEffect.render(scene, camera);
  
  // Restore the orbit position, so that the OrbitControls can
  // pickup where it left off.
  camera.position.copy(orbitPos);
};

render();

window.addEventListener('resize', function onWindowResize() {
  camera.aspect = window.innerWidth / window.innerHeight;
  camera.updateProjectionMatrix();
  vrEffect.setSize( window.innerWidth, window.innerHeight );
}, false );
<script src=""https://code.jquery.com/jquery-2.1.1.min.js""></script>
<script src=""https://cdn.rawgit.com/mrdoob/three.js/8125c7dac9bf0c7df19bd1d7d9695cbfc0425867/build/three.js""></script>
<script src=""https://cdn.rawgit.com/mrdoob/three.js/8125c7dac9bf0c7df19bd1d7d9695cbfc0425867/examples/js/effects/VREffect.js""></script>
<script src=""https://cdn.rawgit.com/mrdoob/three.js/8125c7dac9bf0c7df19bd1d7d9695cbfc0425867/examples/js/controls/VRControls.js""></script>
<script src=""https://cdn.rawgit.com/mrdoob/three.js/8125c7dac9bf0c7df19bd1d7d9695cbfc0425867/examples/js/controls/OrbitControls.js""></script>



"
"
I have created a scene in A-Frame using 'A-Frame Physics System' (https://github.com/donmccurdy/aframe-physics-system) :
<!DOCTYPE>
<html>
<head>

<script src=""aframe.min.js""></script>
<script src=""aframe-extras.min.js""></script>
<script src=""aframe-physics-system-master/dist/aframe-physics-system.min.js""></script>
</head>

  <a-scene id=""myscene"" physics>
    <!--CAMERA-->
    <a-entity camera=""userHeight: 1.6"" look-controls></a-entity>

    <!--BALL1-->
    <a-sphere color=""red"" radius=""0.3"" position=""5 5 5"" dynamic-body></a-sphere>

    <!--BALL2-->
    <a-sphere color=""green"" radius=""0.3"" position=""6 5 5"" dynamic-body></a-sphere>

    <!--GROUND-->
    <a-plane id=""ground"" height=""200"" width=""200"" rotation=""-90 0 0"" position=""0 0 0"" metalness=""0"" roughness=""1"" static-body></a-plane>

  </a-scene>
</body>
</html>

The scene consist of two spheres and one plane. I want one ball to bounce more than other when it hits plane. I got to know from the documentation that we can change properties like friction and restitution for whole scene using :
<a-scene physics=""friction: 0.1; restitution: 0.5"">
    <!-- ... -->
</a-scene>

But I want different friction and restitution values for different spheres. Please let me know if it is possible in A-Frame. Thanks in advance!
","
According to the physics component documentation: specifying different collision behaviors for different objects is available through the CANNON.js JavaScript API.
For custom behaviour, You need to dig in to the Cannon.js documentation and find Your desired methods and classes. Nonetheless, implementing custom materials goes like this:

Cannon physics are computed/made in their own Cannon.world
Cannon Objecs have thier own materials
Cannon.ContactMaterial defines the physics, when those objects collide/interact with each other. It needs to be added to the Cannon.world as it is responsible for the physics.

With these, You can start by doing the following:

Get the CANNON.world reference: 
var world = $('a-scene')[0].systems.physics.world;
create two custom materials like this: 
var firstMaterial = new CANNON.Material(""firstMaterial"");
var secondMaterial = new CANNON.Material(""secondMaterial"");
apply the materials to the a-frame objects:
$('#cannon')[0].body.material=firstMaterial;
$('floor')[0].body.material=secondMaterial;
create a contact material and add it to the world 
var secondCM = new CANNON.ContactMaterial(firstMaterial,secondMaterial,
[restitution = 2]); 
world.addContactMaterial(secondCM);

Here You can find a working fiddle.

"
"
I'm currently trying to spawn an object in case there's an intersection between a laser object and a collidable object. I'm using a raycaster to detect the collision.
To spawn the object I want to do it only in the case there's a collision and also the user has pressed the trigger button.
I was thinking on creating a global variable pressed when the triggerdown event listener was pressed and in the raycaster-intersection event listener spawn an object only if this variable was set to true.
const pressed = false

AFRAME.registerComponent('laser', {
  init: function () {
    const laser = this.el.sceneEl.querySelector('#laser');
    laser.addEventListener('raycaster-intersection', function(event) {
      console.log('raycaster-intersection', event.detail.intersections[0].point)
      if (pressed) {
        console.log('spawn')
      }
    });
  }
})

AFRAME.registerComponent('spawner', {
  init: function () {
    const blockHand = this.el.sceneEl.querySelector('#blockHand');
    blockHand.addEventListener('triggerdown', function(event) {
      pressed = true
    });
    blockHand.addEventListener('triggerup', function(event) {
      pressed = false
    });
  }
})

I don't like to use global variables but I don't know how could I tackle this problem without them in this case.
Any suggestions?
Thanks!
","
Some solutions:

Do this.el.addState('pressed') and this.el.removeState('pressed') to update the state, and use this.el.is('pressed') to check it.
Combine the components together and store it as this.pressed.
A-Frame master build, soon 0.6.0, has a laser-controls component you can use https://aframe.io/docs/master/components/laser-controls.html so all you have to do is listen for click rather than having to listen for both raycaster intersection and trigger down. And you get all controller support for free.

"
"
I want to find the best setup to stream a VR experience over Wi-Fi.
To be able to play with different connection settings, it would be great if I could run a VR game or demo all the time, rather than having to be there and move things around.
This would allow to track the FPS more accurately and assess if the streaming is good enough.
Do you know how to do it?
What I've found so far is the ""Playlist mode"" but that does not include any automation
","
If you are developing in Unity you can try this: https://github.com/andrewjc/VRRecorder
"
"
Building a VR app in unity for iOS.  Added the GvrEditorEmulator into my project and as you can see the images that are sent to each eye are far more different than they should be. For example, the mountain in the left eye doesn't appear anywhere in the right eye.  As a result the 3D effect is not working when I put the phone into Google Cardboadr.  Anyone know how to fix this or why this might be happening?

","
Turns out this is a known bug with Google Cardboard:
https://forum.unity3d.com/threads/ios-cardboard-support-broken.461239/
"
"
Following the Minecraft demo. How can I make the blocks not to spawn in the same place? This is the script which spawns the boxes.
There are two approaches which I can think of: 

Create a tracking list of the coordinates which I've set up a box, and prevent creating one if the intersection point matches (or is close to a threshold). 
Check if the reticule matches a bounding box (Using Threejs' Box3, which I don't know how to use). 

Any ideas on the best way to approach this?
","
This is my answer to #2:
I created an array. This array will contain the points of evt.detail.intersection.point. Before inserting a new object, I apply Pythagoras (x, and z points), and compare it with a threshold. Only if it's above the threshold I allow it to continue, and store the new points in it.  
The whole code is below, with comments:
I used some TypeScript in here, I will not include it so it reaches a broader audience. 
I wrapped the intersection-spawn component with an ES2015 class just for the sake of separating the code. I don't know of a native way of creating A-Frame through ES2015. 
This is the main class ""intersection-spawn.js""
export default class IntersectionSpawn {
    constructor(lamp) {
        //The array that will track the position. 
        this.positionHistory = new Array();
        //The spacing which it will allow to span another light.
        this.minSpacing = 2;
        //Captures the class' *this* so it can be used in the 
        //Event Listener.
        const _this = this;
        //Dependency Injection. Injects the lamp class that manages
        //the lamp creation. 
        this.lamp = lamp;

        AFRAME.registerComponent('intersection-spawn', {
            schema: {
                default: '',
                parse: AFRAME.utils.styleParser.parse
            },

            init: function () {
                //This data comes from the HTML's <a-entity> attribute
                const data = this.data;
                //References the current element. This comes from A-Frame.
                const el = this.el;
                //Reducing the code a little bit.
                //This will create an event listener and pass it to the
                //intersection method.
                el.addEventListener(data.event, evt => {
                    _this.intersection(evt, el, data)
                });
            }
        });
    }

    //This takes care of create the element and insert it. 
    intersection(evt, el, data) {

        //Just a safeguard. If the event data doesn't contain 
        //the intersection property, then I can't do anything. 
        if (evt.detail.hasOwnProperty(""intersection"") === false)
            return;
        //Define a position object to keep hold of everything.
        //Note that in here I'm just selecting points x and z
        //because in my app, those are the only ones which interests
        //me. ""y"" is also available by using vt.detail.intersection.point.y
        let pos = {
            x: evt.detail.intersection.point.x,
            z: evt.detail.intersection.point.z
        };

        //If true then it continues, and adds the element. 
        //Otherwise exit. 
        if (!this.canAddToGrid(pos))
            return;

        //Creates a new lamp to be inserted. 
        const elem = this.lamp.generate(data, pos);
        el.sceneEl.appendChild(elem);
        this.appendToHistory(pos);
    }
    //Adds to the current history to be tracked. 
    appendToHistory(pos) {
        this.positionHistory.push(pos);
    }

    /**
     * Checks whether it's posisble to add to the grid or not.
     * This will check if the distance of the current insertion point
     * is equal or smaller to the distance to any of the cylinders. 
     * If that's the case, it will return false. Otherwise it will return
     * true.
     * The position of the current object to be inserted.
     * @param pos 
     */
    canAddToGrid(pos) {
        for (let position of this.positionHistory) {
            if (this.calcDistance(pos.x, pos.z, position.x, position.z) <= this.minSpacing) {
                return false;
            }
        }
        return true;
    }

    /**
     * Calculates the distance from the center of the lamp to the center 
     * of the insertion points.
     * 
     * @param x1 Position x of the object to be inserted
     * @param z1 Position z of the object to be inserted
     * @param x2 Position x of the object inside the array
     * @param z2 Position z of the object inside the array
     */
    calcDistance(x1, z1, x2, z2) {
        return Math.sqrt(Math.pow((x2 - x1), 2) + Math.pow((z2 - z1), 2));
    }

}

This is lamp.js (The one which creates the lamp object)and is injected to the intersectionSpawn class:
export default class Lamp {
    constructor() {

    }
    /**
     * Creates the Lamp. Right now it's a cylinder. 
     * @param pos The positions you want the lamp to be in.
     */
    create(pos) {
        let elem = (document.createElement(""a-cylinder""));
        elem.setAttribute('width', ""1"");
        elem.setAttribute('height', ""4"");
        elem.setAttribute('depth', ""1"");
        elem.setAttribute('position', `${pos.x} 0 ${pos.z}`);
        return elem;
    }

    /**
     * This works like a decorator. this was originaly in the intersection-spawn.
     * I do not know a lot what it does, but it's necessary for the element to work.
     * @param elem The generated element from the create(pos) method.
     * @param data Comes from A-Frame's data. 
     */
    AddAframeUtils(elem, data) {
        Object.keys(data).forEach(name => {
            if (name === 'event') { return; }
            AFRAME.utils.entity.setComponentProperty(elem, name, data[name]);
        });

        return elem;

    }

    /**
     * The public method which generates a fully functional element. 
     * @param data This comes from A-Frame's data.
     * @param position The position in which I want to create the element.
     */
    generate(data, position) {
        return this.AddAframeUtils(this.create(position), data);
    }
}










script.js which includes both classes:
import Lamp from './js/lamp/lamp';
import IntersectionSpawn from './js/components/intersection-spawn';

new IntersectionSpawn(new Lamp());

And now, the index.html:
<!DOCTYPE html>
<html lang=""en"">

<head>
    <meta charset=""UTF-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <meta http-equiv=""X-UA-Compatible"" content=""ie=edge"">
    <title>Document</title>
    <script src=""https://aframe.io/releases/0.5.0/aframe.min.js""></script>
    <script src=""https://rawgit.com/mayognaise/aframe-mouse-cursor-component/master/dist/aframe-mouse-cursor-component.min.js""></script>
    <script src=""script.js""></script>
</head>

<body>
    <a-scene>
        <a-sky color=""#ECECEC""></a-sky>
        <a-camera>
            <!-- We include the intersection-spawn in here:-->
            <a-cursor intersection-spawn=""event: click;""></a-cursor>
        </a-camera>
    </a-scene>
</body>

</html>

"
"
I am attempting to create an input field in a google daydream app using the latest Google VR SDK for Unity and Unity 5.4.2. 
I am wondering if someone may have insight into integrating the Daydream Keyboard. I can place a canvas and input field but when I run the application on the Pixel and Daydream View, and select the inputfield the standard android keyboard renders. 
I am not sure if the Daydream Keyboard needs to be integrated separately in unity. 
The Daydream keyboard is installed and is functional on published daydream apps. 
","
Nope, the official Daydream Keyboard is not Unity ready, see https://forum.unity3d.com/threads/daydream-keyboard-in-unity.441448/
"
"
I am using Android Studio to build a Android vr app with Google DayDream android vr sdk. 
I would like my users to be able to interact with the vr environment with their controllers that come with the vr headset.
But when I looked at the official documentation, I noticed that the controller support seems to be available only for Unity and Unreal. So I am wondering if I am still able to display a controller 3D model and its laser visualization with only the Android sdk. 
Link to controller support info for Unity and Unreal
If not, is it recommended that I use Unity for my Android vr app development.
","
I would recommend using the C++ version of the of the arm model that is included as part of Unreal. The arm model has no dependencies on Unreal code, so you should be able to integrate it into your app. If you are currently writing a java only app, this would require you to use the android NDK. Alternatively, you could look at porting the code to Java.
The arm model will only give you the position/orientation for the controller. If you do this, you will still need to render the controller and laser yourself in your application. You can pull the art assets from Unreal or Unity to do this.
Daydream development in Unity is fully supported, but you'll have to decide for yourself what is the best platform to use for your needs based on what type of development environment you prefer.
"
"
I have replaced the Steam VR loading scene with a Skybox that I created, but when loading in, there is still this floating square Steam VR panel with a few moving bars on it. The loading scene is meant to be scenic so this box really removes that effect. Does anybody know how to remove / disable it? 
","
Set the SteamVR_LoadLevel LoadingScreentexture property to a texture so it overrides the default texture (there doesn't seem to be a way to select no texture). 
Set the SteamVR_LoadLevel Loading Screen Width In Meters to 0
This will render your loading texture at a size of 0 making it look invisible.
The texture you set can probably be super small and single color because you're not going to ever see it.
If you're doing this in code it'll be something like the following:
public Texture LoadingScreenTexture;

public void LevelLoader()
{
    var loader = new GameObject(""loader"").AddComponent<SteamVR_LoadLevel>();

    // Loading progress bar
    loader.loadingScreen = LoadingScreenTexture;

    loader.loadingScreenWidthInMeters = 0;
}

"
"







Closed. This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet Stack Overflow guidelines. It is not currently accepting answers.
                                
                            











 We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.


Closed 7 years ago.







                        Improve this question
                    



I want to import a 3d model/mesh inside a game. Kinda like import a mesh from file explorer kinda thing.
Is there any good way to do this? Without much coding.
","
I'm assuming you want to load custom 3D meshes (like .fbx or .obj) that weren't preprocesed by editor first and are not in game folders, and you want to do it at runtime while game is playing. 
What I get know from recent reaseach in this topic: UE4 dosen't support any kind of API for loading such meshes, you could legaly use in your game at runtime. There are, of course, existent API for that but it's part of Editor codebase and you are not allowed to use it in game directly, it is restricted by current licence (look at Epic Dev response in this thread).
In this case you could use thrid-party loaders and integrate them in your project or write your own loaders. Loaded meshes can be displayed using Procedural Mesh Component.
"
"
Well I am doing a VR application that displays some images. I am using VrPanoramaView.
Any way when I launch the activity to open it in vr mode? Instead of clicking on the button of VR of the panoView 
Here's my code:
public class MainVrActivity extends Activity
{

    private VrPanoramaView panoWidgetView;
    private ImageLoaderTask backgroundImageLoaderTask;
    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main_vr);

        panoWidgetView = (VrPanoramaView) findViewById(R.id.pano_view);

        loadPanoImage();

    }


    @Override
    public void onPause() {
        panoWidgetView.pauseRendering();
        super.onPause();
    }

    @Override
    public void onResume() {
        panoWidgetView.resumeRendering();
        super.onResume();
    }

    @Override
    public void onDestroy() {
        // Destroy the widget and free memory.
        panoWidgetView.shutdown();
        super.onDestroy();
    }

    private synchronized void loadPanoImage() {
        ImageLoaderTask task = backgroundImageLoaderTask;
        if (task != null && !task.isCancelled()) {
            // Cancel any task from a previous loading.
            task.cancel(true);
        }

        // pass in the name of the image to load from assets.
        VrPanoramaView.Options viewOptions = new VrPanoramaView.Options();
        viewOptions.inputType = VrPanoramaView.Options.TYPE_STEREO_OVER_UNDER;

        // use the name of the image in the assets/ directory.
        String panoImageName = ""Converted.jpg"";

        // create the task passing the widget view and call execute to start.
        task = new ImageLoaderTask(panoWidgetView, viewOptions, panoImageName);
        task.execute(getAssets());
        backgroundImageLoaderTask = task;
    }

}

    public class ImageLoaderTask extends AsyncTask<AssetManager, Void, Bitmap> {

    /*We use a WeakReference for the VrPanoramaView since the view could be destroyed while loading the image.
     *A common cause of this is rotating the phone to another orientation. By using a weak reference,
     *the object can be garbage collected immediately instead of waiting for this async task to be destroyed.
     */
    private static final String TAG = ""ImageLoaderTask"";
    private final String assetName;
    private final WeakReference<VrPanoramaView> viewReference;
    private final VrPanoramaView.Options viewOptions;

    private static WeakReference<Bitmap> lastBitmap = new WeakReference<>(null);
    private static String lastName;

    @Override
    protected Bitmap doInBackground(AssetManager... params) {
        AssetManager assetManager = params[0];

        if (assetName.equals(lastName) && lastBitmap.get() != null) {
            return lastBitmap.get();
        }

        try(InputStream istr = assetManager.open(assetName)) {
            Bitmap b = BitmapFactory.decodeStream(istr);
            lastBitmap = new WeakReference<>(b);
            lastName = assetName;
            return b;
        } catch (IOException e) {
            Log.e(TAG, ""Could not decode default bitmap: "" + e);
            return null;
        }
    }

    //Displaying the image
    @Override
    protected void onPostExecute(Bitmap bitmap) {
        final VrPanoramaView vw = viewReference.get();
        if (vw != null && bitmap != null) {
            vw.loadImageFromBitmap(bitmap, viewOptions);
        }
    }

    public ImageLoaderTask(VrPanoramaView view, VrPanoramaView.Options viewOptions, String assetName) {
        viewReference = new WeakReference<>(view);
        this.viewOptions = viewOptions;
        this.assetName = assetName;
    }

}

","
You can set the display mode with VrWidgetView#setDisplayMode:        

panoWidgetView.setDisplayMode(VrWidgetView.DisplayMode.FULLSCREEN_STEREO);

"
"
I'm developing a VR app in Unity for the Samsung Gear VR and I'm trying to implement a pointer so the user can interact with the objects in the scene. When you look at distant objects it looks fine, but when you focus on close objects (which is highly needed for the app mechanics) the pointer appears to be duplicated, so you need to center the desired object in the middle of the points :P
What I've tried
-Using the GvrReticlePointer that comes with the GoogleVR package for cardboard
-Creating my own pointer by adding a canvas to the main camera with an image in the center
-Changing some of the Camera settings like field of view, stereo separation, etc.
-Configure my phone via a QR code https://i.sstatic.net/M0JXn.jpg
Steps to reproduce (With canvas added to camera)
1.- Create a simple scene with a few objects to look at in Unity
2.- Set build settings for android
3.- Configure player settings to enable ""Virtiual Reality Supported""
4.- Add Oculus as Virtual Reality SDK
5.- Set package name and minimum API level
6.- Add a canvas to the camera
7.- Add an image to the canvas, a cross will do the job
Observations
I'm using Unity 5.6.0b10 since google cardboard's site recommends using this version for the GoogleVR package. And I'm using the Samsung Gear VR with a Samsung Galaxy S6 edge + phone.
","
Solved
Apparently this is a well documented issue called voluntary Diplopia, and it's a human bug not a software one (read here, Unity's documentation, section The Reticle Interaction in VR).
The problem is trying to put the reticle at a fixed point in the user interface, like traditional 3D games. When looking at closer objects in VR this is going to cause this seeing double problem.
The solution is to position the reticle at the point in the 3D space the user is looking at. If he's looking closer, the reticle is drawn closer. Of course now you also have to scale the reticle accordingly, so the users can see it the same size no matter where they're looking at. 
Unity also provides some example scripts about this, you can find them in the assets store, is called VR Samples.
Now I have performance issues (I'm working on mobile platforms): sometimes, when you turn your head fast you can see the reticle where it was drawn before. But looks way better than the double reticle version.
"
"
I created a a-scene with some objects to drag. The final purpose is exactly what aframe-click-drag-component does. Unfortunately, this component is not compatible with the last version of A-Frame.
I created a custom component.
AFRAME.registerComponent('draggable', {
    init: function () {
        /* Some code */
    }
});

I use the aframe-mouse-cursor-component to be able to get the mouseenter and mouseleave events on the draggable object, and detect when the mouse position allows the user to select the object. 
I added an EventListener on document.body to know when the dragging starts:
document.body.addEventListener('mousedown', function (e) {
    // start dragging
});

I continuously update a global variable to update the mouse position when a mousemove occurs:
document.addEventListener('DOMContentLoaded', function () {
    document.body.addEventListener('mousemove', function (e) {
        window.mouseX = e.clientX;
        window.mouseY = e.clientY;
    });
});

This way, I can easily get the position of the mouse during the dragging. But I do not know how to convert the position of the mouse on the client to a position in the Virtual Reality (restricted to a 2D plan to make it possible).
I solved this issue by using the raycaster coming from the cursor in the middle of the a-camera, but I want to drag the objects with the mouse-cursor, and this component does not have a raycaster.
I also tried to use some maths to convert the mouse coordinates to a coordinates set relative to the camera, without success (essentially because of the screen size which can vary).
What solutions are available? I would like to update the click-drag or the mouse-cursor, but I have no knowledge of THREE.js.
","
See https://github.com/mayognaise/aframe-mouse-cursor-component or https://github.com/mrdoob/three.js/blob/dev/examples/js/controls/DragControls.js or https://www.npmjs.com/package/aframe-click-drag-component for examples
The main chunk of code is like:
    canvas.addEventListener( 'mousemove', function () {

      var mouse = new THREE.Vector2();

      var rect = canvas.getBoundingClientRect();

      mouse.x = ( (event.clientX - rect.left) / rect.width ) * 2 - 1;
      mouse.y = - ( (event.clientY - rect.top) / rect.height ) * 2 + 1;

      raycaster.setFromCamera( mouse, camera );
    }, false);

"
"
I'm trying to use Google Cardboard SDK with Unity 5.4.1f (Free). After importing the unitypackage with SDK, I tried to run the demo scene, but I see only freezing camera. I can't look around with mouse, just static picture. And I see the errors in Console and Inspector tab. Here is the screenshot: 

I tried to import different versions of Cardboard SDK and also tried Unity 5.3, but got the same errors. Please help me, I don't know what's wrong with it.
","
The project is likely corrupted. Make sure that the version of Unity you have is > the version used to build the Google Cardboard SDK. 
If that's not the problem, close Unity, delete the current Google Cardboard SDK project and plugin. Download fresh version here.
After downloading it, import the GoogleVRForUnity.unitypackage file. This should fix your problem.
EDIT:
Things changed in the latest Google VR SDK update. The VR Demo Scenes now require VR
Technical preview to work. You can get the latest Unity version for VR here. Scroll down to the end of the page.
The latest version 5.4.2f2-GVR10.
Once you download this version of Unity UNITY_HAS_GOOGLEVR will be defined in Unity and you can now run the ControllerDemo demo.
Note:
You can run the DemoScene scene in Assets\GoogleVR\DemoScenes\HeadsetDemo directory without downloading the Unity GVR version. Right click on the Project tab then click Reimport All.
You just cannot run the ControllerDemo scene in Assets\GoogleVR\DemoScenes directory without downloading the Unity GVR version.
"
"
for a project I am working on I will need to automatically decide if a video is a VR (360) video, and if so what format it is. Is there any way to tell? I was thinking the metadata but could not find any information on this.
","
Checking size is pointless, most properly encoded movies are using standard sizes like 1080p (1920x1080), WQHD (2560×1440) or 4K (3840×2160) because of better hardware decoding. To do that they don't have square pixels. Therefore you shouldn't guess anything by ratio.
What you should do, is check the presence of zenith and nadir. That is check for the topmost and bottommost region of image if it is the same color (assuming the most standard equirectangular projection).
This approach will need some adjusting if you have stereoscopy involved. You would have to repeat this procedure for each eye region. As a bonus, you can also deduce some stereoscopy types - for example you could differentiate top-bottom, mono and left-right. Unfortunately you couldn't guess which image is for which eye, so you would have to assume the more common situation where the left eye is usually the top or left one in the image.
"
"
I created a VR experience in Unreal Engine, and inside the VR world, there is a button which can open an external exe, which is also a VR experence. It's like a VR menu room (Unreal application) and a mini VR game (not created by Unreal and is an external exe). When you play the mini game, Unreal VR menu is still open at the back.
I tried different ways to open external exe, such as:
FPlatformProcess::CreateProc

FMonitoredProcess

FInteractiveProcess

They can all successfully launch external exe, but the problem is, when the external exe is done playing and closed, the Unreal VR menu application will lose vision tracking of Oculus. This problem only happens if the external exe is a VR application. I tried to launch a regular exe, such as browser, and there is no lose tracking problem when I ""alt tab"" to jump back to Unreal VR application.
This is what I got from output log:

LogHMD: Vision Tracking Acquired
LogHMD: Lost Vision Tracking

Is this a bug of Unreal? Is there any way which can let Oculus retracked by Unreal after running an external VR exe?
Many thanks!
","
Oculus drivers should only handle one process at a time, so whenever that new application is starting, Unreal should give up its connection to the service because someone else is requesting the HMD to render those VR frames now. As a dedicated output, both applications issuing frames at the same time can't be a good thing.
Regarding your context, I suggest you try invoking the ""Stereo On"" Console Command to re-enable your HMD, though you might want to also try the Oculus specific ""HMD Enable"" command before hand.
Personal concern: Keeping that menu in the background must be consuming resources as well and, in VR, that's an optimization issue you might have to address eventually.
"
"
I'm making a bow and arrow game in Unity and C# with virtual reality. I want the arrow to rest on the bow's handle before you shoot. Right now I have a box collider on the handle and another collider on the arrow. It works OK, until you push the arrow through the handle. It glitches out for a sec then it just passes through (like a no clip).
In my VR class we learned that you can add a normal force or a ""push back force"" to an object to keep other objects from passing through it. I don't want this to literally push back the arrow though (that would remove the realism). Is there a way to add a force field to an object? Where other objects are repelled very lightly at all times instead of a one-shot burst of force?
Here's code I've already tried that helps a little but not much. This is a script attached to the bow:
private void OnCollisionEnter(Collision collision) {

    if (collision.gameObject.tag == ""Arrow"") {
        Rigidbody arrowBody = collision.gameObject.GetComponent<Rigidbody>();
        arrowBody.AddForce(this.transform.TransformDirection(Vector3.left) * 100f);
    }

}

I should probably add that the arrow is held in a ""hand"" using  a fixed joint, so adding a second fixed joint to the bow handle is not an option (the hand's joint ends up failing and the arrow falls to the ground without being held by either joint).
","
I ended up changing the parent transform of the arrow to the bow upon collision, then letting it go from the controller's fixed joint that was holding it. It binds perfectly now.
Not much code to provide:
private void OnCollisionEnter(Collision col)
{
    if (col.gameObject.tag == ""Bow"")
    {
            this.transform.parent = col.gameObject.transform;
            GameObject.Find(""Controller"").GetComponent<FixedJoint>().connectedBody = null;      
    }
}

"
"
I'd like to create a virtual space where I want to mix 3D elements and html content. I have seem some demos with WebVR but didn't work for me. WebAR is also an interesting option.
Thanks
","
Unfortunately, this isn't possible at the moment, because

there is no proper way to get your hands on the rastered output of the HTML in order to use it as a texture in WebGL and 
there is no way to get any html-element shown in a WebVR-context

There is certainly work being done in both directions, but your best bets would probably be a-frame (https://aframe.io/) or reactVR (https://developer.oculus.com/blog/introducing-the-react-vr-pre-release/) which both at least have a bit of that html-feeling to it and can handle text pretty well (nothing compared to the power of html+css, but they will get there at some point).
Another option is to use a library like html2canvas (https://html2canvas.hertzen.com/) to get some rendered output from a piece of html which you can then use as a texture. Also has it's limits but might be worth a try...
Have fun!
"
"
There's already a question and it has an answer here, however it doesn't work the way I expect it. 
# Assume my radius is 1 for simplicity

x = cos(s) * sin(t)
y = sin(s) * sin(t)
z = cos(t)

When t=0, regardless of my s, 
(x,y,z)=(0,0,1)  

# Since sin 0 = 0 on x 
# and y and z is independent of s

So here's how my world is

But actually when s increases, the point on the sphere varies, doesn't remain at (0,0,1). For eg. if my s=(-45)deg and t=0, point on sphere should be (0,0.707,0.707) right?
UPDATE: Here's what I need:
(s,t)   |  (x,y,z)
---------------
(0,0)   |  (0,0,1)
(45,0)  |  (.707,0,0.707)
(90,0)  |  (1,0,0)
(180,0) |  (0,0,-1)
(270,0) |  (-1,0,0)
(0,-45) | (0,0.707,0.707)
(0,45)  | (0,-0.707,0.707)

But I don't get those results from the above equations...! What do I do?
","
with your formula t=0 means that you are at pole so the radius is zero. No matter what s is the output should be always (x,y,z)=(0,0,1). If you need the standard spherical coordinates instead use this:
x = cos(s) * cos(t)
y = sin(s) * cos(t)
z =          sin(t)

s = <0,360> [deg]
t = <-90,+90> [deg]

for (s=45deg,t=0deg) it should return (x,y,z)=(0.707,0.707,0.000)
PS. I am not sure why you have mixed coordinates y,z instead of x,y in OP. 
[Edit1]
To match your image reference frame try to use these:
x = sin(s) * cos(t)
y =        - sin(t)
z = cos(s) * cos(t)

s = <0,360> [deg]
t = <-90,+90> [deg]

"
"
Unity
I create two collider effects and want them can have some function while them collider each other . but i don't know how to make two particle collider.
How to make fire and water can collider each and fire will be put out by  water. 
Thank for help!!!!
","
The unity particle system can collide with colliders(objects) in the scene if collision is enebled for particle system.
But particle systems do not collide with each other. Particles have no volume, so it's not really possible.
You cannot use built-in particle system for fire and water and have them collide with each other. However, you can create a custom particle system using actual objects having colliders which can work as you expect.
"
"
I'm creating my firs VR app i unity for google cardboard. I noticed VR support setting in the inspector. I searched a little bit and it isn't clear for me if I should enable it for google cardboard. Here is the link for unity manual about it. Thanks for help!
","
For a while you should not enable VR support since Unity has no native support for Cardboard yet, although it was announced on Vision Summit 2016 for future versions.
By now, you should only use the Unity package with the appropriated prefabs for Google Cardboard available at https://developers.google.com/vr/unity/download.
"
"
I want to develop a simple VR environment on iOS in order to play around with different interaction forms using Cardboard VR. This might include the need to access iPhones camera and microphone. 
This is my very first experience with VR. On Google VR page there is an SDK for Unity and an SDK for iOS. Which path should I go? 
I know that I can create a Unity application and deploy it on iOS. But is it possible then easily change/customise the generated code?
","
If you want to use Unity then go for the Unity SDK. The iOS SDK is for developing natively in iOS.
When you build for iOS using Unity, Unity will generate an XCode project which you can then run. A few classes of this project will be readable (such as UnityAppController.mm, etc.), but most of the code will be auto-generated and therefore practically impossible to follow. See an example below:

So all the app's functionality should be developed directly in Unity
"
"
Im trying to use cardboard view (found example in internet)
private class CardboardOverlayEyeView extends ViewGroup {
        private final ImageView imageView;
        private final TextView textView;
        private float offset;

        public CardboardOverlayEyeView(Context context, AttributeSet attrs) {
            super(context, attrs);
            imageView = new ImageView(context, attrs);
            imageView.setScaleType(ImageView.ScaleType.CENTER_INSIDE);
            imageView.setAdjustViewBounds(true);  // Preserve aspect ratio.
            addView(imageView);

            textView = new TextView(context, attrs);
            textView.setTextSize(TypedValue.COMPLEX_UNIT_DIP, 14.0f);
            textView.setTypeface(textView.getTypeface(), Typeface.BOLD);
            textView.setGravity(Gravity.CENTER);
            textView.setShadowLayer(3.0f, 0.0f, 0.0f, Color.DKGRAY);
            addView(textView);
        }

        public void setColor(int color) {
            imageView.setColorFilter(color);
            textView.setTextColor(color);
        }

        public void setText(String text) {
            textView.setText(text);
        }

        public void setTextViewAlpha(float alpha) {
            textView.setAlpha(alpha);
        }

        public void setOffset(float offset) {
            this.offset = offset;
        }

        @Override
        protected void onLayout(boolean changed, int left, int top, int right, int bottom) {
           //bla bla view positions.
        }
    }
}

I want to  add menu here for gamepad (via bluetooth). I just write another layout.xml with my menu and in CardboardOverlayEyeView using inflate i add this layout to my current viewGroup. But for correct work - extends ViewGroup was changed to extends RelativeLayout.
My menu showing good. There are few buttons like ""point 1, point 2, point 3""
when im using controls on joystick (UP/DOWN) its on leftView showing focus (like another color for button)- and its good.But in right view no focus.Also But if im using control on joystic left/right - i can switch from left to right view . 
I want to remove switch and if leftView has focused button (with another color) i also want this steps in rightView. Can any one help me? how to do it.

picture
","
Finnaly i did it.
simple part of code . Where we need array of buttons as a copy of main buttons from left and right view.
public void setFocusedButton() {
        for(int i=0; i<mLeftView.btns.length;i++){
            if(mLeftView.btns[i].isFocused() || mRightView.btns[i].isFocused()) {
                mLeftView.btns[i].setBackgroundResource(R.drawable.button_pressed);
                mRightView.btns[i].setBackgroundResource(R.drawable.button_pressed);
            }else {
                mLeftView.btns[i].setBackgroundResource(R.drawable.button);
                mRightView.btns[i].setBackgroundResource(R.drawable.button);
            }
        }

    }

its not full focused. But its looks great.
"
"
I am writing an android app using VrPanoramaView
I know there are two ways to navigate in VR apps

Use the single button on the cardboard as a click
Treat looking at something for a long period of time as a click

How can i implement navigation using one of these so that if user clicks button panoview will skip to the next picture?
Below is the sample Vr PanoramaView code
**
 * A basic PanoWidget Activity to load panorama images from disk. It will load a test image by
 * default. It can also load an arbitrary image from disk using:
 *   adb shell am start -a ""android.intent.action.VIEW"" \
 *     -n ""com.google.vr.sdk.samples.simplepanowidget/.SimpleVrPanoramaActivity"" \
 *     -d ""/sdcard/FILENAME.JPG""
 *
 * To load stereo images, ""--ei inputType 2"" can be used to pass in an integer extra which will set
 * VrPanoramaView.Options.inputType.
 */
public class SimpleVrPanoramaActivity extends Activity {
  InputStream istr = null;

  private static final String TAG = SimpleVrPanoramaActivity.class.getSimpleName();
  /** Actual panorama widget. **/
  private VrPanoramaView panoWidgetView;
  /**
   * Arbitrary variable to track load status. In this example, this variable should only be accessed
   * on the UI thread. In a real app, this variable would be code that performs some UI actions when
   * the panorama is fully loaded.
   */
  public boolean loadImageSuccessful;
  /** Tracks the file to be loaded across the lifetime of this app. **/
  private Uri fileUri;
  /** Configuration information for the panorama. **/
  private Options panoOptions = new Options();
  private ImageLoaderTask backgroundImageLoaderTask;

  /**
   * Called when the app is launched via the app icon or an intent using the adb command above. This
   * initializes the app and loads the image to render.
   */
  @Override
  protected void onCreate(Bundle savedInstanceState) {
    super.onCreate(savedInstanceState);
    setContentView(R.layout.main_layout);

    // Make the source link clickable.
    TextView sourceText = (TextView) findViewById(R.id.source);
    sourceText.setText(Html.fromHtml(getString(R.string.source)));
    sourceText.setMovementMethod(LinkMovementMethod.getInstance());

    panoWidgetView = (VrPanoramaView) findViewById(R.id.pano_view);
    panoWidgetView.setEventListener(new ActivityEventListener());

    // Initial launch of the app or an Activity recreation due to rotation.
    handleIntent(getIntent());
  }

  /**
   * Called when the Activity is already running and it's given a new intent.
   */
  @Override
  protected void onNewIntent(Intent intent) {
    Log.i(TAG, this.hashCode() + "".onNewIntent()"");
    // Save the intent. This allows the getIntent() call in onCreate() to use this new Intent during
    // future invocations.
    setIntent(intent);
    // Load the new image.
    handleIntent(intent);
  }

  /**
   * Load custom images based on the Intent or load the default image. See the Javadoc for this
   * class for information on generating a custom intent via adb.
   */
  private void handleIntent(Intent intent) {
    // Determine if the Intent contains a file to load.
    if (Intent.ACTION_VIEW.equals(intent.getAction())) {
      Log.i(TAG, ""ACTION_VIEW Intent recieved"");

      fileUri = intent.getData();
      if (fileUri == null) {
        Log.w(TAG, ""No data uri specified. Use \""-d /path/filename\""."");
      } else {
        Log.i(TAG, ""Using file "" + fileUri.toString());
      }

      panoOptions.inputType = intent.getIntExtra(""inputType"", Options.TYPE_MONO);
      Log.i(TAG, ""Options.inputType = "" + panoOptions.inputType);
    } else {
      Log.i(TAG, ""Intent is not ACTION_VIEW. Using default pano image."");
      fileUri = null;
      panoOptions.inputType = Options.TYPE_MONO;
    }

    // Load the bitmap in a background thread to avoid blocking the UI thread. This operation can
    // take 100s of milliseconds.
    if (backgroundImageLoaderTask != null) {
      // Cancel any task from a previous intent sent to this activity.
      backgroundImageLoaderTask.cancel(true);
    }
    backgroundImageLoaderTask = new ImageLoaderTask();
    backgroundImageLoaderTask.execute(Pair.create(fileUri, panoOptions));
  }

  @Override
  protected void onPause() {
    panoWidgetView.pauseRendering();
    super.onPause();
  }

  @Override
  protected void onResume() {
    super.onResume();
    panoWidgetView.resumeRendering();
  }

  @Override
  protected void onDestroy() {
    // Destroy the widget and free memory.
    panoWidgetView.shutdown();

    // The background task has a 5 second timeout so it can potentially stay alive for 5 seconds
    // after the activity is destroyed unless it is explicitly cancelled.
    if (backgroundImageLoaderTask != null) {
      backgroundImageLoaderTask.cancel(true);
    }
    super.onDestroy();
  }

  /**
   * Helper class to manage threading.
   */
  class ImageLoaderTask extends AsyncTask<Pair<Uri, Options>, Void, Boolean> {

    /**
     * Reads the bitmap from disk in the background and waits until it's loaded by pano widget.
     */
    @Override
    protected Boolean doInBackground(Pair<Uri, Options>... fileInformation) {
      Options panoOptions = null;  // It's safe to use null VrPanoramaView.Options.
      InputStream istr = null;
      if (fileInformation == null || fileInformation.length < 1
          || fileInformation[0] == null || fileInformation[0].first == null) {
        AssetManager assetManager = getAssets();
        try {
          istr=new URL(""https://s18.postimg.org/rnoymr5o9/andes.jpg"").openStream();
          //istr = assetManager.open(""andes.jpg"");
          panoOptions = new Options();
          panoOptions.inputType = Options.TYPE_STEREO_OVER_UNDER;
        } catch (IOException e) {
          Log.e(TAG, ""Could not decode default bitmap: "" + e);
          return false;
        }
      } else {
        try {
          istr = new FileInputStream(new File(fileInformation[0].first.getPath()));
          panoOptions = fileInformation[0].second;
        } catch (IOException e) {
          Log.e(TAG, ""Could not load file: "" + e);
          return false;
        }
      }

      try {
        istr.close();
      } catch (IOException e) {
        Log.e(TAG, ""Could not close input stream: "" + e);
      }

      return true;
    }

    @Override
    protected void onPostExecute(Boolean aBoolean) {
      super.onPostExecute(aBoolean);

      panoWidgetView.loadImageFromBitmap(BitmapFactory.decodeStream(istr), panoOptions);

    }
  }

  /**
   * Listen to the important events from widget.
   */
  private class ActivityEventListener extends VrPanoramaEventListener {
    /**
     * Called by pano widget on the UI thread when it's done loading the image.
     */
    @Override
    public void onLoadSuccess() {
      loadImageSuccessful = true;
    }

    /**
     * Called by pano widget on the UI thread on any asynchronous error.
     */
    @Override
    public void onLoadError(String errorMessage) {
      loadImageSuccessful = false;
      Toast.makeText(
          SimpleVrPanoramaActivity.this, ""Error loading pano: "" + errorMessage, Toast.LENGTH_LONG)
          .show();
      Log.e(TAG, ""Error loading pano: "" + errorMessage);
    }
  }
}

","

You need to declare Sensors
private SensorManager mSensorManager;
private OrientationSensor mOrientationSensor;
mSensorManager = (SensorManager) this.getSystemService(SENSOR_SERVICE);
mOrientationSensor = new OrientationSensor(this, mSensorManager, OrientationSensor.MODE_LOOK_THROUGH);

Make your activity or fragment implements SensorEventListener
Create vector to retreive rotation values
 private float[] mHeadRotation = new float[2];

In onSensorChanged
@Override
public void onSensorChanged(SensorEvent sensorEvent) {
    mUiPanoWidgetView.getHeadRotation(mHeadRotation);
    updateReticule();
}

Do what you want depending on where you are on picture. Example:
private void updateReticule() {
    if(mHeadRotation[1] > -20 && mHeadRotation[1] < 20 && mHeadRotation[0] > -15 && mHeadRotation[0] < 15){
        showButton();
    } else {
        showReticule();
    }
}

showButton() can show or hide an arrow (ImageButton) in the center of the screen. Then you set an OnClickListener on the arrow. If users click, then you can go to next picture.

Hope it will help you :)
"
"







Closed. This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet Stack Overflow guidelines. It is not currently accepting answers.
                                
                            











 We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.


Closed 7 years ago.







                        Improve this question
                    



Can someone guide me to create a VR photo viewer in Unity using the Google VR (GVR) sdk. Thanks
","
Steps in Unity:


Add a new Sphere GameObject.
In your assets Create new Material-> change its albedo texture with your 360 photo.
Grab the Material and apply it to your Sphere.
Position the main Camera/Player inside the Sphere .


Voila
"
"
i want to work with web V.R. in android studio. i found that i should use gecko view or chrome view or something like them. but i cant find anything in the web. all the references are belong to eclipse and don't work with android studio. 
alternatively i put my files in external storage and load them with chrome but it didn't work properly.
https://crosswalk-project.org/documentation/downloads.html
https://github.com/pwnall/chromeview
I use these but i cant run my project
","
you can import eclipse library project in android studio as well. Please refer to this link
"
"







This question already has answers here:
                                
                            




Enable/Disable VR from code

                                (5 answers)
                            

Closed 7 years ago.



last week I tried to integrate my Unity project to Daydream, the integration went fine.
Now I am trying to seperate the first scenes (which include login process so I need the android soft-keyboard) from the rest of the application - I want the application to open like any android-non-vr application and to switch to VR mode only after the first two scenes.
I read in some answer by Unity staff member that by adding ""none"" before ""Daydream"" in the ""Virtual Reality Supported"" settings the application starts in a non-VR mode.
Virtual Reality Supported settings

and now I am trying to switch in runtime to VR mode through C# script.
By now I tried to change ""VR Mode Enable"" of the GvrViewerMain in Runtime but It doesn't switching the application to VR Mode.
Does anyone know how/if it can be done?
","
Change your minimum api level in build setting to Android 7.0 'Nougat'.
"
"
I want to play a stereo 360 degree video in virtual reality in Unity on an Android. So far I have been doing some research and I have two cameras for the right and left eye with each a sphere around them. I also need a custom shader to make the image render on the inside of the sphere. I have the upper half of the image showing on one sphere by setting the y-tiling to 0.5 and the lower half shows on the other sphere with y-tiling 0.5 and y-offset 0.5. With this I can show a 3D 360 degree image already correct. The whole idea is from this tutorial.
Now for video, I need control over the Video speed so it turned out I need the VideoPlayer from the new Unity 5.6 beta. Now my setup so far would require the Video Player to play the video on both spheres with one sphere playing the upper part (one eye) and the other video playing the lower part (other eye).
Here is my problem: I don't know how to get the video Player to play the same video on two different materials (since they have different tiling values). Is there a way to do that?
I got a hint that I could use the same material and achieve the tiling effect via UV, but I don't know how that works and I haven't even got the video player to play the video on two objects using the same material on both of them. I have a screenshot of that here. The Right sphere just has the material videoMaterial. No tiling since I'd have to do that via UV.
Which way to go and how to do it? Am I on the right way here?
","

Am I on the right way here?

Almost but you are currently using Renderer  and Material instead of RenderTexture and Material.

Which way to go and how to do it?

You need to use RenderTexture for this. Basically, you render the Video to RenderTexture then you assign that Texture to the material of both Spheres.
1.Create a RenderTexture and assign it to the VideoPlayer.
2.Create two materials for the spheres. 
3.Set VideoPlayer.renderMode to VideoRenderMode.RenderTexture;
4.Set the Texture of both Spheres to the Texture from the RenderTexture
5.Prepare and Play Video.
The code below is doing that exact thing. It should work out of the box. The only thing you need to do is to modify the tiling and offset of each material to your needs. 
You should also comment out:
leftSphere = createSphere(""LeftEye"", new Vector3(-5f, 0f, 0f), new Vector3(4f, 4f, 4f));
rightSphere = createSphere(""RightEye"", new Vector3(5f, 0f, 0f), new Vector3(4f, 4f, 4f));

then use a Sphere imported from any 3D application. That line of code is only there for testing purposes and it's not a good idea to play video with Unity's sphere because the spheres don't have enough details to make the video smooth.
using UnityEngine;
using UnityEngine.Video;

public class StereoscopicVideoPlayer : MonoBehaviour
{
    RenderTexture renderTexture;

    Material leftSphereMat;
    Material rightSphereMat;

    public GameObject leftSphere;
    public GameObject rightSphere;

    private VideoPlayer videoPlayer;

    //Audio
    private AudioSource audioSource;

    void Start()
    {
        //Create Render Texture
        renderTexture = createRenderTexture();

        //Create Left and Right Sphere Materials
        leftSphereMat = createMaterial();
        rightSphereMat = createMaterial();

        //Create the Left and Right Sphere Spheres
        leftSphere = createSphere(""LeftEye"", new Vector3(-5f, 0f, 0f), new Vector3(4f, 4f, 4f));
        rightSphere = createSphere(""RightEye"", new Vector3(5f, 0f, 0f), new Vector3(4f, 4f, 4f));

        //Assign material to the Spheres
        leftSphere.GetComponent<MeshRenderer>().material = leftSphereMat;
        rightSphere.GetComponent<MeshRenderer>().material = rightSphereMat;

        //Add VideoPlayer to the GameObject
        videoPlayer = gameObject.AddComponent<VideoPlayer>();

        //Add AudioSource
        audioSource = gameObject.AddComponent<AudioSource>();

        //Disable Play on Awake for both Video and Audio
        videoPlayer.playOnAwake = false;
        audioSource.playOnAwake = false;

        // We want to play from url
        videoPlayer.source = VideoSource.Url;
        videoPlayer.url = ""http://www.quirksmode.org/html5/videos/big_buck_bunny.mp4"";

        //Set Audio Output to AudioSource
        videoPlayer.audioOutputMode = VideoAudioOutputMode.AudioSource;

        //Assign the Audio from Video to AudioSource to be played
        videoPlayer.EnableAudioTrack(0, true);
        videoPlayer.SetTargetAudioSource(0, audioSource);

        //Set the mode of output to be RenderTexture
        videoPlayer.renderMode = VideoRenderMode.RenderTexture;

        //Set the RenderTexture to store the images to
        videoPlayer.targetTexture = renderTexture;

        //Set the Texture of both Spheres to the Texture from the RenderTexture
        assignTextureToSphere();

        //Prepare Video to prevent Buffering
        videoPlayer.Prepare();

        //Subscribe to prepareCompleted event
        videoPlayer.prepareCompleted += OnVideoPrepared;
    }


    RenderTexture createRenderTexture()
    {

        RenderTexture rd = new RenderTexture(1024, 1024, 16, RenderTextureFormat.ARGB32);
        rd.Create();
        return rd;
    }

    Material createMaterial()
    {
        return new Material(Shader.Find(""Specular""));
    }

    void assignTextureToSphere()
    {
        //Set the Texture of both Spheres to the Texture from the RenderTexture
        leftSphereMat.mainTexture = renderTexture;
        rightSphereMat.mainTexture = renderTexture;
    }

    GameObject createSphere(string name, Vector3 spherePos, Vector3 sphereScale)
    {
        GameObject sphere = GameObject.CreatePrimitive(PrimitiveType.Sphere);
        sphere.transform.position = spherePos;
        sphere.transform.localScale = sphereScale;
        sphere.name = name;
        return sphere;
    }

    void OnVideoPrepared(VideoPlayer source)
    {
        Debug.Log(""Done Preparing Video"");

        //Play Video
        videoPlayer.Play();

        //Play Sound
        audioSource.Play();

        //Change Play Speed
        if (videoPlayer.canSetPlaybackSpeed)
        {
            videoPlayer.playbackSpeed = 1f;
        }
    }
}

There is also Unity tutorial on how to do this with a special shader but this does not work for me and some other people. I suggest you use the method above until VR support is added to the VideoPlayer API.
"
"
I wrote a sample app that allows the Android user to take a picture and have the text content from a view as an overlay on the image and saved to a gallery album:

What I would like to to is transform the text bitmap before joining the two images.  Specifically, I'd like to make the text curve up on the sides (simulating wrapping around a cylinder), and make it larger at the top than the bottom (simulating a top down perspective), as illustrated here:

There is no need to interpret the camera image in order to determine the amount of curvature or perspective change.  The question is how to manipulate the bitmap such that the two transforms can be made.
Here's the code I used to get the non-transformed text into the camera image and into the gallery:
private void combinePictureWithText(String fileName) {
    Log.v(TAG, ""combinePictureWithText"");

    int targetW = getWindowManager().getDefaultDisplay().getWidth();
    int targetH = getWindowManager().getDefaultDisplay().getHeight();

    /* Get the size of the image */
    BitmapFactory.Options bmOptions = new BitmapFactory.Options();
    bmOptions.inJustDecodeBounds = true;
    BitmapFactory.decodeFile(fileName, bmOptions);
    int photoW = bmOptions.outWidth;
    int photoH = bmOptions.outHeight;

    /* Figure out which way needs to be reduced less */
    int scaleFactor = 1;
    if ((targetW > 0) || (targetH > 0)) {
        scaleFactor = Math.min(photoW/targetW, photoH/targetH);
    }
    Log.v(TAG, ""Scale Factor: "" + scaleFactor);

    /* Set bitmap options to scale the image decode target */
    bmOptions.inJustDecodeBounds = false;
    bmOptions.inSampleSize = scaleFactor;
    bmOptions.inPurgeable = true;

    mBeerLayout.setDrawingCacheEnabled(true);
    Bitmap mDrawingCache = mBeerLayout.getDrawingCache();

    Bitmap cameraBitmap = BitmapFactory.decodeFile(fileName, bmOptions);
    Bitmap textBitmap = Bitmap.createBitmap(mDrawingCache);
    Bitmap combinedBitmap = Bitmap.createBitmap(targetW, targetH, Bitmap.Config.ARGB_8888);
    Canvas comboImage = new Canvas(combinedBitmap);
    cameraBitmap = Bitmap.createScaledBitmap(cameraBitmap, targetW, targetH, true);
    comboImage.drawBitmap(cameraBitmap, 0, 0, null);
    comboImage.drawBitmap(textBitmap, 0, 0, null); // WAS: matrix (instead of 0, 0)

    /* Save to the file system */
    Log.v(TAG, ""save combined picture to the file system"");
    try {
        File aFile = new File(fileName);
        if (aFile.exists()) {
            Log.v(TAG, ""File "" + fileName + ""  existed. Deleting it."");
            //aFile.delete();
        } else {
            Log.v(TAG, ""File "" + fileName + "" did not exist."");
        }
        FileOutputStream out = new FileOutputStream(fileName);
        combinedBitmap.compress(Bitmap.CompressFormat.JPEG, 90, out);
        out.flush();
        out.close();
        Log.v(TAG, ""Saved "" + fileName);
    } catch (Exception e) {
        Log.v(TAG, ""Failed in file output stream "" + e.getMessage());
    }

    /* Associate the Bitmap to the ImageView */
    //mImageView.setImageBitmap(combinedBitmap); // DRS was ""bitmap""
    //mImageView.setVisibility(View.VISIBLE);

    /* Add as a gallery picture */
    Log.v(TAG, ""galleryAddPic"");
    Intent mediaScanIntent = new Intent(""android.intent.action.MEDIA_SCANNER_SCAN_FILE"");
    File f = new File(fileName);
    Uri contentUri = Uri.fromFile(f);

    mediaScanIntent.setData(contentUri);
    this.sendBroadcast(mediaScanIntent);
}

This question/answer might provide details for how to do the perspective alteration, but I do not believe it answers the question for simulation of wrapping text around a cylinder.
","
One solution to this problem is to use Canvas.drawBitmapMesh().  This solution has the benefit of not requiring the added complexity of OpenGL.
The idea behind using a mesh is that whatever bitmap is fed into the function is warped to fit the defined points.  The image shows what's happening with just 5 points per ellipse:

So the portion of the bitmap that falls in the left, rectangular area will be manipulated to fit in the shape of the left polygon of the area defined by the vertices that we calculate.
Modeling portions of two ellipses provide the values the mesh.
There are going to be many ways to compute vertices that would do generally what is required, and this implementation is not necessarily the best, most efficient, most understandable, etc, but it has the benefit of doing what is asked about in the original question.
private static float[] computeVerticesFromEllipse(int width, int height, int steps, float curveFactor, float topBottomRatio) {
    double p = width / 2d;
    double q = 0d;
    double a = width / 2d;
    double b = curveFactor * a;

    float[] verticies = new float[(steps-1) * 4];
    int j = 0;

    double increment = width / (double)steps;

    for (int i = 1; i < steps; i++, j=j+2) {
        verticies[j] = (float)(increment * (double)i);
        verticies[j+1] =-(float) (-Math.sqrt((1-Math.pow(((increment * (double)i)-p), 2)/Math.pow(a,2)) * Math.pow(b,2)) + q);
        Log.v(TAG, ""Point, "" + verticies[j] + "", "" + verticies[j+1] + "", "" + j + "", "" +(j+1));
    }

    double width2 = topBottomRatio * width;
    p = width2 / 2d;
    q = (width - width2) / 2d;
    a = width2 / 2d;
    b = curveFactor * a;
    increment = width2 / (double)steps;

    double shift = width * (1d - topBottomRatio) / 2d;

    for (int i = 1; i < steps; i++, j=j+2) {
        verticies[j] = (float)(increment * (double)i) + (float)shift;
        verticies[j+1] =(float) -(-Math.sqrt((1-Math.pow(((increment * (double)i)-p), 2)/Math.pow(a,2)) * Math.pow(b,2)) + q)+ height;
        Log.v(TAG, ""Point, "" + verticies[j] + "", "" + verticies[j+1] + "", "" + j + "", "" +(j+1));
    }

    return verticies;
}

To integrate this new method into the code of the original question, we define some items to control the look of the result (curveFactor, topBottomRatio).  The steps variable can be adjusted to get a smoother looking result.  Finally, we defined items to enter into the drawBitmapMesh() method (columns, scaledVertices).  Then, rather than using drawBitmap(), we use drawBitmapMesh().  
    // 2017 03 22 - Added 5
    float curveFactor = 0.4f;
    float topBottomRatio = 0.7f;
    int steps = 48;
    float[] scaledVertices = computeVerticesFromEllipse(textBitmap.getWidth(), textBitmap.getHeight(), steps, curveFactor, topBottomRatio);
    int columns = (scaledVertices.length / 4) - 1; // divide by 2 since for two points, divide by 2 again for two rows

    Bitmap combinedBitmap = Bitmap.createBitmap(targetW, targetH, Bitmap.Config.ARGB_8888);
    Canvas comboImage = new Canvas(combinedBitmap);
    cameraBitmap = Bitmap.createScaledBitmap(cameraBitmap, targetW, targetH, true);
    comboImage.drawBitmap(cameraBitmap, 0, 0, null);
    // 2017 03 22 - Commented 1, Added 1
    // comboImage.drawBitmap(textBitmap, 0, 0, null); 
    comboImage.drawBitmapMesh(textBitmap, columns, 1, scaledVertices, 0, null, 0, null);


"
"
I have been searching through internet on how to export an android application to play it in vr mode once I install it and run on my android device (using OpenSpace3d)
Can anyone explain on how can I accomplish this with OpenSpace3d?
So far I was able to create an application that does Augmented Reality, but I want it to have split screen and then stick it in CardBoard, or whatever
Maybe there is some button in OpenSpace that does this, but so far I did not find it
Thank you very much!
","
just use the rendering/stereo plugit and choose cardboard mode.
for head tracking add the input/orientation sensor plugit and link the control event to the FPS plugit control action.
"
"
I have a virtual reality helmet: Oculus rift Dk2,
I want to develop games for this type of helmet.
Is there a  free framework for this type of helmet  like Immerse Framework (not free)
","
VRTK supports Occulus SDK - more info here:

https://github.com/thestonefox/VRTK

You can see some examples built with VRTK here: https://github.com/thestonefox/SteamVR_Unity_Toolkit/blob/master/MADEWITHVRTK.md
"
"
i have imported sktechup object inn unity. then i have apply the box coilder on a object but itishowing far away from the object i have try to rescal it and adjust it on a object but its not work. actually i want to movie and transform it in VR. thats why i am using ray caster with a camera. So,the rays will pass through the object because there is no any colider on a object. 
","
Solution 1:
Move Center of your box collider and resize it.
Solution 2:
Add empty gameobject to you object as child. Set its(empty gameobject) position to the center of your object. Add box collider to that empty object and resize it.
"
"
I am using SteamVR and would like to rotate the camera based on the height that a controller has been raised, I have written the script that sends the the height of the controller through to a method on my camera script.
When I do the following, the rotation works:
transform.Rotate(0, 0, 30); // 30 is just to test, we will use the controller height later
But I would like to do this rotation slowly, this must not happen immediately.
I have also tried using Quaternion.Lerp, but it does not seem to be working.
Quaternion.Lerp(this.transform.rotation, new Quaternion(this.transform.rotation.eulerAngles.x, this.transform.rotation.eulerAngles.y, zPosition, 0), Time.deltaTime * 5);
Any suggestions?
","
Quaternion.Lerp() should work fine for your needs - however, your usage of it supplies some incorrect parameters to the method, which is why it wasn't working as expected. (Also, you probably shouldn't be constructing a Quaternion using its constructor, there are helper methods to properly convert from Euler angles.)
What you need to do is record the initial starting and ending rotations, and pass them into Quaternion.Lerp() every time you call it. Note that you can't just reference transform.rotation every frame, because it will be changing as the object rotates. For example, adjusting your code to work:
Quaternion startRotation;
Quaternion endRotation;
float rotationProgress = -1;

// Call this to start the rotation
void StartRotating(float zPosition){

    // Here we cache the starting and target rotations
    startRotation = transform.rotation;
    endRotation = Quaternion.Euler(transform.rotation.eulerAngles.x, transform.rotation.eulerAngles.y, zPosition);

    // This starts the rotation, but you can use a boolean flag if it's clearer for you
    rotationProgress = 0;
}

void Update() {
    if (rotationProgress < 1 && rotationProgress >= 0){
        rotationProgress += Time.deltaTime * 5;

        // Here we assign the interpolated rotation to transform.rotation
        // It will range from startRotation (rotationProgress == 0) to endRotation (rotationProgress >= 1)
        transform.rotation = Quaternion.Lerp(startRotation, endRotation, rotationProgress);
    }
}

Hope this helps! Let me know if you have any questions.
"
"

This is image that I'm getting in Game mode. I'm using the latest GoogleVR for Unity available as of today and Unity 5.6.0b7. How to correct this?
","
You should know that v5.6.0 is a beta version and it is not very suitable for development. 
As suggested in the Unity Forum - For VR development it's better to stick to using Unity 5.4.2 with Google VR SDK 10.3.
"
"
I'm trying to import the google vr sdk to unity. Where the googlevr package has no demoscenes. Instead it has only the demo.
enter image description here
this is how I get demo in google vr how to add this sdk and make it into a  sterographic view(actual picture of the tutorial.Here it is ""DEMOSCENES"". But for me I get it as ""demo"")
(the video link of the tutorial 
check the video from 3:45:-
https://www.youtube.com/watch?v=cK1_EnZkwCE )
","
There should be one demo scene called ""ControllerDemo"" in the ControllerDemo folder and another demo scene called ""DemoScene""  in the HeadsetDemo folder. 
You have not clicked on any these folders yet based on your screenshot. You are currently inside the DemoScenes folder. 
If you into those two folders and don't see the demo scenes then get the latest Google SDK from this link.
"
"
I'm creating a project using the Gear VR, where you can rotate an object and display information based on the swipe and tap controls on the side of the headset.
Everything works great, I can rotate and select stuff when I use the touchpad on the side of the Gear VR, but when I change scenes and return to the main menu, and then go back into the scene I was just on, the functionality stops working.
I am using this script I've made:
using UnityEngine;
using UnityEngine.SceneManagement;
using System.Collections;
using System;

public class GearVRTouchpad : MonoBehaviour
{
    public GameObject heart;

    public float speed;

    Rigidbody heartRb;

    void Start ()
    {
        OVRTouchpad.Create();
        OVRTouchpad.TouchHandler += Touchpad;

        heartRb = heart.GetComponent<Rigidbody>();
    }  

    void Update()
    {
        if (Input.GetKeyDown(KeyCode.W))
        {
            SceneManager.LoadScene(""Main Menu"");
        }
    }


    void Touchpad(object sender, EventArgs e)
    {
        var touches = (OVRTouchpad.TouchArgs)e;

        switch (touches.TouchType)
        {
            case OVRTouchpad.TouchEvent.SingleTap:                
                // Do some stuff    
                break;      

            case OVRTouchpad.TouchEvent.Up:
                // Do some stuff
                break;
                //etc for other directions

        }
    }
}

I've noticed that when I start my game, an OVRTouchpadHelper is created. I don't know if that has anything to do with my problem.
The error I am getting is:

MissingReferenceException: The object of type 'GearVRTouchpad' has
  been destroyed but you are still trying to access it. Your script
  should either check if it is null or you should not destroy the
  object.

BUT I have not referenced this script anywhere else.
When I check my scene in play mode, the script is still there with the variable assignments still present.
Any help would be great!
","
OVRTouchpad.TouchHandler is a static EventHandler (so it will persist through the lifetime of the game).  Your script is subscribing to it when it's created but isn't unsubscribing when it's destroyed.  When you reload the scene the old subscription is still in the event but the old GearVRTouchpad instance is gone.  This will result in the MissingReferenceException next time the TouchHandler event fires.  Add this to your class:
void OnDestroy() {
    OVRTouchpad.TouchHandler -= Touchpad;
}

Now, whenever a GameObject with the GearVRTouchpad behaviour is destroyed, the static event in OVRTouchpad will no longer have a reference to it.
"
"
Good day everyone. I have a problem with my game since I'm using the openvr when building my game. The problem is whenever I build my game as a .exe, the position of my camera from my start menu is a little bit off, what I mean is that it is not placed like in the unity editor. And the second one is that even if i disable my track rotation under my head gameobject it still detects the trackrotation (When built). 
So far what i had tried are:
1.) setting the trackrotation in the script into trackrotation = false; (still didn't work)
2.) Removing the GvrHead Script from the head(Still effin rotates).
Why is that it still rotate even if I already remove the script from the head and set the trackrotation = false in the script.
or should i disable the gyroscope from my project?
I need to disable it because of the animation I have on my game. It needs to disable the trackrotation.
Someone help.
","
I'm using my 3rd party software so i did go to vridge app on my phone and disable the tracking rotation of the 3rd party software . So that solves my problem . 
"
"
I'm building a first person VR shooting game and the script responsible for CountDown and displaying Canvas on which Play Again Button appears does not play..
Here is the screen of the game play and the script for better understanding:

`

using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using UnityEngine.SceneManagement;
using UnityEngine.UI;

public class playGameAgainScript : MonoBehaviour {

    //Use this for Timer
    public Text countDown;
    private float timer = 60;

    //Declare Button
    public Button playAgainButton;

    void Start() {
        //Set countDown value
        countDown = GetComponent<Text>();
    }

    void Update() {
        timer -= Time.deltaTime;
        countDown.text = """" + timer.ToString (""f0"");
        if (timer <= 0) {
            timer = 0;
            playAgainButton.gameObject.SetActive (true);
        } 
    }

    public void PlayAgain() {
        UnityEngine.SceneManagement.SceneManager.LoadScene (""Main Scene"");
    }
}

`
","
The problem was that the playGameAgainButton was set to inactive as default and therefore the editor had nothing to look for..
And by moving the functionality from this script to a different place, it solved the probblem.. I moved it to the main playerScript and it works fine.
"
"
I'm working on a Unity Pro project that uses the Cardboard SDK to make an App for iPhone. I've followed all the instructions on the Cardboard Getting Started for IOS page. 
My problem comes when importing the project to Xcode on my Mac. The build fails with:
ld: library not found for -liontext
(null): Linker command failed with exit code 1 (use -v to see invocation)

When checking the log, i see that all the calls to the included libraries are misspelled (It's not liontext, it's libiontext.). Somehow the build script stripped the 'bi' in the middle. This happens for a lot of other libraries. See screenshots.
Error Log:

Actual Libraries:

Question is if there is a way i can edit the names manually somehow, or make the build pass this error. I already tried building the project several times in Unity but the same happens. Plus, i have already published my VR App to the Play Store, but Xcode is making it impossible to do the same on the App Store.
Any help is appreciated.
","
It is occurred because that there is something misspelled by unity in its search paths in XCode project. check your XCode project's Build Settings, look for search paths, in Library Search Paths it is written
//:configuration = Debug
LIBRARY_SEARCH_PATHS = $(inherited) ""$(SRCROOT)"" ""$(SRCROOT)/Libraries"" ""$(SRCROOT)/Libraries\Standard Assets/Plugins/iOS""

//:configuration = Release
LIBRARY_SEARCH_PATHS = $(inherited) ""$(SRCROOT)"" ""$(SRCROOT)/Libraries"" ""$(SRCROOT)/Libraries\Standard Assets/Plugins/iOS""

//:completeSettings = some
LIBRARY_SEARCH_PATHS


Look for the backslash '\', replace it with slash '/' and try rebuilding your project, this solved my problem.
It should be like this:
//:configuration = Debug
LIBRARY_SEARCH_PATHS = $(inherited) ""$(SRCROOT)"" ""$(SRCROOT)/Libraries"" ""$(SRCROOT)/Libraries/Standard Assets/Plugins/iOS""

//:configuration = Release
LIBRARY_SEARCH_PATHS = $(inherited) ""$(SRCROOT)"" ""$(SRCROOT)/Libraries"" ""$(SRCROOT)/Libraries/Standard Assets/Plugins/iOS""

//:completeSettings = some
LIBRARY_SEARCH_PATHS

"
"

Receiving this error in Unity version 5.6.0b7 : Assets/GoogleVR/Scripts/UI/GvrLaserPointer.cs(83,18): warning CS0618: UnityEngine.LineRenderer.SetColors(UnityEngine.Color, UnityEngine.Color)' is obsolete:Use startColor, endColor or colorGradient instead.'
Can't seem to get this right. Any ideas?
","
You need to do what it tells to do:
 lineRenderer.SetColors(Color.white, Color.black);

becomes
lineRenderer.startColor = Color.white;
lineRenderer.endColor = Color.black

Same will apply to SetWidth => start/endWidth
And for the SetVertexCount => numPositions
"
"

Receiving this error in Unity version 5.6.0b7 : Assets/GoogleVR/Scripts/UI/GvrLaserPointer.cs(83,18): warning CS0618: UnityEngine.LineRenderer.SetColors(UnityEngine.Color, UnityEngine.Color)' is obsolete:Use startColor, endColor or colorGradient instead.'
Can't seem to get this right. Any ideas?
","
You need to do what it tells to do:
 lineRenderer.SetColors(Color.white, Color.black);

becomes
lineRenderer.startColor = Color.white;
lineRenderer.endColor = Color.black

Same will apply to SetWidth => start/endWidth
And for the SetVertexCount => numPositions
"
"
I have a Occulus DK2. I'm using the newest occulus app 1.1 from the official website. The occulus is working great, I am in the standard room you get spawned in and I can launch the dreamdeck demo and everything works. Now I followed the instructions on how to use the experimental chromium build here, but chromium shows up on my default pc monitor, instead of the occulus. How do I see the webbrowser on the occulus?
What I did is download the browser, enable the two settings that are specified and then start the browser. Is there something more I need to do?
","
I don't know, but yesterday the VR did not recognize the browser, today I started it again and it alteast loaded the chrome.exe. I got an error message and found this issue. As mentioned there not all webVR examples work atm. I was trying this but it was not working, but using example 4 it finally worked. I guess it's a little buggy and you need to restart both the device and the browser a few times untill it works. Will leave the question open, if somebody else has a better advice then just to restart many times.
EDIT: I noticed 2 things. 1) It does like it when I plug it in immediately after plugging out my big tv, I don't know why but I tested this and it helped. 2) It does not like it if a WebVR capabable browser already is running when you plug it in. Even when restarting it will not start the browser application, so be sure that the browser is not running while initializing the occulus.
"
"
I am currently using unity with a Oculus rift running a OVRCameraRig and a script attached to the CenterEyeAnchor on it. This script uses the following code.
GameObject dot;
    void Start () {
        dot = GameObject.CreatePrimitive(PrimitiveType.Sphere);
        dot.GetComponent<Renderer>().material.color = Color.red;
        dot.layer = 2;
        dot.transform.localScale = new Vector3(0.5f, 0.5f, 0.5f);
    }

void Update () {
        RaycastHit hit;
        if (Physics.Raycast(transform.position, Vector3.forward, out hit, 80.0f))
        {
            dot.transform.position = hit.point;               
        }
        else
        {
            dot.transform.position = transform.forward * 80 + transform.position;
        }
    }

When I run the following code without a rift headset plugged in, the RayCast collides with all objects (whilst moving the OVRCameraRig within the scene) however if i restart the program with my rift plugged in, the raycast goes through all objects and hence moves the ball 80 units away from you.
I am wondering why the RayCasts behaviour would change after i plug in the headset and if there would be a way to stop this.
Edit: While debugging the following code it seems that when the rift is connected the raycast always fails to hit on all objects
","
Was a simple mistype of using Vector3.forward in the Raycast rather than transform.forward
"
"
I'm making a Shooting game for VR in Unity and I'm unable to shoot the object. Everytime I point the object, it throws this error. I tried other posts with same error but they does not answer my problem.
ERROR-
ArgumentException: The Object you want to instantiate is null.
UnityEngine.Object.CheckNullArgument (System.Object arg, System.String message) (at /Users/builduser/buildslave/unity/build/Runtime/Export/UnityEngineObject.cs:239)
UnityEngine.Object.Instantiate (UnityEngine.Object original) (at /Users/builduser/buildslave/unity/build/Runtime/Export/UnityEngineObject.cs:176)
playerScript+c__Iterator0.MoveNext () (at Assets/Scripts/playerScript.cs:30)
UnityEngine.SetupCoroutine.InvokeMoveNext (IEnumerator enumerator, IntPtr returnValueAddress) (at /Users/builduser/buildslave/unity/build/Runtime/Export/Coroutines.cs:17)
UnityEngine.MonoBehaviour:StartCoroutine(String)
playerScript:Update() (at Assets/Scripts/playerScript.cs:61)
I'm attaching an image for better understanding of the scene. The yellow cubes are the Shooting Object.

Here's the code I'm using-
using UnityEngine;
using System.Collections;
public class playerScript : MonoBehaviour {
//declare GameObjects and create isShooting boolean.
private GameObject gun;
private GameObject spawnPoint;
private bool isShooting;

// Use this for initialization
void Start () {

    //only needed for IOS
    Application.targetFrameRate = 60;

    //create references to gun and bullet spawnPoint objects
    gun = gameObject.transform.GetChild (0).gameObject;
    spawnPoint = gun.transform.GetChild (0).gameObject;

    //set isShooting bool to default of false
    isShooting = false;
}

//Shoot function is IEnumerator so we can delay for seconds
IEnumerator Shoot() {
    //set is shooting to true so we can't shoot continuosly
    isShooting = true;
    //instantiate the bullet
    GameObject bullet = Instantiate(Resources.Load(""bullet"", typeof(GameObject))) as GameObject;
    //Get the bullet's rigid body component and set its position and rotation equal to that of the spawnPoint
    Rigidbody rb = bullet.GetComponent<Rigidbody>();
    bullet.transform.rotation = spawnPoint.transform.rotation;
    bullet.transform.position = spawnPoint.transform.position;
    //add force to the bullet in the direction of the spawnPoint's forward vector
    rb.AddForce(spawnPoint.transform.forward * 500f);
    //play the gun shot sound and gun animation
    GetComponent<AudioSource>().Play ();
    gun.GetComponent<Animation>().Play ();
    //destroy the bullet after 1 second
    Destroy (bullet, 1);
    //wait for 1 second and set isShooting to false so we can shoot again
    yield return new WaitForSeconds (1f);
    isShooting = false;
}

// Update is called once per frame
void Update () {

    //declare a new RayCastHit
    RaycastHit hit;
    //draw the ray for debuging purposes (will only show up in scene view)
    Debug.DrawRay(spawnPoint.transform.position, spawnPoint.transform.forward, Color.green);

    //cast a ray from the spawnpoint in the direction of its forward vector
    if (Physics.Raycast(spawnPoint.transform.position, spawnPoint.transform.forward, out hit, 100)){

        //if the raycast hits any game object where its name contains ""zombie"" and we aren't already shooting we will start the shooting coroutine
        if (hit.collider.name.Contains(""Shooting Object"")) {
            if (!isShooting) {
                StartCoroutine (""Shoot"");
            }

        }

    }

}

}
","
The problem is this line
GameObject bullet = Instantiate(Resources.Load(""bullet"", typeof(GameObject))) as GameObject;

It can't find the resource ""bullet.""  Make sure you've deployed it into the right folder.
"
"
I've been interested in creating an add-on for Blender(an animation software) that would allow animation with the Oculus Rift.
The Oculus Rift SDK operates with C, but Blender operates with Python, which brings me to my question:
How can I realistically hope to communicate between the two?
If this question is too vague, please let me know.
","
I haven't written anything in Blender, but if the python works the same as usual you can extend it with c libraries.
So you could create a c module to work with the Oculus Rift and call it from Python.
Try this link and see if it helps: Extending Python with C or C++
"
"
I am making a raspberry pi robot with an FVP (First Person View) camera mounted on a Pan/tilt Servo. I want to make it VR compatible by connecting it to my Phone. But my phone doesn't have Gyroscope sensor to detect horizontal movements, but it has magnetometer and accelerometer. How can I combine data from accelerometer and magnetometer to make a virtual gyroscope that can move with my camera. I am noob in all of these.
","
You should have an rotation vector sensor that is already fusing the two. You will not get better results than it.
Note that this will not be as high quality as a proper gyroscope and will have artifacts if the robot moves. 
If you're still interested in how to make this yourself, you can get roll and pitch information from the accelerometer, then get the yaw information from the magnetometer. Best if you find a library for 3d maths and do this with quaternions or matrices. This seems like a use case where you will be affected by gimbal locks easily, so euler angles will be problematic.
I guess you want to use this for VR? Don't try to move the servos to compensate for head movement directly, you'll only make a motion sickness generator. Look at how timewarp works - you move the servos in the general direction a person is looking at and render the video reprojected on a sphere - this way you have almost zero lag.
"
"
I have seen a lot of apps and tools appeared lately that play virtual reality for any google glasses (google cardboard, vr one, etc)
I guess it is only a mathematical conversiom that splits screen and creates 2 cameras for 3d world, each one with lookAt matrix with different vreference point but, which one is the formula? Or which is the way to convert 3d models to vr scene? 
","
you can find documentation on internet but I recommend you using unity with cardboard sdk 
"
"
Problem:
I am using the prefab OVRCameraRig from the Oculus SDK with Unity. I use the viewport rect to show the seen view on the main monitor (mirroring), but after I disable the Oculus with VRSettings.enabled = false, the view on the main monitor gets stretched.
Thanks in advance.
OS: Windows
IDE: Unity v5.4.0b25b
Unity Oculus SDK: v.1.1.0
","
Perhaps try to reset your camera aspect ratio? This might help:
Camera.mainCamera.aspect = Screen.width/Screen.height;

"
"
I am starting a new business which will allow users to download videos and watch them through the Samsung Gear VR.
My videos are 3d 180 degrees using the Gopro Hero3+ duel system.
Now I can get my video to play as a 360 by naming is filename_360.mp4
The gearVR recognises this and makes it stretch to 360, however I cannot get it to work and only show 180 degrees.
I have downloaded a video from another website which has 180 degrees vision, and when you look around you can see 180 degrees pitch black. So somehow they have managed to trick the gearvr into it being 360, but with only 180 degree of content present.
Does anyone know how I can achieve this ?
","
Here's another thread on the subject: https://forums.oculus.com/community/discussion/23890/how-to-play-180-3d-video
If changing the name of the video doesn't work, there is of course the 'ugly' solution of adding enough padding around your video so that it appears 180 even though it's 360.
"
"
I have a Mono 360 photo which uses a single pano and I need a Stereo 360 photo which uses two stacked panos.
My question:
Is there any way to convert it in my app with a few lines of Code?
Here is a usefull link which describes the Problem:
https://developers.google.com/vr/concepts/vrview
For example:
This is what I have:

This is what I want:

","
You only have to do this in the ImageLoaderTask Class to use 2:1 images:  
 panoOptions = new Options();
 panoOptions.inputType = Options.TYPE_MONO;

instead of :
panoOptions = new Options();
panoOptions.inputType = Options.TYPE_STEREO_OVER_UNDER;

"
"
I'm using GoogleVR library for iOS to play VR video. I use GVRVideoView. But there is a problem that I don't know how to get current play time. I need to show a Slider on UI, that includes current play time.
https://developers.google.com/vr/ios/get-started
So, how can I get current play back time of GVRVideoView?
","
GVRVideoView has a delegate method
- (void)videoView:(GVRVideoView*)videoView didUpdatePosition:(NSTimeInterval)position;
You can do with the position, for example, set progress with position/videoView.duration.
"
"
I'm trying to make a WebVR environment using Three.js. I exported some scenes from Cinema4D and loaded them in with the Colladaloader of Three.js. Now I wanted to try this environment in my Google Cardboard but I needed to have the split screen for both my eyes, of course.
I used the npm module three-stereo-effect to achieve the VR effect, but it's overlapping when using it in a cardboard. I looked it up and saw that a lot of WebVR examples had a rounded rectangle for each eye (example of what I mean), not a straight rectangle, I thought I needed to find matrices to fix that (When looking at the examples of this repository). But then I downloaded a VR tunnel racing game and saw that they used straight rectangles and the vision was fine.
Now I'm thinking the eyeSeparation of my stereo effect is incorrect, I saw someone use the property eyeSeparation on the StereoEffect module and tried that out, but I think I shouldn't just be guessing a value...
Am I on the right track here to find a solution? Or am I looking in the total wrong direction why my 3D scene does not give a good vision when using a Cardboard?
This is the code I'm experimenting with.
import {sets} from './data/';

import * as THREE from 'three';
import threeOrbitControls from 'three-orbit-controls';
import ColladaLoader from 'three-collada-loader';
import threeStereoEffect from 'three-stereo-effect';

import {BufferLoader} from './modules/sound';
import {SpawnObject} from './modules/render';

const OrbitControls = threeOrbitControls(THREE);
const StereoEffect = threeStereoEffect(THREE);

let scene, camera, renderer;
let audioCtx, bufferLoader;

const notes = [];
let stereoEffect = null;

const init = () => {
  window.AudioContext = window.AudioContext || window.webkitAudioContext;

  audioCtx = new AudioContext();
  bufferLoader = new BufferLoader(audioCtx);

  bufferLoader.load(sets.drums)
    .then(data => spawnObject(data));


  initEnvironment();

};

const spawnObject = data => {

  for (let i = 0;i < 5;i ++) {
    const bol = new SpawnObject(`object.dae`, audioCtx, data[0], scene, false);
    notes.push(bol);
  }

  // console.log(notes);
};

const initEnvironment = () => {

  scene = new THREE.Scene();
  camera = new THREE.PerspectiveCamera(
    45, window.innerWidth / window.innerHeight,
    1, 10000
  );

  renderer = new THREE.WebGLRenderer();
  renderer.setSize(window.innerWidth, window.innerHeight);

  stereoEffect = new StereoEffect(renderer);
  // stereoEffect.eyeSeparation = 1;
  stereoEffect.setSize(window.innerWidth, window.innerHeight);

  console.log(stereoEffect);

  document.querySelector(`main`).appendChild(renderer.domElement);

  camera.position.set(0, 0, 2);
  camera.lookAt(scene.position);

  new OrbitControls(camera);

  //LIGHTS
  const light = new THREE.PointLight(0xFFFFFF);
  light.position.set(0, 0, 9);
  light.castShadow = true;
  light.shadow.mapSize.width = 1024;
  light.shadow.mapSize.height = 1024;
  light.shadow.camera.near = 10;
  light.shadow.camera.far = 100;
  scene.add(light);

  // const hemiLight = new THREE.HemisphereLight(0xffffff, 0xffffff, 0.6);
  // hemiLight.color.setHSL(0.6, 1, 0.6);
  // hemiLight.groundColor.setHSL(0.095, 1, 0.75);
  // hemiLight.position.set(0, 500, 0);
  // scene.add(hemiLight);
  //
  // const dirLight = new THREE.DirectionalLight(0xffffff, 1);
  // dirLight.color.setHSL(0.1, 1, 0.95);
  // dirLight.position.set(- 1, 1.75, 1);
  // dirLight.position.multiplyScalar(50);
  // scene.add(dirLight);
  // dirLight.castShadow = true;


  //FLOOR
  const matFloor = new THREE.MeshPhongMaterial();
  const geoFloor = new THREE.BoxGeometry(2000, 1, 2000);
  const mshFloor = new THREE.Mesh(geoFloor, matFloor);

  matFloor.color.set(0x212E39);
  mshFloor.receiveShadow = true;
  mshFloor.position.set(0, - 1, 0);

  scene.add(mshFloor);


  //ENVIRONMENT
  const loader = new ColladaLoader();

  loader.load(`../assets/environment.dae`, collada => {
    collada.scene.traverse(child => {
      child.castShadow = true;
      child.receiveShadow = true;
    });

    scene.add(collada.scene);
    render();
  });
};

const render = () => {

  // stereoEffect.render(scene, camera);
  // effect.render(scene, camera);

  renderer.shadowMap.enabled = true;
  renderer.shadowMap.type = THREE.PCFSoftShadowMap;

  renderer.gammaInput = true;
  renderer.gammaOutput = true;

  renderer.setClearColor(0xdddddd, 1);
  stereoEffect.render(scene, camera);

  requestAnimationFrame(render);
};



init();

","
From the PDF ""Scalable Multi-view Stereo Camera Array for Real World Real-Time Image Capture and Three-Dimensional Displays"":

2.1.1 Binocular Disparity 
Binocular disparity is the positional difference between the two retinal projections of a given point in space. This positional difference results from the fact that the two eyes are laterally separated and therefore see the world from the two slightly different vantage points. For the average person the mean lateral separation also known as the interocular is 65mm. Most of the population has an eye separation within ±10mm of the average interocular.

It would seem, with a little testing with friends with a variety of face shapes you will find a happy average for the eyeSeparation value for the device and the people using it. I would then also provide some settings panel which allows a few different settings of the eyeSeparation for users to choose from if they find disparity or overlap in their stereo experience. Normally I think this would be done with a keyboard connected to the same system to dial in the stereo alignment, but you're in cardboard, so the user may need trial and error to get it right.
"
"
I need to know how hard will it be to get input from a heartbeat sensor connected to arduino to a android running a unity made app. Im making for my degree final project a vr system that take a real time heartbeat input to effect the unity game, and i need to decide if i want a simple android vr or oculus (which is less common and more expensive). The plan is to connect the arduino via usb and call the serial port, but i think it may be more complicated then that. Since I have no prior experience in arduino i would love to get your advice in what I will need to do to make it work.
","
The easiest way is to send the collected data through Bluetooth controller. You can buy an HC-05 Wireless Bluetooth Serial Transceiver Module for 3 US dollars. You can connect the device to Rx and Tx terminals on Arduino. See this link for details on how to use HC-05 with Arduino.
In android device side Install or create an android application which can send data through Bluetooth serial terminal. You can find lot of BT serial port terminal applications in play store. I think some of the applications are open source and you can use their code in your own application.
"
"
I've been working with the Google VR SDK and I wrote a simple app that uses the GvrActivity and GvrView. The app used to work without any issues, but after I started using the Daydream headset, the app won't run.
Instead, when I deploy the app with Android Studio, I get an ""Incompatible App"" message that says ""This Cardboard application is not compatible with Daydream headsets.""
I didn't make any changes to the app. Stereo mode and VR mode are disabled, so I don't know what the problem is. Any thoughts?
","
I fixed the problem by turning off Google VR Services. This can be done by going to Settings > Apps and selecting ""Show System"" in the menu. Then scroll down to ""Google VR Services"" and select ""Disable"". After this, regular apps can run normally.
"
"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 7 years ago.







                        Improve this question
                    



I am developing an Android game which is based on VR, In game player is free to move around an amusement park which will be filled with stuff like stalls, toys, Ferris wheels, rails and other carnival rides. Now if I put a big mesh like rails and Ferris wheels which have around 400k+ polycount each and when I run it on my smartphone (galaxy s7 edge) I face hitching and lagg viewing the scean in VR. I also reduced polygons from some objects, fixed texture resolution but still no way near getting it close to run at 50-60fps. 


","
If you are building it for Oculus platforms, here are some performance requirements:

60 frames per second
50-100 drawcalls per frame
50k – 100k polygons per frame
1 ~ 3 ms spent in script execution (Unity Update()) 

To optimize your game:

Use Lightmaps/baked lights instead of run-time/dynamic lighting.
Mark stationary objects as static to use static/dynamic batching. This will reduce drawcalls.
Use Oculusion culling to save rendering power. This will reduce tricount and drawcalls.
Use fewer materials in your objects by putting separate textures into a larger texture atlas.
Use Compressed textures to decrease the size of your textures. 
Use Generate MipMaps option on importing textures to save GPU power.

Read More about optimization for VR.
Read More about Graphic Optimization in Unity 
Hope this helps
"
"
I made a simple android app that have a main menu with 5 buttons and when you press a button a box will appear beside the menu depending on which button you pressed.
I made the menu and boxes as a UI( buttons, panels and scrolls).
I want to make it working with VR or Google cardboard and instead of pressing a button just gaze on it, and I want it as a 2d. The menu is fixed and not moving as user move the VR, and only the pointer (gvrReticle) is moving to click a button.
I downloaded ""gvr unity sdk"" and try to make my app using it.
","
You need to do few things:

Import Google VR SDK 
Run sample scene to know how things are placed
(specifically UI). 
Change the canvas from screenspace to world space
etc

Follow this tutorial further: https://www.raywenderlich.com/116805/make-vr-game-unity-google-cardboard
"
"
I have been looking at the lens correction shader code from the Google GVR SDK for Unity and have been scratching my head as to the use of the z component of the view space position (UNITY_MATRIX_MV, without the perspective transform of UNITY_MATRIX_MVP) in the undistort() functions (this one of the simpler variants):
float r2 = clamp(dot(pos.xy, pos.xy) / (pos.z*pos.z), 0, _MaxRadSq);
pos.xy *= 1 + (_Undistortion.x + _Undistortion.y*r2)*r2;

Given my understand that we want to warp the rendered image in 2d screenspace to counteract distortion that will be applied by lens the screen is viewed through, what on earth are we doing dividing our radius(?) by the linear depth (pos.z) squared? I can conceive that this is in lieu of dividing by w for perspective, but then why would we want to divide by the square of the z component (how would that ever be more correct than simply dividing by z or w) ?
","
Felt a bit silly in hind sight, as this is just the result of a regular optimisation.
The division is regular perspective division (but leaving the z coord used for depth buffer/culling as linear, and presumably w should thus be 1.0 to ensure proper depth interpolation). Reorganising the computation presumably was found to save shader cycles and/or accuracy.
This code is equivalent to foreshortening pos.xy by dividing it by pos.z first, then doing taking the dot product of pos.xy with itself to get its length squared in 2D screenspace (and then clamping it, etc.)
"
"
I have an WPF+SharpDX Windows application that displays to the OSVR HDK via a fullscreen window on the screen that is the HDK. This setup works well, but it requires users to state which screen the HDK is on. 
I would like to have that automatically detected, but haven't seen anything in the API on which screen is the headset.
Currently I render in a window:
var bounds = dxgiDevice.Adapter.Outputs[_selectedOutput].Description.DesktopBounds;
form.DesktopBounds = new System.Drawing.Rectangle(
    bounds.X, bounds.Y, bounds.Width, bounds.Height);

And _selectedOutputis the thing I'm looking for.
I don't support direct mode at this time and I'm using Managed-OSVR. The application will run on Windows 8/8.1/10.
","
It's been a while since I coded anything for OSVR, but here's from what I remember:
If you're running in extended mode, the OSVR is treated as a regular display. You can rearrange it as any other screen. The output location can be configured in the OSVR config file.
I used the following (Java) to retrieve the position and size to set up my window:
osvrContext.getRenderManagerConfig().getXPosition()
osvrContext.getRenderManagerConfig().getYPosition()
osvrContext.getDisplayParameters().getResolution(0).getWidth()
osvrContext.getDisplayParameters().getResolution(0).getHeight()

To clarify: I don't know if you can retrieve the id of the display in extended mode. From what I know, it's only defined as a position and size on the desktop.
I hope that it helps you, somewhat.
"
"







Closed. This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet Stack Overflow guidelines. It is not currently accepting answers.
                                
                            











 We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.


Closed 7 years ago.







                        Improve this question
                    



Can anyone please recommend me an SDK to start with VR(from the list of Cardboard SDK,Daydream,Unity SDK and Oculus)keeping in mind that I'm fairly good at Java only and have developed apps using Android Studio.
Any reference links would also help much.Thanks.
","
For Unity, Google Cardboard is best for now. 
Google Cardboard Introduction
Unity Cardboard SDK and Samples
Another Tutorial

I'm fairly good at Java only and have developed apps using Android
  Studio

You have to learn C#, Unity API. Its similar to Java but you still need to learn it. 
"
"
I'm creating a Vive VR Passive VR experience, where your in a space ship and without any controls, it moves passively through the whole solar system. It's not AI, there will be a predetermined destination.
My question: How to make on object move passively?(A.K.A Space Ship with cameras)
","
You have a starting point, and a destination point, then Lerp between them. The examle in the unity documentation has a example for your exact question.
using UnityEngine;
using System.Collections;

public class ExampleClass : MonoBehaviour {
    public Transform startMarker;
    public Transform endMarker;
    public float speed = 1.0F;
    private float startTime;
    private float journeyLength;
    void Start() {
        startTime = Time.time;
        journeyLength = Vector3.Distance(startMarker.position, endMarker.position);
    }
    void Update() {
        float distCovered = (Time.time - startTime) * speed;
        float fracJourney = distCovered / journeyLength;
        transform.position = Vector3.Lerp(startMarker.position, endMarker.position, fracJourney);
    }
}

You would attach that script to your ""Spaceship"" root object, you would then make the player a child of the spaceship so it will move with the ship as it goes along it's route.
"
"
Is this possible in nature? What role of SDL library in OpenVR API? Does it needed for OpenGL context or only for mirroring the stereo image to SDL window?
","
It is possible. If you don't use SDL, you'll have to create your rendering context and window by yourself. The whole code would be too long for this answer, but on Windows you could use functions like CreateWindowEx and wglCreateContext. OpenVR doesn't require anything different from a normal context setup, but you need to use a somewhat modern version of OpenGL (4.1 at least works for me).
"
"
I'm making a (2D) 360 image viewer in Unity for GoogleVR/Cardboard, and each eye is receiving a slightly different view of the scene for a stereoscopic effect. I'd like to disable this, and have each eye receive the same image. 
","
Not sure why you would like to disable this but assuming you're using GoogleVR assets, you can set the values of Eye property of GvrEye scripts to Center (they are found on Main Camera Left and Main Camera Right game objects at runtime), then you set the transform.position.x to 0 and the transform.localEulerAngles.y to respectively -8 and +8 on Main Camera Left and Main Camera Right (those values seem good to me but you can tweak them I guess).
"
"
I was running some gvrf sample applications. I was going through the code and I found out that the GVRSceneobject has some properties called ""stack number"" and ""slice number"".

For GVRCylinderSceneObject default values are 2 and 36 respectively.
For GVRSphereSceneObject default values are 18 and 36 respectively.

Could somebody explain me what do the ""stack number"" and ""slice number"" mean?
","
I don't have experience with Gear VR, but stacks and slices are common terms in Virtual Reality, so I try to help.
Basically, the slices' value is the number of subdivisions around the X axis. Similarly, the stacks' value is the number of subdivisions around the Y axis (X and Y axis are used in OpenGL's definitions, I hope that they correspond to Gear VR as well).
So, you can imagine slices and stacks as latitude and longitude.
This image (from here) represents a sphere with different values for slices and stacks. It should clarify the concept (higher values imply more subdivisions):

"
"







Closed. This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet Stack Overflow guidelines. It is not currently accepting answers.
                                
                            











 We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.


Closed 7 years ago.







                        Improve this question
                    



Lately I'm interested in VR and I decided to learn more about it and technologies beyond this.
I also have a background of Mathematic science from my university field.(Applied Math.)
Can anyone suggest me some article or book or anything to learn more?
Free references preferred.
Thank you.
","
I would suggest you start by actually making things in VR. To read about it and to experience it are completely different things. I personally work with Oculus Rift. It is easy to develop for it because of its integration with various game engines. But as a cheaper option you could get started with Google Cardboard.
If you want to read up about the core technology behind VR, I would suggest start by reading about Computer Graphics. After you have a grip on that you can start to explore other topics for example how tracking works, issues of FOV etc.
For readings about Graphics, I would suggest you have a look at this thread.
https://gamedev.stackexchange.com/questions/12299/what-are-some-good-books-which-detail-the-fundamentals-of-graphics-processing
"
"
After I updated GVR android SDK AAR files to version 1.0.0, my app can't play videos and app has stopped!
My logcat is here:

java.lang.NoClassDefFoundError: com.google.vr.sdk.widgets.video.nano.SphericalMetadataOuterClass$SphericalMetadata

Can U help me please...
","
Just add this to your Gradle
compile 'com.google.protobuf.nano:protobuf-javanano:3.0.0-alpha-7'

"
"
I have Created a VR video player using Google Cardboard SDK and Easy Movie Texture plugin in Unity.
I want to add control buttons like play,pause ,stream like a normal video player.
Anybody knows how it can be implemented ?
noob at Unity. Need a great help. Thanks in advance
","
I would suggest using the magnetic sliding trigger at the cardboard's side to allow the user to toggle a menu, and use the phone's gyroscope or compass to track user rotation and allow the user to select Play/Pause etc... A nod of the head, detectable by the phone's accelerometer, will be used to confirm an action. 
Use the Cardboard.SDK.OnTrigger event provided by the Google Cardboard API to detect the magnetic trigger pull in your script as such:
void OnEnable() {
    Cardboard.SDK.OnTrigger += triggerPulledEvent;
}

void triggerPulledEvent() {
    menuOpen = !menuOpen;//Toggle the menu
}

Then, poll the mobile device for sensors:
To select option based on head rotation:
Input.compass.enabled = true;

int totalOptions = 3;//Play, pause, exit
int selectionNumber = Input.compass.magneticHeading / 360 * totalOptions;

enum Options {
    PLAY = 0, PAUSE = 1, EXIT = 2//Add more as needed
}

Of course, you'll need to display the current option the user is looking at, but I'll leave it to you.
For the accelerometer to detect nod, check for a significant value acceleration in the Y axis:
if(Math.Abs(Input.acceleration.y) > 13) {
    ConfirmOption();
}

void ConfirmOption() {
    //Code that plays, pauses, stops the video etc
}

"
"
Is there any app similar to Google Cardboard that can work without Gyroscope sensor. And by work I mean:

Play simple movies that doesn't require head movement
And if possible, can simulate gyroscope with help of accelerometer and compass. It can be done.  Here is a open source development going on about  Virtual Gyro

Help is really appreciated.
","
Some VR games on Android work fine without gyro :
try
Space X Hunter VR : 
https://play.google.com/store/apps/details?id=com.creation3d.spacexhuntervr
VR Bowl :
https://play.google.com/store/apps/details?id=com.creation3d.vrbowl
and the games from Fibrum
"
"
Could anyone please tell me how to remove having to press alt-mouse up/down in unity google cardboard VR when testing on a PC ? I need my PC test VR app just to use the mouse in the regular FPS way. The mouse works fine for left and right but up down is mapped to pressing the alt-up/down key.
","
found the answer in EditorDevice.cs:  
if (Input.GetKey(KeyCode.LeftAlt) || Input.GetKey(KeyCode.RightAlt)) 

comment this out and it works fine
"
"
I just start to learn HTC vive app development with unity and want to interact with Object and want to get it as controller interact with it.
Try to learning form StreamVR Unity Toolkit documentation. I found a scene in sample demo where Gameobject are interacting through controller but there are so many scripts involved. I am amazed that on a cube VRTK_InteractalbeObject is attached and it is responding to controller. how can i get interacted object in htc vive.?
","
So, here is one of many solutions to interact with objects with controllers - I will show here the easiest one.
A. Find [CamerRig] prefab from SteamVR folder and place it in hierarchy:

B. Find Controller (right)->Model GameObject in it:

C. Create GameObject with Rigidbody and SphereCollider and place it as child of Controller (right)->Model GameObject
C.1. Be sure that this object is in same position as Model GameObject
D. Now play the game, and moving your right vive controller push some other GameObjects that have Rigidbody and any Collider.
E. To use vive controller Buttons use this script, and place it on Controller (right) GameObject.
public class VIVEController : MonoBehaviour 
{
    public SteamVR_TrackedObject trackedObj;
    private SteamVR_Controller.Device controller;

    void Start () 
    {
        controller = SteamVR_Controller.Input ((int)trackedObj.index);
    }

    void Update ()
    {
        if (controller.GetPressDown (Valve.VR.EVRButtonId.k_EButton_SteamVR_Trigger)) 
        {
            OnTriggerPressed ();
        }

        if (controller.GetPressDown (Valve.VR.EVRButtonId.k_EButton_SteamVR_Touchpad)) 
        {
            OnTouchpadPressed ();
        }
    }

    private void OnTriggerPressed()
    {
        Debug.Log(""OnTriggerPresse"");
    }

    private void OnTouchpadPressed()
    {
        Debug.Log(""OnTouchpadPressed"");
    }
}

F. Assign from inspector trackedObj (It is on same GameObject as this script)
G. Find script SteamVR_RenderModel and comment out in method LoadComponents 
        for (int i = 0; i < t.childCount; i++)
        {
            var child = t.GetChild(i);
            child.gameObject.SetActive(false);
            StripMesh(child.gameObject);
        }

This is necessary, as otherwise your custom GameObject with collider that is child of controller model, will become inactive.
Resuming - it is very easy, but you need to do quite few steps. 
Try playing with controllers buttons, to for instance push object only when you press trigger.
As bonus, here is small game I made with same approach:
https://www.youtube.com/watch?v=kzX7Iw6cHZ8
"
"
My version is 0.9.1
enter image description here
","
From what I know it isn't possible right now. I tried searching for it in sdk, but only thing I found was empty function. I tried removing it, but it didn't help as well. Hopefully they will add it in future updates.
"
"
I am developing a VR Application using Google VR SDK.Due to nature of app users very frequently play same video in a day.Using the code below i play videos from URL and this consumes so much bandwidth
I want to decrease bandwidth usage.I think i can store videos in phone memory so next time they play it can simply be played from memory but in this case users would easily access my files and steal it
So is there a way to do this (like some kind of cache) but without simply exposing users to my raw video files?
 class VideoLoaderTask extends AsyncTask<Pair<Uri, Options>, Void, Boolean> {
    @Override
    protected Boolean doInBackground(Pair<Uri, Options>... fileInformation) {
        try {

                Options options = new Options();
                options.inputFormat= Options.FORMAT_DEFAULT;
                options.inputType = Options.TYPE_MONO;
                Uri myUri = Uri.parse(""video.mp4"");
                videoWidgetView.loadVideo(myUri, options);

        } catch (IOException e) {
            // An error here is normally due to being unable to locate the file.
            loadVideoStatus = LOAD_VIDEO_STATUS_ERROR;
            // Since this is a background thread, we need to switch to the main thread to show a toast.
            videoWidgetView.post(new Runnable() {
                @Override
                public void run() {
                    Toast
                            .makeText(SimpleVrVideoActivity.this, ""Error opening file. "", Toast.LENGTH_LONG)
                            .show();
                }
            });
            Log.e(TAG, ""Could not open video: "" + e);
        }

        return true;
    }
}

","
I found the solution in developer.android.com.I can simply just save files to the storage
String FILENAME = ""hello_file"";
String string = ""hello world!"";

FileOutputStream fos = openFileOutput(FILENAME, Context.MODE_PRIVATE);
//Context.MODE_PRIVATE makes them invisible to users
fos.write(string.getBytes());
fos.close();

"
"
I'm starting to experiment VR game development on ios. I learned a lot from google cardboard sdk. It can track user's head orientation, but it can not track user's translation. This shortage cause the use can only look at the virtual environment from a fix location (I known I can add auto walk to the game, but it's just not the same).
I'm searching around the internet, some says translation tracking just can't be done by using sensors, but it seems combining magnetometer, you can track user's movement path, like this example. 
I also found a different method called SLAM, which use camera and opencv to do some feature tracking, then use feature point informations to calculate translation. Here's some example from 13th Lab. And google has a Tango Project which is more advanced, but it require hardware support.
I'm quite new to this kind of topic, so I 'm wondering, if I want to track not only the head orientation but also the head(or body) translation movement in my game, which method should I choose. SLAM seems pretty good, but it's also pretty difficult, and I think it will has a big impact on the cpu.
If you are familiar with this topic, please give some advice, thanks in advance!  
","
If high accuracy is not important, you can try using the accelerometer to detect walking movement (basically a pedometer) and multiply it with an average human step width.  Direction can be determined by the compass / magnetometer.
High accuracy tracking would likely require complex algorithms such as SLAM, though many such algorithms have already been implemented in VR libraries such as Vuforia or Kudan
"
"
i am creating a VR app with gui buttons. i want to trigger the button if user keeps the button for active for a specified time. Like the one explained in Google design patterns.
Here is a youtube link on how to animate fuse buttons in Mobile VR apps
","
First, you have to place a UI element in world space. Add a collider to it. Then, use the Gaze provided by the Cardboard SDK to check for a collision. If your UI element detects a gaze, it starts off a timer and animates a fuse. If the gaze leaves the UI element before the timer reaches 0, you do noting. Otherwise, activate the fuse button.
"
"







This question already has answers here:
                                
                            




Enable/Disable VR from code

                                (5 answers)
                            

Closed 6 years ago.



In android GearVR, is it possible to launch an application in normal/phone mode, and then have it transition to Unity/VR when the user clicks a button to go into VR?
So far the app either launches completely in VR with ""vr_only"" as an option, or launches normally and goes to Oculus Home when we try ""vr_both"" or ""vr_dual"".
Is such a thing possible, or will we have to make two separate apps? Please note that using the ""Virtual Reality Supported"" option in Unity will not work on the Gear.
","
I won't say that it is impossible but would be difficult to achieve as there is not simple way to do this. Using Virtual Reality Supported option in player settings as suggested by @Programmer is not an option. You can either check it or un-check it before building the app. Changing it later through code does not work for android build.
You can build a non-VR App and write native plugin to give users an option to start VR App which may be a different app (with a different bundle identifier).
I am not good with native code so can't help you much with exact way to do this. Instead you can have a look at following links:

http://answers.unity3d.com/questions/259935/open-app-from-other-app.html
http://forum.unity3d.com/threads/android-ios-launch-from-within-a-unity-app-another-unity-app.222709/#post-1486300
Is it practical to start a Unity game from an Android app and revert back from Unity to the app?

For more information on this topic follow this forum post on oculus forum: https://forums.oculus.com/developer/discussion/26935/gearvr-app-within-android-app
"
"
So I would like to change the view in the google Cardboard demo-app to a mono-view. So that instead of displaying a image on each half of the handy-screen for each eye I want to display only one image that fills the complete screen. Any ideas how I can do that?
","
In the Android SDK, you call setVRModeEnabled(false) on the CardboardView object.
In the Unity SDK, you set VRModeEnabled = false on the Cardboard.SDK object.
"
"
I'm creating a virtual reality website and was wondering how could one make visitors browse from one VR website (A) in full screen stereoscopic view to another VR website (B), without exiting full screen stereoscopic view ? Is it even possible ?
This is how I imagine the ""VR"" web would look like, but I might be wrong.
Thank you.
","
In current experimental implementations of the webvr API (https://webvr.info), any website can always call requestPresent to enter VR mode if a headset is available. This is still far from providing a good link traversal experience. A website needs to know what headset was previously used to push the content appropriatley. A set of events have been recently defined for this purpose but are not still completely implemented by browsers. Additional specifications might be needed to define how browsers should behave when traversing links: How do you communicate the user you changed URL? How are content or HTTP errors notified? What is it displayed in the headset when you leave a site and the new one is still loading? Some discusion is happening here: https://github.com/w3c/webvr/issues/69
"
"
I'm new to Leap Motion. I'm trying to implement Leap Motion using my mac. But the error shows:
DllNotFoundException: LeapC
Leap.ClockCorrelator..ctor ()
Leap.Unity.LeapServiceProvider.Awake () (at Assets/LeapMotion/Scripts/LeapServiceProvider.cs:162)

However the LeapC.dll is available in plugins/X86 folder.
","
It sounds like you're trying to use the Orion Unity assets with the V2 software (as the Orion tracking software is not available for OSX). The Orion Unity assets are built on the LeapC API, which was introduced in Orion. You need to switch to a Windows machine or revert to the older V2 Unity assets, which can be found here: https://github.com/leapmotion/LeapMotionCoreAssets
"
"
I am trying to show four videos at once using Google Cardboard. These videos are normal 2D videos that were shot on a normal 16:9 camera. What I want and need is to have one video in front of you then you turn your head 90 degrees and you see another video, turn again and see another until you hit the front video again. Please see my Pablo Picasso Microsoft Paint skills to visualize what I am talking about...
So basically what I need is like four VR movie theater screens that a person can look around in. Is there a program I could use or do I have to do some programming to make this happen? Searching this is not easy with all the articles of VR that pop up. Any help that can point me in the right direction would be greatly appreciated!
","
I actually found an app that did this all for me. The app is called 360 Virtual Reality Player(Google Play Store) and it takes any 2D video and makes it into a head-tracked VR video. Once I found this app, all I needed to do is stitch the videos together with a black bar in between them using OpenCV to get the desired effect.
"
"
I am doing a tutorial from VR Dev School. The lesson is Picking Up an Object and Parent the Transform. This is the code I copied exactly from the lesson. I have the script and a sphere collider attached to controlller(left). I've tried toggling 'is trigger' on/off. The collision is not being detected in the console. I am not receiving any errors or warnings.
Any assistance is appreciated and I will answer any questions 
using UnityEngine;
using System.Collections;

[RequireComponent(typeof(SteamVR_TrackedObject))]

public class PickupParent : MonoBehaviour {

    SteamVR_TrackedObject trackedObj;
    SteamVR_Controller.Device device;


    void Awake () {
        trackedObj = GetComponent<SteamVR_TrackedObject>();

    }

    void FixedUpdate () {
        device = SteamVR_Controller.Input((int)trackedObj.index);
        if(device.GetTouch(SteamVR_Controller.ButtonMask.Trigger))
        {
            Debug.Log(""You are holding 'Touch' on the trigger"");
        }
        if (device.GetTouchDown(SteamVR_Controller.ButtonMask.Trigger))
        {
            Debug.Log(""You activated touchdown on the trigger"");
        }
        if (device.GetTouchUp(SteamVR_Controller.ButtonMask.Trigger))
        {
            Debug.Log(""You activated TouchUp on the trigger"");
        }
        if (device.GetPress(SteamVR_Controller.ButtonMask.Trigger))
        {
            Debug.Log(""You are holding 'Press' on the trigger"");
        }
        if (device.GetPressDown(SteamVR_Controller.ButtonMask.Trigger))
        {
            Debug.Log(""You activated press down on the trigger"");
        }
        if (device.GetPressUp(SteamVR_Controller.ButtonMask.Trigger))
        {
            Debug.Log(""You activated press Up on the trigger"");
        }
    }
    void onTriggerStay(Collider col)
    {
        Debug.Log(""You have collided with "" + col.name + "" and activated onTriggerStay"");
        if (device.GetTouch(SteamVR_Controller.ButtonMask.Trigger))
        {
            Debug.Log(""You have collided with "" + col.name + "" while holding down Touch"");
            col.attachedRigidbody.isKinematic = true;
            col.gameObject.transform.SetParent(gameObject.transform);
        }
    }
}

","
This is a simple mistake. It should be OnTriggerStay not onTriggerStay. Please capitalize the O and trigger/collision should be detected.
"
"
i have a little problem. I am using HTC Vive in a unity scene and i have another human rigged character in front of me. I want to force him to move like me (only arms obviously). So suppose that i move up the right arm, he must move up the right arm ( i only move an object like a cube and then the IK do the rest). The problem is that if i am in the same direction of the other character it work, but if i turn around (always with the right arm in front of me) he moves the arm, instead of keep the arm in front of him like he should do. I hope that this is clear otherwise i'll bring up some sketches.
","
I am not sure if I understand clearly, but it seems like you have a problem with origin Y orientation, what you should do is apply
otherplayer.transform.rotation.y = htcplayer.transform.rotation.y

Is that make sense ?
"
"
I'm making a VR application for iOS/Android.
I just bought Easy Movie Texture, using latest Unity version.
I can play one of the video. But I don't know how to play multiple videos.
For example, how do I make a cardboard tap trigger to change a movie?
I’m using the Sphere Example.
I created a gameobject, inside it’s update() function has a if(Carboard.SDK.Triggered) that I want to change the target parameter in sphere (Media Player Control Script). (Change the video)
How do I access this object? Where Exactly that object is created?
","
In short, mediaPlayerCtrl.Load(movieName) changes the movie. The movie must be in StreamingAssets for this to work properly (SD card is possible on Android, but requires more effort).
As you have issues with locating your MediaPlayerCtrl instance, here's an easy solution. Add this script to an example from Easy Movie Texture and use the ChangeMovie method.
using UnityEngine;

public class EMTChangeFile : MonoBehaviour
{

    // drag an instance of Media Player Ctrl or leave empty for autodetection
    public MediaPlayerCtrl mediaPlayerCtrl;

    void Awake ()
    {
        if (mediaPlayerCtrl == null)
        {
            mediaPlayerCtrl = FindObjectOfType<MediaPlayerCtrl>();
            if (mediaPlayerCtrl == null)
                throw new UnityException(""No Media Player Ctrl object in scene"");
        }
    }

    public void ChangeMovie(string movieName, bool autoplay = true)
    {
        mediaPlayerCtrl.Stop();
        mediaPlayerCtrl.Load(movieName);
        if(autoplay)
            mediaPlayerCtrl.Play();
        else
            mediaPlayerCtrl.Stop();
    }

}

"
"
I'm new with both directx and openvr, and trying to wrap my head around how the OpenVR compositor API works. The docs call for rendering each eye and handing each one to the compositor via Submit(). But I'm a bit confused about how Present() factors in to this flow. I expected to need to call Present() to render each eye, but from examining some existing VR games, this doesn't happen. Present is called to render any view on the main (non-VR) monitor, but is not called at all for the stuff drawn by the compositor.
Does somebody else call Present() or something lower-level?
","
Present is to display in a traditional swap chain in a window on your screen. Because with VR, you use an alternative mechanisum and API to present the image to the HMD, you do not need a Present at all.
You only need one if you want to display a copy or anything else on the monitor along side the hmd.
"
"
A team of us have been working in Unity for the last 6 months using Git for source control (I know it might not be the recommended method).
Every time we pull a new commit from Git, we have to rebuild the lighting (or if 'auto' is selected in the lighting preferences Unity will auto re-build the lighting once the scene has been opened).
Should this happen? Re baking all the lighting takes ages and is a real pain if we have to go through it every time a new commit is pulled? Shouldn't the full lighting data be being stored on git along with the baked lighting files and preferences?
","
Use below lighting settings when working on the game other than lighting.
Lighting >> Precomputed Realtime GI >> Realtime Resolution >> 0.2f (Or less)
Default value is 2f.
Downsize atlas from default 1024.
Lighting >> General GI >> Atlas size >> 512. (Or less)
Turn off Ambient Occlusion.
Lighting >> baked GI >> Ambient Occlusion >> 0.
These will not look good but you can work as ease at speed. Bake a better lightmap when have time.
"
"
I am developing a VR game in unity.. here im planning to get the movement through an external controller.. Im able to program the controller for movement based on the mouse inputs.. But the GVR turn inputs are not accssible.. please help me.
","
I'm using Cardboard SDK 0.7, it has a script called UnityEditorDevice.cs
which has code for the ALT key, so i'd modify that (or maybe you can override it elsewhere)
if (Input.GetKey(KeyCode.LeftAlt) || Input.GetKey(KeyCode.RightAlt)) {
mouseX += Input.GetAxis(""Mouse X"") * 5;
if (mouseX <= -180) {
  mouseX += 360;
} else if (mouseX > 180) {
  mouseX -= 360;
}

"
"
We are developing a Gear VR Application using GVRf. There are two GVRScenes in the app, and we want it such that if the user short presses back button in scene B it goes back to scene A, and if the user short presses back button in scene A it exits the app. Also, long pressing the back button should show the Oculus settings menu, and pressing back from that should resume the app(according to Oculus store requirements). 
Since GVRActivity.onBackPressed() is not called from the framework, we tried to override the onKeyUp from GVRActivity:
@Override
public boolean onKeyUp(int keyCode, KeyEvent event) {
    if (KeyEvent.KEYCODE_BACK == keyCode) {
        if (mScript.onBackPressed()) {//mScript.onBackPressed() handles change scene logic 
                                      //and returns true if we are in scene B, and returns  
                                      //false if we are in scene A
             super.onBackPressed();
        }
        return true;
    }
    return super.onKeyUp(keyCode, event);
}

Right now short press from scene B is ok, and long press is showing the oculus settings. The problem is that if we press back from scene B and press back from settings, it switches to scene A, and if we do the same from scene A the app restarts.  
What is the correct way to implement this? Thanks in advance. 
","
We found a work-around for this. 
First, modify GVRActivity's mPaused to protected.
    protected boolean mPaused = true;

And then we added overrode the onKeyUp() in our Activity: 
@Override
public boolean onKeyUp(int keyCode, KeyEvent event) {
    Log.d(TAG, ""onKeyUp: ""+keyCode);
    if(!mPaused && keyCode==KeyEvent.KEYCODE_BACK){
        if(!mScript.onBackPressed())
            return true;
    }
    return super.onKeyUp(keyCode, event);
}

Hope this can help someone else, if anyone has a better solution, feel free to post it. 
"
"
I'm using Unity 5 and OVR utilities, I'd like to avoid the scene being recentered every time I load a new scene. Basically, I'd like the player to be able to sit down and even if he's looking at his right at the moment of a new scene loading, I'd like him to be able to continue playing without having to use some manual recentering or having to change his sitting position. 
I've found 2 other posts with the same question but only for other VR platforms and they didn't solve my problem:
Persist Google Cardboard forward direction across scene loads in Unity3D 4.6.7?
Google Cardboard - how to face the same way when changing scenes?
","
Turns out, if you're using OVR Utilities from Oculus, there's a very simple boolean-based solution that you can set in the Editor:

Find the OVRManager script attached to the OVRCameraRig
Uncheck 'Reset Tracker On Load'

"
"
I'm creating a VR game, where if a user looks at a button, a scene will change.
The problem is that if I load a new scene using SceneManager, the direction that the user is facing resets, making it the new forward. For example, I have a quit button located behind the user. If they look at it, a new scene will load, but the new ""forward"" will be behind them, causing them to have to completely turn around in order for their body to face forward again.
I looked online and came across this post here but this was in Unity 4. It talks about commenting out Recenter() and device.Reset()
I have repeated the steps in Unity 5 (the version I am using) but this has no effect. I cannot see a device.Reset() piece of code in the Unity 5 Google Cardboard scripts either.
How can I keep facing the same direction when I load a new scene from a current scene in play mode?
Thanks!
","
Simply store the All the Camera's position and rotation values on exit. Load them in the Start function. I did something similar about a year ago and will share the script. Just attach it to any GameObject in the scene. Make sure to put cameras in each slot. Move the camera then exit the scene. It should hold the position of all the cameras next time you play again. 
If it stops working, call the reset() function ONCE to reset the saved data then comment it out. It should only be called when it stops working which never happened to me. 
#define USEWITHSECENE

using UnityEngine;
using System.Collections;

public class VRCAMRESTORE : MonoBehaviour
{

    [System.Serializable]
    class VRInfo
    {
        //Transform Info
        [SerializeField]
        public Vector3 mainCameraPos;
        [SerializeField]
        public Vector3 mainCameraLeftPos;
        [SerializeField]
        public Vector3 mainCameraRightPos;
        [SerializeField]
        public Vector3 reticleUnderMainCameraPos;

        [SerializeField]
        public Vector3 PreRenderCameraUnderStereoRenderPos;
        [SerializeField]
        public Vector3 PostRenderCameraUnderStereoRenderPos;

        //Rotation Info
        [SerializeField]
        public Quaternion mainCameraRot;
        [SerializeField]
        public Quaternion mainCameraLeftRot;
        [SerializeField]
        public Quaternion mainCameraRightRot;
        [SerializeField]
        public Quaternion reticleUnderMainCameraRot;

        [SerializeField]
        public Quaternion PreRenderCameraUnderStereoRenderRot;
        [SerializeField]
        public Quaternion PostRenderCameraUnderStereoRenderRot;
    }


    public Camera mainCamera;
    public Camera mainCameraLeft;
    public Camera mainCameraRight;
    public GameObject reticleUnderMainCamera;

    public Camera PreRenderCameraUnderStereoRender;
    public Camera PostRenderCameraUnderStereoRender;

    private VRInfo vrInfo = null;
    private Transform c1, c2, c3, c4, c5, c6;

    //Use this for initialization
    void Start()
    {
        intitTransform();
        vrInfo = new VRInfo();

        //check if vrInfo exist
        string savedJsonValue = PlayerPrefs.GetString(""vrInfo"");
        if (savedJsonValue == null)
        {
            updateCamTransform();
            return;
        }


        //Convert back to class
        vrInfo = JsonUtility.FromJson<VRInfo>(savedJsonValue);

        //If Null, dont load the saved location
        if (vrInfo == null)
        {
            updateCamTransform();
            return;
        }

        //Load Aettings from the Savings
        loadTransform();
    }

    public void reset()
    {
        PlayerPrefs.DeleteKey(""vrInfo"");
    }

    void intitTransform()
    {
        c1 = mainCamera.GetComponent<Transform>();
        c2 = mainCameraLeft.GetComponent<Transform>();
        c3 = mainCameraRight.GetComponent<Transform>();
        c4 = reticleUnderMainCamera.GetComponent<Transform>();
        c5 = PreRenderCameraUnderStereoRender.GetComponent<Transform>();
        c6 = PostRenderCameraUnderStereoRender.GetComponent<Transform>();
    }

    private void loadTransform()
    {
        c1.position = vrInfo.mainCameraPos;
        c1.rotation = vrInfo.mainCameraRot;

        c2.position = vrInfo.mainCameraLeftPos;
        c2.rotation = vrInfo.mainCameraLeftRot;

        c3.position = vrInfo.mainCameraRightPos;
        c3.rotation = vrInfo.mainCameraRightRot;

        c4.position = vrInfo.reticleUnderMainCameraPos;
        c4.rotation = vrInfo.reticleUnderMainCameraRot;

        c5.position = vrInfo.PreRenderCameraUnderStereoRenderPos;
        c5.rotation = vrInfo.PreRenderCameraUnderStereoRenderRot;

        c6.position = vrInfo.PostRenderCameraUnderStereoRenderPos;
        c6.rotation = vrInfo.PostRenderCameraUnderStereoRenderRot;
    }

    bool firstRun = true;

    private void updateCamTransform()
    {
        //Prevents vrInfo == null from running every frame
        if (firstRun)
        {
            firstRun = false;
            if (vrInfo == null)
            {
                vrInfo = new VRInfo();
            }
        }

        vrInfo.mainCameraPos = c1.position;
        vrInfo.mainCameraRot = c1.rotation;

        vrInfo.mainCameraLeftPos = c2.position;
        vrInfo.mainCameraLeftRot = c2.rotation;

        vrInfo.mainCameraRightPos = c3.position;
        vrInfo.mainCameraRightRot = c3.rotation;

        vrInfo.reticleUnderMainCameraPos = c4.position;
        vrInfo.reticleUnderMainCameraRot = c4.rotation;

        vrInfo.PreRenderCameraUnderStereoRenderPos = c5.position;
        vrInfo.PreRenderCameraUnderStereoRenderRot = c5.rotation;

        vrInfo.PostRenderCameraUnderStereoRenderPos = c6.position;
        vrInfo.PostRenderCameraUnderStereoRenderRot = c6.rotation;
    }

    void saveToDrive()
    {
        string json = JsonUtility.ToJson(vrInfo);
        PlayerPrefs.SetString(""vrInfo"", json);
        PlayerPrefs.Save();
        Debug.Log(json);
        Debug.Log(""Quit"");
    }

#if USEWITHSECENE
    void Update()
    {
        updateCamTransform();
    }

    void OnDisable()
    {
        saveToDrive();
    }
#else
    //For iOS
#if UNITY_IOS
    public void OnApplicationPause(bool pauseStatus)
    {
        if (pauseStatus)
        {
            //Convert to Json and Save
            updateCamTransform();
            saveToDrive();
        }
    }
#else
    //For Other Devices 
    public void OnApplicationQuit()
    {
        //Convert to Json and Save
        updateCamTransform();
        saveToDrive();
    }
#endif
#endif
}

"
"
I'm currently reading some code (specifically example code for the Oculus Mobile SDK which just renders some coloured cubes floating around) where he retrieves a vertex shader attribute with glGetAttribLocation (in the shader it's a mat4, it's the transform matrix), then a little further down he does some stuff where he adds to that shader attribute index.
VertexTransformAttribute = glGetAttribLocation( Program.Program, ""VertexTransform"" );

//some stuff happens in between

for ( int i = 0; i < 4; i++ )
{
    GL( glEnableVertexAttribArray( VertexTransformAttribute + i ) );
    GL( glVertexAttribPointer( VertexTransformAttribute + i, 4, GL_FLOAT,
                            false, 4 * 4 * sizeof( float ), (void *)( i * 4 * sizeof( float ) ) ) );
    GL( glVertexAttribDivisor( VertexTransformAttribute + i, 1 ) );
}

To some extent, I understand that he's doing something with the transform matrix for all cubes, but I'm not sure what adding to the index does. 
Any help explaining this part would be appreciated!
","
Every vertex attribute delivers 4 values to OpenGL, even if you don't use all 4 of them. But in GLSL, you can use matrices and arrays of types for the type of a vertex shader input variable. Since these take more than 4 values, you need more than 1 attribute to deliver the data, despite having only declared one actual variable.
So the attribute location for the input variable's name is the first attribute index. And OpenGL automatically assigns the next X attribute indices, consecutively, to fulfill the request for your variable. In the case of a mat4, that is a 4x4 matrix. So OpenGL needs to assign it 4 attributes total, with each attribute representing a single column of the matrix. These are assigned in order, starting from the attribute index associated with the variable name and ending 3 attribute indices later.
"
"
I use vuforia to create android application for virtual reality. 
If I have models like .obj I used JPCT library for loading models.
Now I have models in collada file (.dea).
I cannot find any library for loading this collada files.
I there any way to load collada files to vuforia?
Thank you.
","
No, Vuforia does not import collada files.
https://developer.vuforia.com/forum/faq/technical-what-3d-model-formats-does-vuforia-support
If you use Vuforia in Unity, Unity will handle loading your collada file:
http://answers.unity3d.com/questions/167548/collada-import-with-textures.html
Update:
The steps you'd need to take are:

Download and install Unity 5.

Download the Vuforia Unity bundle. (It's the third option.)

Create a new Unity project (File > New Project...) and Import the Vuforia bundle (Assets > Import Package > Custom Package...)

FIRST drag the collada textures into Unity's Project pane, followed by the .dae files.
When you've confirmed this works for you, then you have the larger tasks of migrating your native Android Vuforia project into Unity. This requires moving from Java to C#. Then build and run on Android.


"
"
Visually speaking, the ""displayed image"" (in the steam/vive window) looks very similar to any other game being rendered on the desktop. Eg: Counterstrike, WoW, etc. 
Question: Why is it then these games don't ""feel"" like being in a VR environment?
Also, programmatically speaking (image rendering, camera angles, depth field, etc)
Question: Can a non-VR game work with the VR sets as long as you configure the controls to the headset and wands? Eg: Headset = joystick; wand buttons = menu etc.
Thank you.
Edit: Please let me know if you have any reading recommendations on this subject. 
","
The non-VR games simply weren't made for VR. 
That said, there are hacks that make non-VR games semi-work in VR. You can check out Vorpx for Oculus, but I don't know of anything for Vive. There will be very big issues and headaches, though.
A lot of things will look bad - like missing graphics as almost all games go through shortcuts so they don't render what you will not see. For example there is no sky in RTS games and the map ends just after the end of scrollable space. Or when you're driving a car in a race game, there probably isn't even more to the car then the dashboard (no seats, back of the car etc). No one should see them, so no one made them.
It's even worse with the user interface of these games, no one had depth in mind when they designed this, so you'll have an ammo counter that makes you cross eyes end such.
I could go on and on with the issues, as this is just the tip of the iceberg.
"
"
I'm trying to install Oculus DK2, and that for the first time. My computer is fresh and I didn't have any prior installation of Oculus. But I keep getting this error message:
Sorry, we encountered an error during installation. Please restart your computer and try running Oculus Setup again
Does anybody know how I can fix it? 
P.S. Same problem has been reported before, and they suggested removing Oculus folder inside Program Files. But there is no such folder there for me.
Also, I have MCoffee antivirus on which is mandated by the university. I can not turn it off.
Actually when I download the setup, there is no a single executable setup. It is an online installer which apparently downloads the setup files to a local folder. Here is how that local folder looks like. But there is no msi installer for setup here except the visual CPP installer ones! I'm confused!

","
The installation error was due to McAfee Enterprise antivirus. We disabled it and installation was successful. 
"
"
I am currently able to invoke all inputs on my Vive controller in unity except for directional input from the touch pad.
How can I access the direction the user is pressing?
Here is what I have been trying to use ""EVRButtonId.k_EButton_DPad_Up"" but have had no success.
","
I had the same issue. What I did to circumvent this issue was:
    if(Controller.GetAxis(Valve.VR.EVRButtonId.k_EButton_Axis0)[0] > 0.8f
        && Controller.GetPressDown(Valve.VR.EVRButtonId.k_EButton_SteamVR_Touchpad))
    {
        // code continues...
    }

This seemed to work. Basically, i just used the fact that we know when the pad is pressed down and we know where on the pad user is touching. 
This particular example was for the Right side of the touchpad using the x-axis (GetAxis(Valve.VR.EVRButtonId.k_EButton_Axis0)[0] . I am unsure if this is the best way, and i probably doubt it, but it works and there is not a lot out there currently in terms of help.
"
"
If i am at location 1 and hit it,after the delay of 10 sec arrows to the location 3 generates.
Now I move to location 3, when I hit location 3, Arrows generated by location 2 should turn off.
I tried hard to get my Deactivate previous function working but failed. Can anybody help regarding this?
 using UnityEngine;
 using System.Collections;

public class GameController : MonoBehaviour {

public int _currentCheckPoint = 0;
public ArrowObjects _arrowObjectInstance;
public float _checkPointWaiTime = 10;
public float _elappsedTime;
public bool GenerateArrow = false;

    void Start()
    {
            _arrowObjectInstance = GetComponent<ArrowObjects> ();
            foreach (var item in _arrowObjectInstance._listArrowGameObject) {
                    item.SetActive (false);
            }
    }

    void Update()
    {

    if (_currentCheckPoint != 0 && GenerateArrow) {
        GenerateArrow = false;
        Invoke (""DelayedActive"", 10f);

        }
    print (_currentCheckPoint);
    }

void DelayedActive()
{
    _arrowObjectInstance._listArrowGameObject [_currentCheckPoint - 1].SetActive (true);

}

void DeactivePrevious()
{
    _arrowObjectInstance._listArrowGameObject [_currentCheckPoint - 1].SetActive (false);
}


}

","
Changed the update part from the above script to this.
void Update()
    {

    if (_currentCheckPoint != 0 && GenerateArrow) {
        GenerateArrow = false;
        if (_currentCheckPoint > 1) {
            _arrowObjectInstance._listArrowGameObject [_currentCheckPoint - 2].SetActive (false);
        }
        Invoke (""DelayedActive"", 10f);


        }

and added a check in 
"
"
I'm in the process of developing a chrome VR web app. Now I'm desperately trying to figure out how to render a website into my into my stereoscopic scene which has some meshes in it. 
So I have my renderer for the meshes, which works well. The following code is only the relevant snippets:
var renderer = new THREE.WebGLRenderer();

Then i have my stereoeffect renderer which receives the webgl renderer:
var effect = new THREE.StereoEffect(renderer);

Next is that I create the website renderer, and apply it to the stereoeffect:
var rendererCSS = new THREE.CSS3DRenderer();
var effectHUD = new THREE.StereoEffect(rendererCSS);

Then I have scenes which are being rendered:
var scene = new THREE.Scene();
var sceneCSS = new THREE.Scene();
function render(dt) {
      effect.render(scene, camera);
      effectHUD.render( sceneCSS, camera );
    }

No what I receive is this: 

A stereoscopic image of my mesh, but a non stereoscopic image of the website. The problem is that the when I try to add the css renderer into the StereoEffect renderer, the setViewport function (among others, probably) are missing. 
I've tried adding the website object to the webgl renderer, but it doesn't want to render my website, so adding the website object to the scene with the meshes doesn't work. 
Any help is greatly appreciated!
","
I've been having the same problem.
It appears there was a CSS Stereo renderer but it's been removed along with all examples. This could have been for any number of reasons so use the following with caution, or until they reintroduce compatibility:
After searching I found a remaining copy of their demo from which you can mine the code and copy its CSS3DStereoRenderer.js file.
Hopefully this helps.
"
"







Closed. This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet Stack Overflow guidelines. It is not currently accepting answers.
                                
                            











 We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.


Closed 6 years ago.







                        Improve this question
                    



I'm looking for a VR solution (presumably software for an Oculus or Vive device) that presents 2 to 4 high-resolution development displays. The main requirement is that it provides all the functionality a software developer needs for part-time use.
Although I welcome you to include your favorite solution and note its benefits, as StackOverflow is not for product comparison any solution will answer my question.
It's OK if this isn't suitable for full work-week use, such as concerns about eyestrain from 8 hours of straight use (although severe eyestrain after short use would make it unusable), but it's not OK if I lose key currently-standard functionality commonly used by most developers, such as ability to run multiple displays and snap an IDE to fill one and a browser to fill another, ability to have a high resolution or high virtual resolution, poor text quality, or ability to use a debugger.
I'd prefer one that I can use with a laptop as a mobile solution outside (yes, I know, the headset ruins the beach), but it isn't required.
Does something like that exist?
","
Check out Virtual Desktop. The resolution of the Rift & Vive is still a bit low to use this for coding & development but it's got a lot of potential!
"
"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 8 years ago.







                        Improve this question
                    



Let's imagine we have some C++ OpenGL game. It uses our own engine for render (nor Unity, nor UE, etc). Let's simplify our problem. 
For example, we need to render some simple cube in VR mode. What we should do for that?
I know we need to split our screen into two parts. But what then? How to calculate rotation, distance for both parts?
By VR I mean devices such as VR Box, Gear VR, Oculus Rift, etc.
","
All the major headsets have API documentation on their sites that explain how to integrate VR support into your engine. You should refer to that documentation for details. My experience is mostly with the Oculus SDK but other SDKs are similar.
You generally don't directly split the screen into two yourself - you provide images with left and right eye views to the SDK and the SDK performs warping for the lens optics and sends the outputs to the HMD display(s).
The SDK provides APIs to get the camera and viewport parameters you need to render each eye's view. With the Oculus SDK you also obtain your render targets for each eye view through API calls. You build view and projection matrices and set viewports for each eye view based on the information provided to you by the APIs for the HMD position, orientation, Field of View, target resolution, etc.
Rendering for each eye is essentially the same as whatever you are already doing in your engine but of course you have to render twice (once for each eye) using the camera and viewport information provided by the SDK and may wish to render a third view for display on the regular monitor. You may want to restructure parts of your engine for efficiency since the left and right eye views are very similar rather than naively render the entire scene twice but that is not strictly necessary.
There will probably be a call at the end of a frame to tell the SDK you've finished rendering and submit the completed eye buffers for display. Other than that there's not that much to it. Most of the challenge of VR rendering lies in achieving the required performance not in the integration of the SDKs which are fairly simple on the display side of things.
"
"
I have created a VR app in Unity with Google Cardboard SDK and am now attempting to deploy it to Web and as a standalone. The game works well on iPhone and Android and the user navigates the game by turning their head and touching the screen (the ""Fire1"" command I believe). The problem comes when I switch platforms.
In the editor when testing, even though my computer is not strapped to my head I have been able to still play the game just as well by holding down the Alt key to turn and just clicking the screen to move. I assumed this would be the same when deploying to a standalone but I'm having a problem:
When deploying to web and standalone, the user cannot move at all thus I'm not even sure if the game is working at all - Holding the alt key does nothing and clicking the screen does nothing. They cannot play the game. 
What is wrong here? Are there other inputs that I can implement specifically for standalone to replace head tracking, etc? How are other Google Cardboard VR apps deployed to web?
In my WebplayerDevice script: 
//For web support

#if UNITY_WEBPLAYER

using UnityEngine;
using System.Runtime.InteropServices;
using System.Collections.Generic;

public class CardboardWebplayerDevice : BaseCardboardDevice {

    // Simulated neck model.  Vector from the neck pivot point to the point between the eyes.
    private static readonly Vector3 neckOffset = new Vector3(0, 0.075f, 0.08f);

    // Use mouse to emulate head in the editor.
    private float mouseX = 0;
    private float mouseY = 0;
    private float mouseZ = 0;

    public override void Init() {
        Input.gyro.enabled = true;
        Debug.Log (""ITS WEB!!"");

    }

    public override bool SupportsNativeDistortionCorrection(List<string> diagnostics) {
        return false;  // No need for diagnostic message.
    }

    public override bool SupportsNativeUILayer(List<string> diagnostics) {
        return false;  // No need for diagnostic message.
    }

    // Since we can check all these settings by asking Cardboard.SDK, no need
    // to keep a separate copy here.
    public override void SetUILayerEnabled(bool enabled) {}
    public override void SetVRModeEnabled(bool enabled) {}
    public override void SetDistortionCorrectionEnabled(bool enabled) {}
    public override void SetStereoScreen(RenderTexture stereoScreen) {}
    public override void SetSettingsButtonEnabled(bool enabled) {}
    public override void SetAlignmentMarkerEnabled(bool enabled) {}
    public override void SetVRBackButtonEnabled(bool enabled) {}
    public override void SetShowVrBackButtonOnlyInVR(bool only) {}
    public override void SetNeckModelScale(float scale) {}
    public override void SetAutoDriftCorrectionEnabled(bool enabled) {}
    public override void SetElectronicDisplayStabilizationEnabled(bool enabled) {}
    public override void SetTapIsTrigger(bool enabled) {}

    private Quaternion initialRotation = Quaternion.identity;

    private bool remoteCommunicating = false;
    private bool RemoteCommunicating {
        get {
            if (!remoteCommunicating) {
remoteCommunicating = Vector3.Dot(Input.gyro.rotationRate, Input.gyro.rotationRate) > 0.05;
            }
            return remoteCommunicating;
        }
    }

    public override void UpdateState() {
        Quaternion rot;
        if (Cardboard.SDK.UseUnityRemoteInput && RemoteCommunicating) {
            var att = Input.gyro.attitude * initialRotation;
            att = new Quaternion(att.x, att.y, -att.z, -att.w);
            rot = Quaternion.Euler(90, 0, 0) * att;
        } else {
            bool rolled = false;
            if (Input.GetKey(KeyCode.LeftAlt) || Input.GetKey(KeyCode.RightAlt)) {
                mouseX += Input.GetAxis(""Mouse X"") * 5;
                if (mouseX <= -180) {
                    mouseX += 360;
                } else if (mouseX > 180) {
                    mouseX -= 360;
                }
                mouseY -= Input.GetAxis(""Mouse Y"") * 2.4f;
                mouseY = Mathf.Clamp(mouseY, -85, 85);
            } else if (Input.GetKey(KeyCode.LeftControl) || Input.GetKey(KeyCode.RightControl)) {
                rolled = true;
                mouseZ += Input.GetAxis(""Mouse X"") * 5;
                mouseZ = Mathf.Clamp(mouseZ, -85, 85);
            }
            if (!rolled && Cardboard.SDK.autoUntiltHead) {
                // People don't usually leave their heads tilted to one side for long.
                mouseZ = Mathf.Lerp(mouseZ, 0, Time.deltaTime / (Time.deltaTime + 0.1f));
            }
            rot = Quaternion.Euler(mouseY, mouseX, mouseZ);
        }
        var neck = (rot * neckOffset - neckOffset.y * Vector3.up) * Cardboard.SDK.NeckModelScale;
        headPose.Set(neck, rot);

        triggered = Input.GetMouseButtonDown(0);
        tilted = Input.GetKeyUp(KeyCode.Escape);
    }


    public override void PostRender() {
        // Do nothing.
    }

    public override void UpdateScreenData() {
        Profile = CardboardProfile.GetKnownProfile(Cardboard.SDK.ScreenSize, Cardboard.SDK.DeviceType);
        ComputeEyesFromProfile();
        profileChanged = true;
    }

    public override void Recenter() {
        mouseX = mouseZ = 0;  // Do not reset pitch, which is how it works on the phone.
        if (RemoteCommunicating) {
            //initialRotation = Quaternion.Inverse(Input.gyro.attitude);
        }
    }
}
#endif

Errors (even when I changed the Webplayer device class to BaseVRDevice this made no difference) :
","
The compile errors you are seeing are because the variables you are referring to in the Cardboard.cs script (autoUntiltHead, Screen, Device, etc) are all in a #if UNITY_EDITOR section.  Change that to #if UNITY_EDITOR || UNITY_WEBPLAYER, or just remove the #if.  Then you'll be able to compile.
"
"
I am creating a VR app in Unity3d using Google cardboard and need to know how to record the path taken by the player (they are traversing a maze). Is there a way of drawing the path taken by the user (possibly in the console; not in the actual game and not visible to the user) and saving this path as an image? 
I need to save an image or just a line of where the player went in the game so that I can then email this image/data to the player.. What is the best way to accomplish this?
","
You need to store the Player's path in List as Vector3. Then you can use LineRenderer to draw the line.change the vertext amount of the LineRenderer to List.Count with LineRenderer.SetVertexCount then loop over the List and change the position of the LineRenderer with LineRenderer.SetPosition(loopIndex,playersPo[loopIndex]).
List<Vector3> playerPos = new List<Vector3>();

//Store players positions somewhere
//playerPos.Add(pPos);
//playerPos.Add(pPos); 
//playerPos.Add(pPos);


Color red = Color.red;
LineRenderer lineRenderer = gameObject.AddComponent<LineRenderer>();
lineRenderer.material = new Material(Shader.Find(""Particles/Additive""));
lineRenderer.SetColors(red, red);
lineRenderer.SetWidth(0.2F, 0.2F);

//Change how mant points based on the mount of positions is the List
lineRenderer.SetVertexCount(playerPos.Count);

for (int i = 0; i < playerPos.Count; i++ )
{
    //Change the postion of the lines
    lineRenderer.SetPosition(i, playerPos[i]);
}

"
"
I have a VR Scene with a C# Script on the camera that allows the user to Click Once to move and again to stop. 
    public float speed = 1.0f;
public bool startedMoving = true;
public GameObject myCam;


    // Update is called once per frame
    void Update () {

        if (startedMoving) {
            transform.position += myCam.transform.forward * speed * Time.deltaTime;
        }

//      if(Input.GetButtonDown(""Fire1"")){
        if (Input.GetMouseButton(0)){
            startedMoving = !startedMoving;
        }
    }

What I want to know is how I can CLICK & HOLD to move Backwards..?
Thank you!
~ b
","
Use enum the represent the status of the mouse instead of startedMoving or multiple booleans that will make everything easier to implement. The comment in the code describes how it works.
using UnityEngine;
using System.Collections;

public class ClickAndHeld : MonoBehaviour
{
    public GameObject myCam;
    CLICK_MODE clickMode = CLICK_MODE.NO_CLICK;
    MOVE_DIRECTION moveDir = MOVE_DIRECTION.IDLE;

    public float speed = 1.0f;


    //If down for 0.5 secods the it is considered Click and Hold instead of Click
    float clickHoldDetectTime = 0.5f;
    float clickCounter = 0; //Dont not change

    void Start()
    {
        StartCoroutine(mover());
    }

    // Update is called once per frame
    void Update()
    {

        if (Input.GetMouseButton(0))
        {

            //If in NO_CLICK, set to CLICKED
            if (clickMode == CLICK_MODE.NO_CLICK)
            {
                //Change the mode to CLICKED
                clickMode = CLICK_MODE.CLICKED;
            }

            //If in CLICKED mode, start counting to clickHoldDetectTime
            if (clickMode == CLICK_MODE.CLICKED)
            {
                clickCounter += Time.deltaTime; //Increment counter

                //Check if we have reached the clickHoldDetectTime time
                if (clickCounter > clickHoldDetectTime)
                {
                    //Reset Counter
                    clickCounter = 0;

                    //Change the mode to CLICK_AND_HELD
                    clickMode = CLICK_MODE.CLICK_AND_HELD;
                }
            }

            //If STILL down and the the current mode is CLICK_AND_HELD then do clickedAndHeldDown stuff
            if (clickMode == CLICK_MODE.CLICK_AND_HELD)
            {
                clickedAndHeldDown();
            }
        }
        else
        {
            //If released and the current mode is CLICKED then do clicked stuff
            if (clickMode == CLICK_MODE.CLICKED)
            {
                clicked();
            }

            //If released and the current mode is CLICK_AND_HELD, change to RELEASED then do relased stuff
            if (clickMode == CLICK_MODE.CLICK_AND_HELD)
            {
                //Change the mode to RELEASED
                clickMode = CLICK_MODE.RELEASED;
                mouseReleasedAfterBeingHeld();
            }


            //Reset each time mouse button is released
            reset();
        }

    }

    IEnumerator mover()
    {
        while (true)
        {

            if (moveDir == MOVE_DIRECTION.IDLE)
            {

            }

            //Move Forward
            if (moveDir == MOVE_DIRECTION.FORWARD)
            {

                transform.position += myCam.transform.forward * speed * Time.deltaTime;
            }

            //Move Backward
            if (moveDir == MOVE_DIRECTION.BACKWARD)
            {
                transform.position -= myCam.transform.forward * speed * Time.deltaTime;
            }
            yield return null;
        }
    }

    private void clicked()
    {
        Debug.Log(""CLICKED"");

        //If Idle, become Forward
        if (moveDir == MOVE_DIRECTION.IDLE)
        {
            moveDir = MOVE_DIRECTION.FORWARD;
        }

        //If forward, moves become idle
        else if (moveDir == MOVE_DIRECTION.FORWARD)
        {
            moveDir = MOVE_DIRECTION.IDLE;
        }

         //--------------------------------------------------
        //If backward, moves become idle
        else if (moveDir == MOVE_DIRECTION.BACKWARD)
        {
            moveDir = MOVE_DIRECTION.IDLE;
        }
    }

    private void clickedAndHeldDown()
    {
        Debug.Log(""CLICKED AND HELD"");

        //If Idle, becomes backward
        if (moveDir == MOVE_DIRECTION.IDLE)
        {
            moveDir = MOVE_DIRECTION.BACKWARD;
        }
    }


    //Called when released after being RELEASED from CLICKED_HELD
    private void mouseReleasedAfterBeingHeld()
    {
        Debug.Log(""RELEASED AFTER CLICKED AND HELD"");

        //If backward, move becomes idle
        if (moveDir == MOVE_DIRECTION.BACKWARD)
        {
            moveDir = MOVE_DIRECTION.IDLE;
        }

        //--------------------------------------------------
        //If forward, move becomes idle
        else if (moveDir == MOVE_DIRECTION.FORWARD)
        {
            moveDir = MOVE_DIRECTION.IDLE;
        }
    }

    void reset()
    {
        clickMode = CLICK_MODE.NO_CLICK;
        clickCounter = 0;
    }
}

public enum CLICK_MODE
{
    NO_CLICK, CLICKED, CLICK_AND_HELD, RELEASED
}

public enum MOVE_DIRECTION
{
    IDLE, FORWARD, BACKWARD
}

"
"
I am developing an Android application using Unity3D for the Samsung Gear VR, with the Samsung Galaxy S7.
In my app, I use the bluetooth to create a local network between two devices. When the system popup asking permissions to enable bluetooth discoverability is supposed to appear in my app, the whole app freeze, the popup is not shown and I have to reboot the phone to use it again. 
Is there a solution to maybe ask for that permission before my app is launch? For example, another request, regarding the access to local file system is asked before the splash screen of my app, but I don't know why and how this permission is asked at this time.
Thanks in advance,
","
This is a bug that is now fixed in Unity 5.4.0B15. 
Unity 5.4.0B15 Android Marshmallow - Fixed issues when querying for custom permissions.
Update to 5.4.0B15 to fix this problem.
Notice: 
In Android 6.0 Marshmallow, Android permission is now asked when during run-time instead of install time. There is nothing you can do about that. 
The first time permission is asked, you get something like this

The second time, the player will have a chance to choose Never ask again.

The reason for this is to give the user complete control of which permission to allow and which ones to decline. For example, you can allow WiFi but reject Bluetooth unlike before, when you had to either allow every permission or reject all of them.
"
"
I'm looking at the Google Cardboard kit, an inexpensive VR setup that uses Android Devices to play 3d VR games. I see that they have an Android demo project, but is there any iOS or Objective-C port of the Cardboard project? 
If not, are there any other VR projects for iOS that can be modified to work with the cardboard kit?

","
June 2015 update
Google has added iOS support to their official Unity SDK. Even though the Unity plugin bundles a pre-compiled iOS Cardboard binary, there doesn't seem to be an easy way to use it in a standalone fashion.

I just completed an iOS CardboardSDK port for the company I'm working for. 
It (mostly) has feature parity with Android's CardboardSDK v0.5.1. Works on iOS 7+. Tested on iPhone 6 and iPhone 5.
You can check it out here: CardboardSDK-iOS.
"
"
I need to get value from another script but I keep getting this error that says 

The type name 'head' does not exist in the type 'SteamVR_Camera'.

My code:
using UnityEngine;
using System.Collections;
using UnityEngine.VR;

public class HMDHelper : MonoBehaviour
{
    private SteamVR_Camera.head.localPosition HMDLocalPos; //Error is thrown here.

    void Start()
    {
    }

    // Update is called once per frame
    void Update()
    {
        if (Input.GetKeyDown(""g""))
        {
            AutoRotate();
        }
    }

    void AutoRotate()
    {
        HMDLocalPos = InputTracking.GetLocalPosition(Head);
        Debug.Log(HMDLocalPos);
    }
}

What exactly do I have to do to fix error?
This is the script that I retrieved the other value (HMDLocalPos) from...
//========= Copyright 2014, Valve Corporation, All rights reserved. ===========
//
// Purpose: Adds SteamVR render support to existing camera objects
//
//=============================================================================

using UnityEngine;
using System.Collections;
using System.Reflection;
using Valve.VR;

[RequireComponent(typeof(Camera))]
public class SteamVR_Camera : MonoBehaviour
{
    [SerializeField]
    private Transform _head;
    public Transform head { get { return _head; } }
    public Transform offset { get { return _head; } } // legacy
    public Transform origin { get { return _head.parent; } }

    [SerializeField]
    private Transform _ears;
    public Transform ears { get { return _ears; } }

    public Ray GetRay()
    {
        return new Ray(_head.position, _head.forward);
    }

    public bool wireframe = false;

    [SerializeField]
    private SteamVR_CameraFlip flip;

    #region Materials

    static public Material blitMaterial;

    // Using a single shared offscreen buffer to render the scene.  This needs to be larger
    // than the backbuffer to account for distortion correction.  The default resolution
    // gives us 1:1 sized pixels in the center of view, but quality can be adjusted up or
    // down using the following scale value to balance performance.
    static public float sceneResolutionScale = 1.0f;
    static private RenderTexture _sceneTexture;
    static public RenderTexture GetSceneTexture(bool hdr)
    {
        var vr = SteamVR.instance;
        if (vr == null)
            return null;

        int w = (int)(vr.sceneWidth * sceneResolutionScale);
        int h = (int)(vr.sceneHeight * sceneResolutionScale);
        int aa = QualitySettings.antiAliasing == 0 ? 1 : QualitySettings.antiAliasing;
        var format = hdr ? RenderTextureFormat.ARGBHalf : RenderTextureFormat.ARGB32;

        if (_sceneTexture != null)
        {
            if (_sceneTexture.width != w || _sceneTexture.height != h || _sceneTexture.antiAliasing != aa || _sceneTexture.format != format)
            {
                Debug.Log(string.Format(""Recreating scene texture.. Old: {0}x{1} MSAA={2} [{3}] New: {4}x{5} MSAA={6} [{7}]"",
                    _sceneTexture.width, _sceneTexture.height, _sceneTexture.antiAliasing, _sceneTexture.format, w, h, aa, format));
                Object.Destroy(_sceneTexture);
                _sceneTexture = null;
            }
        }

        if (_sceneTexture == null)
        {
            _sceneTexture = new RenderTexture(w, h, 0, format);
            _sceneTexture.antiAliasing = aa;

#if (UNITY_5_3 || UNITY_5_2 || UNITY_5_1 || UNITY_5_0)
            // OpenVR assumes floating point render targets are linear unless otherwise specified.
            var colorSpace = (hdr && QualitySettings.activeColorSpace == ColorSpace.Gamma) ? EColorSpace.Gamma : EColorSpace.Auto;
            SteamVR.Unity.SetColorSpace(colorSpace);
#endif
        }

        return _sceneTexture;
    }

    #endregion

    #region Enable / Disable

    void OnDisable()
    {
        SteamVR_Render.Remove(this);
    }

    void OnEnable()
    {
#if !(UNITY_5_3 || UNITY_5_2 || UNITY_5_1 || UNITY_5_0)
        // Convert camera rig for native OpenVR integration.
        var t = transform;
        if (head != t)
        {
            Expand();

            t.parent = origin;

            while (head.childCount > 0)
                head.GetChild(0).parent = t;
            DestroyImmediate(head.gameObject);
            _head = t;
        }

        if (flip != null)
        {
            DestroyImmediate(flip);
            flip = null;
        }

        if (!SteamVR.usingNativeSupport)
        {
            enabled = false;
            return;
        }
#else
        // Bail if no hmd is connected
        var vr = SteamVR.instance;
        if (vr == null)
        {
            if (head != null)
            {
                head.GetComponent<SteamVR_GameView>().enabled = false;
                head.GetComponent<SteamVR_TrackedObject>().enabled = false;
            }

            if (flip != null)
                flip.enabled = false;

            enabled = false;
            return;
        }

        // Ensure rig is properly set up
        Expand();

        if (blitMaterial == null)
        {
            blitMaterial = new Material(Shader.Find(""Custom/SteamVR_Blit""));
        }

        // Set remaining hmd specific settings
        var camera = GetComponent<Camera>();
        camera.fieldOfView = vr.fieldOfView;
        camera.aspect = vr.aspect;
        camera.eventMask = 0;           // disable mouse events
        camera.orthographic = false;    // force perspective
        camera.enabled = false;         // manually rendered by SteamVR_Render

        if (camera.actualRenderingPath != RenderingPath.Forward && QualitySettings.antiAliasing > 1)
        {
            Debug.LogWarning(""MSAA only supported in Forward rendering path. (disabling MSAA)"");
            QualitySettings.antiAliasing = 0;
        }

        // Ensure game view camera hdr setting matches
        var headCam = head.GetComponent<Camera>();
        if (headCam != null)
        {
            headCam.hdr = camera.hdr;
            headCam.renderingPath = camera.renderingPath;
        }
#endif
        ears.GetComponent<SteamVR_Ears>().vrcam = this;
        SteamVR_Render.Add(this);
    }

    #endregion

    #region Functionality to ensure SteamVR_Camera component is always the last component on an object

    void Awake() { ForceLast(); }

    static Hashtable values;

    public void ForceLast()
    {
        if (values != null)
        {
            // Restore values on new instance
            foreach (DictionaryEntry entry in values)
            {
                var f = entry.Key as FieldInfo;
                f.SetValue(this, entry.Value);
            }
            values = null;
        }
        else
        {
            // Make sure it's the last component
            var components = GetComponents<Component>();

            // But first make sure there aren't any other SteamVR_Cameras on this object.
            for (int i = 0; i < components.Length; i++)
            {
                var c = components[i] as SteamVR_Camera;
                if (c != null && c != this)
                {
                    if (c.flip != null)
                        DestroyImmediate(c.flip);
                    DestroyImmediate(c);
                }
            }

            components = GetComponents<Component>();

#if !(UNITY_5_3 || UNITY_5_2 || UNITY_5_1 || UNITY_5_0)
            if (this != components[components.Length - 1])
            {
#else
            if (this != components[components.Length - 1] || flip == null)
            {
                if (flip == null)
                    flip = gameObject.AddComponent<SteamVR_CameraFlip>();
#endif
                // Store off values to be restored on new instance
                values = new Hashtable();
                var fields = GetType().GetFields(BindingFlags.Instance | BindingFlags.NonPublic | BindingFlags.Public);
                foreach (var f in fields)
                    if (f.IsPublic || f.IsDefined(typeof(SerializeField), true))
                        values[f] = f.GetValue(this);

                var go = gameObject;
                DestroyImmediate(this);
                go.AddComponent<SteamVR_Camera>().ForceLast();
            }
        }
    }

    #endregion

    #region Expand / Collapse object hierarchy

#if UNITY_EDITOR
    public bool isExpanded { get { return head != null && transform.parent == head; } }
#endif
    const string eyeSuffix = "" (eye)"";
    const string earsSuffix = "" (ears)"";
    const string headSuffix = "" (head)"";
    const string originSuffix = "" (origin)"";
    public string baseName { get { return name.EndsWith(eyeSuffix) ? name.Substring(0, name.Length - eyeSuffix.Length) : name; } }

    // Object hierarchy creation to make it easy to parent other objects appropriately,
    // otherwise this gets called on demand at runtime. Remaining initialization is
    // performed at startup, once the hmd has been identified.
    public void Expand()
    {
        var _origin = transform.parent;
        if (_origin == null)
        {
            _origin = new GameObject(name + originSuffix).transform;
            _origin.localPosition = transform.localPosition;
            _origin.localRotation = transform.localRotation;
            _origin.localScale = transform.localScale;
        }

        if (head == null)
        {
            _head = new GameObject(name + headSuffix, typeof(SteamVR_GameView), typeof(SteamVR_TrackedObject)).transform;
            head.parent = _origin;
            head.position = transform.position;
            head.rotation = transform.rotation;
            head.localScale = Vector3.one;
            head.tag = tag;

            var camera = head.GetComponent<Camera>();
            camera.clearFlags = CameraClearFlags.Nothing;
            camera.cullingMask = 0;
            camera.eventMask = 0;
            camera.orthographic = true;
            camera.orthographicSize = 1;
            camera.nearClipPlane = 0;
            camera.farClipPlane = 1;
            camera.useOcclusionCulling = false;
        }

        if (transform.parent != head)
        {
            transform.parent = head;
            transform.localPosition = Vector3.zero;
            transform.localRotation = Quaternion.identity;
            transform.localScale = Vector3.one;

            while (transform.childCount > 0)
                transform.GetChild(0).parent = head;

            var guiLayer = GetComponent<GUILayer>();
            if (guiLayer != null)
            {
                DestroyImmediate(guiLayer);
                head.gameObject.AddComponent<GUILayer>();
            }

            var audioListener = GetComponent<AudioListener>();
            if (audioListener != null)
            {
                DestroyImmediate(audioListener);
                _ears = new GameObject(name + earsSuffix, typeof(SteamVR_Ears)).transform;
                ears.parent = _head;
                ears.localPosition = Vector3.zero;
                ears.localRotation = Quaternion.identity;
                ears.localScale = Vector3.one;
            }
        }

        if (!name.EndsWith(eyeSuffix))
            name += eyeSuffix;
    }

    public void Collapse()
    {
        transform.parent = null;

        // Move children and components from head back to camera.
        while (head.childCount > 0)
            head.GetChild(0).parent = transform;

        var guiLayer = head.GetComponent<GUILayer>();
        if (guiLayer != null)
        {
            DestroyImmediate(guiLayer);
            gameObject.AddComponent<GUILayer>();
        }

        if (ears != null)
        {
            while (ears.childCount > 0)
                ears.GetChild(0).parent = transform;

            DestroyImmediate(ears.gameObject);
            _ears = null;

            gameObject.AddComponent(typeof(AudioListener));
        }

        if (origin != null)
        {
            // If we created the origin originally, destroy it now.
            if (origin.name.EndsWith(originSuffix))
            {
                // Reparent any children so we don't accidentally delete them.
                var _origin = origin;
                while (_origin.childCount > 0)
                    _origin.GetChild(0).parent = _origin.parent;

                DestroyImmediate(_origin.gameObject);
            }
            else
            {
                transform.parent = origin;
            }
        }

        DestroyImmediate(head.gameObject);
        _head = null;

        if (name.EndsWith(eyeSuffix))
            name = name.Substring(0, name.Length - eyeSuffix.Length);
    }

    #endregion

#if (UNITY_5_3 || UNITY_5_2 || UNITY_5_1 || UNITY_5_0)

    #region Render callbacks

    void OnPreRender()
    {
        if (flip)
            flip.enabled = (SteamVR_Render.Top() == this && SteamVR.instance.graphicsAPI == EGraphicsAPIConvention.API_DirectX);

        var headCam = head.GetComponent<Camera>();
        if (headCam != null)
            headCam.enabled = (SteamVR_Render.Top() == this);

        if (wireframe)
            GL.wireframe = true;
    }

    void OnPostRender()
    {
        if (wireframe)
            GL.wireframe = false;
    }

    void OnRenderImage(RenderTexture src, RenderTexture dest)
    {
        if (SteamVR_Render.Top() == this)
        {
            int eventID;
            if (SteamVR_Render.eye == EVREye.Eye_Left)
            {
                // Get gpu started on work early to avoid bubbles at the top of the frame.
                SteamVR_Utils.QueueEventOnRenderThread(SteamVR.Unity.k_nRenderEventID_Flush);

                eventID = SteamVR.Unity.k_nRenderEventID_SubmitL;
            }
            else
            {
                eventID = SteamVR.Unity.k_nRenderEventID_SubmitR;
            }

            // Queue up a call on the render thread to Submit our render target to the compositor.
            SteamVR_Utils.QueueEventOnRenderThread(eventID);
        }

        Graphics.SetRenderTarget(dest);
        SteamVR_Camera.blitMaterial.mainTexture = src;

        GL.PushMatrix();
        GL.LoadOrtho();
        SteamVR_Camera.blitMaterial.SetPass(0);
        GL.Begin(GL.QUADS);
        GL.TexCoord2(0.0f, 0.0f); GL.Vertex3(-1,  1, 0);
        GL.TexCoord2(1.0f, 0.0f); GL.Vertex3( 1,  1, 0);
        GL.TexCoord2(1.0f, 1.0f); GL.Vertex3( 1, -1, 0);
        GL.TexCoord2(0.0f, 1.0f); GL.Vertex3(-1, -1, 0);
        GL.End();
        GL.PopMatrix();

        Graphics.SetRenderTarget(null);
    }

    #endregion

#endif
}

How do I fix this? Thank you.
","
The type is wrong. Needs to be Vector3.
private Vector3 HMDLocalPos;

void Start()
{
    HDMLocalPos = SteamVR_Camera.head.localPosition;
}

Edit:
Replace the HDMLocalPos property with a reference to SteamVR_Camera and access it's properties like this:
public SteamVR_Camera steamCam;    // popuplate this via inspector or with a Find() (or similar)

void Start()
{
    // access like this
    steamCam.head.localPosition = something
}

I think this is what you want (to actually change the head.localPosition of the camera).
"
"
I'm currently trying to set up everything to develop a Gear VR app with Eclipse in Java but I keep getting errors when I clean the Gear VR Framework project:
http://pastebin.com/yK8810bG
""Das System kann die angegebene Datei nicht finden."" simply means:
""The system cannot find the file specified.""
At first Eclipse didn't recognize the directory ""C:\Programme\Programmieren\AndroidNDK"" (originally ""Program Files"" instead of ""Programme"" but it didn't work because of the space; has the subfolders ""build"", ""platforms"",...) as a valid folder for the NDK. According to another thread on Stackexchange, you have to create an empty file without a file extension and with the name ""ndk-build"" in the main folder (so ""AndroidNDK""), which I did - even though there's already a file like that in the subfolder ""build"". That at least made Eclipse happy but now I get the above error messages.
Yes, the folders ""GVRf"" and ""ovr_sdk_mobile"" are in the same root folder and yes, I added ""C:\Programme\Programmieren\AndroidNDK"" to the Windows Path variable but I still get the same error messages.
I checked, the files really don't exist but I don't know, where I would even get them. I followed the steps here, including 1.b., so I should have everything necessary but apparently I don't. Plus, I double checked with this youtube video. I did everything the guy did - except the Git part for GVRf (I downloaded the zip) but including not copying the project into my workspace (that sadly gave me a bunch of other ""cannot find..."" errors) - and it worked just fine for him but not for me.
Since this doesn't work, I also can't even test the samples. :/
Any ideas where I can get the missing files or what actually could be the problem and how I can fix it?
","
I fixed it:

There must not be any spaces in the path, so you shouldn't put the framework + NDK in a subfolder of e.g. ""Program Files"". Eclipse and Java usually don't care but the framework and NDK apparently do.
Eclipse's workspace/your project has to be on the same HDD as the framework. I've tried a couple of things but haven't gotten it to work with my usual workspace since that's on a different HDD.
There seemed to be a problem with the old version of the framework (April 11th to 13th), simply couldn't get it to run. It's now running fine with a newer version (April 15th) - as long as the two above points are met.

"
"
I'm currently trying to set up everything to develop a Gear VR app with Eclipse in Java but I keep getting errors when I clean the Gear VR Framework project:
http://pastebin.com/yK8810bG
""Das System kann die angegebene Datei nicht finden."" simply means:
""The system cannot find the file specified.""
At first Eclipse didn't recognize the directory ""C:\Programme\Programmieren\AndroidNDK"" (originally ""Program Files"" instead of ""Programme"" but it didn't work because of the space; has the subfolders ""build"", ""platforms"",...) as a valid folder for the NDK. According to another thread on Stackexchange, you have to create an empty file without a file extension and with the name ""ndk-build"" in the main folder (so ""AndroidNDK""), which I did - even though there's already a file like that in the subfolder ""build"". That at least made Eclipse happy but now I get the above error messages.
Yes, the folders ""GVRf"" and ""ovr_sdk_mobile"" are in the same root folder and yes, I added ""C:\Programme\Programmieren\AndroidNDK"" to the Windows Path variable but I still get the same error messages.
I checked, the files really don't exist but I don't know, where I would even get them. I followed the steps here, including 1.b., so I should have everything necessary but apparently I don't. Plus, I double checked with this youtube video. I did everything the guy did - except the Git part for GVRf (I downloaded the zip) but including not copying the project into my workspace (that sadly gave me a bunch of other ""cannot find..."" errors) - and it worked just fine for him but not for me.
Since this doesn't work, I also can't even test the samples. :/
Any ideas where I can get the missing files or what actually could be the problem and how I can fix it?
","
I fixed it:

There must not be any spaces in the path, so you shouldn't put the framework + NDK in a subfolder of e.g. ""Program Files"". Eclipse and Java usually don't care but the framework and NDK apparently do.
Eclipse's workspace/your project has to be on the same HDD as the framework. I've tried a couple of things but haven't gotten it to work with my usual workspace since that's on a different HDD.
There seemed to be a problem with the old version of the framework (April 11th to 13th), simply couldn't get it to run. It's now running fine with a newer version (April 15th) - as long as the two above points are met.

"
"
As far as i know, Samsung Gear VR is the only VR headset that has its own head-tracking sensors and sends sensor data to its mounted smartphone(galaxy series).
Am i getting it right?
From technical specifications of ZEISS VR One, it says
Tracking sensors : Internal tracking by smartphone sensors

Does this mean it has no in-built sensors? 
Just like Google Cardboard, all the other low-cost VR headsets are dependent FULLY on smartphone sensors, right?
","
Cardboard-like headsets are just a way for your eyes to look at your phone in stereo mode, they don't provide any sensors (and very often not even an interaction button like the original google cardboard), these devices typically use your phone's gyroscope to track your orientation.
"
"
I'm using a plugin that implements 360 / VR video into our video player. It does this by using Three.js to create a sphere and taking the video itself and making it the material the sphere is created out of. The viewport is then set inside of the sphere to give it the 360 view.
The problem I'm running into is that the material is placed on the sphere using THREE.DoubleSide (THREE.BackSide would also work since we're only viewing it from the inside of the sphere), but the image is inverted since we are viewing it from the inside.
Is there a way to invert the image material that is placed on the sphere?
","
One way to create a spherical panorama, that is not inverted, is to use this pattern:
var geometry = new THREE.SphereBufferGeometry( 100, 32, 16 );
var material = new THREE.MeshBasicMaterial( { map: texture } );

var mesh = new THREE.Mesh( geometry, material );
mesh.scale.set( - 1, 1, 1 );
scene.add( mesh );

It is generally not advisable to set negative scale values in three.js, but in this case, since you are using MeshBasicMaterial which does not utilize normals, it is OK to do so.
three.js r.75
"
"
I try, but I cannot hear any differencies from them, with or without ONSP Audio Source.
Why?
Do I need to export to gear vr, and wear the divice to get the differencies?
","
I got the answer.
There is differences. What you need to do is enable a plugin.
OculusSpatializer in Audio Manager
"
"
I'm creating simple google cardboard game in unity and i want to add score count. I know how to do it normally, but in VR it doesn't work for me. I tried to just put Text and display score in it, and it looked good(not exactly because it was displayed only for one eye) but after I tested it on my phone text was in completely different place. Do you know what's the proper way to do it?
","
Create a World Space canvas and have that canvas attached to the camera. Thats how I get my UI in VR games.
You're going to need a custom shader for your World Space UI objects that has it's Render Order to Overlay and has ZTest turned off.
This is a copy of the Default UI shader with the necessary changes. Should do the trick. Just make a material with this shader, and apply it to everything you want drawn over the top of geometry in your WorldSpace UI.
 Shader ""UI/Default_OverlayNoZTest""
 {
     Properties
     {
         [PerRendererData] _MainTex (""Sprite Texture"", 2D) = ""white"" {}
         _Color (""Tint"", Color) = (1,1,1,1)

         _StencilComp (""Stencil Comparison"", Float) = 8
         _Stencil (""Stencil ID"", Float) = 0
         _StencilOp (""Stencil Operation"", Float) = 0
         _StencilWriteMask (""Stencil Write Mask"", Float) = 255
         _StencilReadMask (""Stencil Read Mask"", Float) = 255

         _ColorMask (""Color Mask"", Float) = 15
     }

     SubShader
     {
         Tags
         { 
             ""Queue""=""Overlay"" 
             ""IgnoreProjector""=""True"" 
             ""RenderType""=""Transparent"" 
             ""PreviewType""=""Plane""
             ""CanUseSpriteAtlas""=""True""
         }

         Stencil
         {
             Ref [_Stencil]
             Comp [_StencilComp]
             Pass [_StencilOp] 
             ReadMask [_StencilReadMask]
             WriteMask [_StencilWriteMask]
         }

         Cull Off
         Lighting Off
         ZWrite Off
         ZTest Off
         Blend SrcAlpha OneMinusSrcAlpha
         ColorMask [_ColorMask]

         Pass
         {
         CGPROGRAM
             #pragma vertex vert
             #pragma fragment frag
             #include ""UnityCG.cginc""

             struct appdata_t
             {
                 float4 vertex   : POSITION;
                 float4 color    : COLOR;
                 float2 texcoord : TEXCOORD0;
             };

             struct v2f
             {
                 float4 vertex   : SV_POSITION;
                 fixed4 color    : COLOR;
                 half2 texcoord  : TEXCOORD0;
             };

             fixed4 _Color;

             v2f vert(appdata_t IN)
             {
                 v2f OUT;
                 OUT.vertex = mul(UNITY_MATRIX_MVP, IN.vertex);
                 OUT.texcoord = IN.texcoord;
 #ifdef UNITY_HALF_TEXEL_OFFSET
                 OUT.vertex.xy += (_ScreenParams.zw-1.0)*float2(-1,1);
 #endif
                 OUT.color = IN.color * _Color;
                 return OUT;
             }

             sampler2D _MainTex;

             fixed4 frag(v2f IN) : SV_Target
             {
                 half4 color = tex2D(_MainTex, IN.texcoord) * IN.color;
                 clip (color.a - 0.01);
                 return color;
             }
         ENDCG
         }
     }
 }

credit goes to http://answers.unity3d.com/questions/878667/world-space-canvas-on-top-of-everything.html
"
"
I'm creating a Google Cardboard VR app in Unity. I want the camera to constantly move forwards horizontally in the direction the camera is facing but not change its Y position, i.e. not rise or fall. 
I've managed to get it so that the camera moves forwards in whichever direction it's looking but can go up and down if you look that way, using this line of code:
transform.position = transform.position + cam.transform.forward * WalkingSpeed * Time.deltaTime;

Is there a simple way of fixing the Y axis, so that I get the behaviour I'm looking for? Here's the full code so far:
using UnityEngine;
using System.Collections;

public class CameraMovement : MonoBehaviour {

    public float WalkingSpeed = 1.0f;
    public bool WalkEnabled = true;

    GameObject cam;

    void Start () {
        // This is the part of the Cardboard camera where I'm getting
        // the forward position from:
        cam = GameObject.FindWithTag(""CCHead"");
    }

    void Update () 
    {
        if (WalkEnabled) 
        {
            walk();
        }
    }

    void walk()
    {
        transform.localPosition = transform.localPosition + cam.transform.forward * WalkingSpeed * Time.deltaTime;
    }
}

Note: ticking the 'freeze position' y value in rigidbody constraints doesn't work.
Many thanks in advance.
","
You can try and split the forward vector up and build a new one where the y axis does not change:
 transform.position =  
 transform.position + 
 new Vector3(
      cam.transform.forward.x * WalkingSpeed * Time.deltaTime,
      0, // add 0 to keep the y coordinate the same
      cam.transform.forward.z * WalkingSpeed * Time.deltaTime);

"
"







Closed. This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet Stack Overflow guidelines. It is not currently accepting answers.
                                
                            











 We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations.


Closed 8 years ago.







                        Improve this question
                    



I'm trying to get familiar with development with Oculus Rift. I have hand-on experiences in Java-based languages (Java and Android), and would rather code in Java than C/C++ or C#. I was wondering if there is any stable Java API or wrapper to use for Virtual Reality development using Oculus Rift?
","
JOVR is a JNA based wrapper for the Oculus SDK, currently supporting the 0.7 SDK (which is compatible with the current 0.8 runtime) and is available on Maven as
<dependency>
  <groupId>org.saintandreas</groupId>
  <artifactId>jovr</artifactId>
  <version>0.7.0.0</version>
</dependency>

Additionally, there is the JRift project, which is more up to date and uses JNI I believe.  
"
"
Can somebody tell me how to do head based steering? I know how to do it while mooving only camera, but i want to moove object by head and camera to follow the object.
I need this because i want to implement game logic to the object(it will aleso work for me if i can treat camera as an normal object).
I found this code for steering only camera:
public float speed = /* some number */;

private CardboardHead head;

void Start()
{
head = // find the CardboardHead
// example:
// head = Camera.main.GetComponent<StereoController>().Head;
// or, make the variable public and use drag-and-drop in the Editor
}

void Update()
{
transform.position += speed * head.Gaze.direction;
}

I'm starting to make games for unity so if I made some mistakes please correct me. 
","
Are you using cardboard plugin for Unity from here?
https://developers.google.com/cardboard/unity/download
if you do, then there is a prefab called cardboard main, put it in the scene, then if you see there is a game object called Head inside it and it contains CardboardHead script. So here is in your script:
public GameObject head; // drag the Head gameobject to here or you can just get it from the start

void Start()
{
    if (!head) // if you didn't drag the gameobject
    {
        head = FindObjectOfType<CardboardHead>().gameObject;
    }
}

void Update()
{
    transform.position += speed * head.transform.forward; // it is better to multiply by Time.deltaTime
}

also btw why it is tagged as Java?

New questions:

So here is what I did. I split the 'visual' and 'mechanic'. I removed the Mesh Renderer from the player, freeze its rotation for the rigidbody. I made a new child for the 'visual' it has a Script called Rotate.cs and removed its collider.
Changes to script:
in Forward.cs use the rb.AddForce(transform.forward * speed); back, and in Rotate.cs just use 
void Update () {
        transform.Rotate(Vector3.right * 5);
}

That is the quickest solution I can think of, I'm sure there are other solutions which you can try
"
"
According to project tango tutorial

Once you're set up, try making a simple VR application on Project
  Tango using our Unity prefab motion tracking tutorial. Also check out
  our Unity code samples on GitHub for more inspiration!

https://developers.google.com/project-tango/apis/unity/
but the prefab motion tracking tutorial don't have any VR instruction
https://developers.google.com/project-tango/apis/unity/unity-prefab-motion-tracking
Anyone know how to make the delta camera camera to VR mode?
","
Found it under tango VR examples 
https://github.com/googlesamples/tango-examples-unity
"
"
I want to make webgl application (using ThreeJS) for Oculus Rift DK2. DK2 need to get 75fps rendering output for the best view. But I have problem with it: one PC output 75fps, another - 60fps only.
My PC configs:

PC1 (outputs 75fps)


Intel Core i5
10Gb DDR3
GeForce GTX 650
Windows 10
Firefox Nighty with WebVR addon

PC2 (outputs 60fps)


Intel Core i7
16Gb DDR3
GeForce GTX 650
Windows 7
Firefox Nighty with WebVR addon


What affects to config PC2 that doesn't output 75fps?
","
Here are some suggestions:

Oculus runtime 0.8 (direct mode only)
Geforce drivers v.358.70-beta (as per the 0.8 runtime instructions)
Firefox Nightly w/ following flags (set via about:config)


dom.vr.enabled: true
gfx.vr.mirror-texture: true
layout.frame_rate: 75 (to match DK2 refresh)


https://mail.mozilla.org/pipermail/web-vr-discuss/2015-November/000929.html
"
"
If I play my game with the VR mode disabled, I can push on volume up and down of my smartphone. However, if the VR is enabled, the volume buttons of the smartphone don't work.
I have search the following words in my complete solution: VolumeLevel, AudioListener, volume, and KeyEvent.KEYCODE_VOLUME_DOWN. No result at all, so nowhere in any script seems to be changing or blocking those controls.
Why aren't the volume buttons working?
","
The Cardboard SDK disables the volume keys (on Android) when VR mode is active.  This is because when the phone in a Cardboard viewer, it is easy for the user to accidentally toggle the volume keys since they are laying flush against the bottom of the viewer.  A little extra pressure there and the volume key events start coming in.  Since the volume bar that Android displays is not in stereo, it causes major eye strain when it pops up in your Cardboard view.
BTW, if you tap the Settings gear at the bottom, so the app pops up the dialog asking if you want to configure your viewer, the volume keys are active at that point.  You can adjust the volume, then hit Cancel to go back to your app.
"
"
I am developing a virtual reality windows based game. I will be using an accelerometer sensor to get the hand movements of the player and use it in the game, where as the player will use it to fight an enemy in the game. I managed to get the accelerometer readings from the sensor.
I just need an idea on how I can now integrate it with my game in Unity. I used .Net to get the readings from the sensor. This is a TI simple link sensor tag (CC2650STK). This will connect via Bluetooth to my Windows phone.
using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.Linq;
using System.Text;
using System.Threading.Tasks;
using Windows.Devices.Bluetooth.GenericAttributeProfile;
using Windows.Devices.Enumeration;
using Windows.Storage.Streams;
using Buffer = Windows.Storage.Streams.Buffer;

namespace SensorTag
{
    /// This class provides access to the SensorTag Accelerometer BLE data

    public class BleAccelerometerService : BleGenericGattService
    {

        public BleAccelerometerService() 
        {
        }


        /// The version of the SensorTag device.  1=CC2541, 2=CC2650.

        public int Version { get; set; }

        static Guid AccelerometerServiceUuid = Guid.Parse(""f000aa10-0451-4000-b000-000000000000"");
        static Guid AccelerometerCharacteristicUuid = Guid.Parse(""f000aa11-0451-4000-b000-000000000000"");
        static Guid AccelerometerCharacteristicConfigUuid = Guid.Parse(""f000aa12-0451-4000-b000-000000000000"");
        static Guid AccelerometerCharacteristicPeriodUuid = Guid.Parse(""f000aa13-0451-4000-b000-000000000000"");

        Delegate _accelerometerValueChanged;

        public event EventHandler<AccelerometerMeasurementEventArgs> AccelerometerMeasurementValueChanged
        {
            add
            {
                if (_accelerometerValueChanged != null)
                {
                    _accelerometerValueChanged = Delegate.Combine(_accelerometerValueChanged, value);
                }
                else
                {
                    _accelerometerValueChanged = value;
                    RegisterForValueChangeEvents(AccelerometerCharacteristicUuid);
                }
            }
            remove
            {
                if (_accelerometerValueChanged != null)
                {
                    _accelerometerValueChanged = Delegate.Remove(_accelerometerValueChanged, value);
                    if (_accelerometerValueChanged == null)
                    {
                        UnregisterForValueChangeEvents(AccelerometerCharacteristicUuid);
                    }
                }
            }
        }

        private async Task<int> GetConfig()
        {
            var ch = GetCharacteristic(AccelerometerCharacteristicConfigUuid);
            if (ch != null)
            {
                var properties = ch.CharacteristicProperties;

                if ((properties & GattCharacteristicProperties.Read) != 0)
                {
                    var result = await ch.ReadValueAsync();
                    IBuffer buffer = result.Value;
                    DataReader reader = DataReader.FromBuffer(buffer);
                    var value = reader.ReadByte();
                    Debug.WriteLine(""Acceleration config = "" + value);
                    return (int)value;
                }
            }
            return -1;
        }

        bool isReading;

        public async Task StartReading()
        {
            if (!isReading)
            {
                await WriteCharacteristicByte(AccelerometerCharacteristicConfigUuid, 1);
                isReading = true;
            }
        }

        public async Task StopReading()
        {
            if (isReading)
            {
                isReading = false;
                await WriteCharacteristicByte(AccelerometerCharacteristicConfigUuid, 0);
            }
        }


        /// Get the rate at which accelerometer is being polled, in milliseconds.  
        /// </summary>
        /// <returns>Returns the value read from the sensor or -1 if something goes wrong.</returns>
        public async Task<int> GetPeriod()
        {
            byte v = await ReadCharacteristicByte(AccelerometerCharacteristicPeriodUuid, Windows.Devices.Bluetooth.BluetoothCacheMode.Uncached);
            return (int)(v * 10);
        }

        /// <summary>
        /// Set the rate at which accelerometer is being polled, in milliseconds.  
        /// </summary>
        /// <param name=""milliseconds"">The delay between updates, accurate only to 10ms intervals. Maximum value is 2550.</param>
        public async Task SetPeriod(int milliseconds)
        {
            int delay = milliseconds / 10;
            byte p = (byte)delay;
            if (p < 1)
            {
                p = 1;
            }

            await WriteCharacteristicByte(AccelerometerCharacteristicPeriodUuid, p);
        }

        private void OnAccelerationMeasurementValueChanged(AccelerometerMeasurementEventArgs args)
        {
            if (_accelerometerValueChanged != null)
            {
                ((EventHandler<AccelerometerMeasurementEventArgs>)_accelerometerValueChanged)(this, args);
            }
        }


        public async Task<bool> ConnectAsync(string deviceContainerId)
        {
            return await this.ConnectAsync(AccelerometerServiceUuid, deviceContainerId);
        }

        protected override void OnCharacteristicValueChanged(GattCharacteristic sender, GattValueChangedEventArgs eventArgs)
        {
            if (sender.Uuid == AccelerometerCharacteristicUuid)
            {
                if (_accelerometerValueChanged != null)
                {
                    uint dataLength = eventArgs.CharacteristicValue.Length;
                    using (DataReader reader = DataReader.FromBuffer(eventArgs.CharacteristicValue))
                    {
                        if (dataLength == 3)
                        {
                            var data = new byte[dataLength];
                            reader.ReadBytes(data);

                            AccelerometerMeasurement measurement = new AccelerometerMeasurement();

                            sbyte x = (sbyte)data[0];
                            sbyte y = (sbyte)data[1];
                            sbyte z = (sbyte)data[2];

                            measurement.X = (double)x / 64.0;
                            measurement.Y = (double)y / 64.0;
                            measurement.Z = (double)z / 64.0;

                            OnAccelerationMeasurementValueChanged(new AccelerometerMeasurementEventArgs(measurement, eventArgs.Timestamp));
                        }
                    }
                }
            }
        }

    }


    public class AccelerometerMeasurement
    {
        /// <summary>
        /// Get/Set X accelerometer in units of 1 g (9.81 m/s^2).
        /// </summary>
        public double X { get; set;}   

        /// <summary>
        /// Get/Set Y accelerometer in units of 1 g (9.81 m/s^2).
        /// </summary>
        public double Y { get; set;}        

        /// <summary>
        /// Get/Set Z accelerometer in units of 1 g (9.81 m/s^2).
        /// </summary>
        public double Z { get; set;}

        public AccelerometerMeasurement()
        {
        }

    }

    public class AccelerometerMeasurementEventArgs : EventArgs
    {
        public AccelerometerMeasurementEventArgs(AccelerometerMeasurement measurement, DateTimeOffset timestamp)
        {
            Measurement = measurement;
            Timestamp = timestamp;
        }

        public AccelerometerMeasurement Measurement
        {
            get;
            private set;
        }

        public DateTimeOffset Timestamp
        {
            get;
            private set;
        }
    }

}

","
One requirement would be to convert the acceleration values into quaternion:
private void ReadAcceleration(AccelerometerMeasurement measure){
    Vector3 accel = new Vector3((float)measure.X, 
            (float)measure.Y, (float)measure.Z);
    Quaternion rotation = Quaternion.LookRotation(accel,Vector3.forward);
}

The rotation quaternion can be used to set an object to the device rotation.
"
"
I'm trying to use https://github.com/povdocs/webvr-starter-kit to create a chrome PageAction extension.
On my manifest.json file, I've used content_scripts to load jquery.js and another inject.js.
On my inject.js, I've the following codes:
function initVR() {
    VR.floor();
    VR.box({ color: '#FFFFFF' }).moveTo(0, 1.4, 0).setScale(5,4,0);
    var text = VR.text({
        wrap: 4.1,
        font: '24pt Roboto',
        textAlign: 'left',
        fillStyle : '#000000',
        text : 'Hello world test'
    })
    .moveTo(.1, 1.4, 0);
}

on my background.js file, I've
chrome.pageAction.onClicked.addListener(function(tab) {
  chrome.tabs.executeScript(tab.ib, {
    file:  ""src/inject/vr.dev.js""
  }, function(){
    initVR();    //Calling the function I declared on inject.js
});

The idea here is, when a person clicks on the pageAction button, it injects the vr.dev.js and initialize the setup.
When I implement this, I get an error saying initVR() is not defined. What am I doing incorrect?

","
You have to use message passing to communicate to content script,because the extension of your browser is a single background script,it have to identify the tab 
https://developer.chrome.com/extensions/messaging
"
"
I am creating an app for google-cardboard, App like 360 degree image.
I created a sphere and I made 62 material with different 360 degree image.
I created 2 button, next and previous, when i focus on next button its changing to next material as i want. but when i focus on previous button, its changing to last material which i dont want. What I want is just previous material.
Here is my code. See and tell me what I am doing wrong. Thanks.
using UnityEngine;
using System.Collections;

public class next : MonoBehaviour {

    public CardboardHead head;
    public Material[] myMaterials = new Material[60];
    int maxMaterials;
    int arrayPos = 0;
    public GameObject Sphere;
    public int timeToCompleteLoading = 3;
    bool wait = false;

    void Start ()
    {
        head = Camera.main.GetComponent<StereoController> ().Head;
        maxMaterials = myMaterials.Length-1;

    }

    public void ToggleVRMode() 
    {
        Cardboard.SDK.VRModeEnabled = !Cardboard.SDK.VRModeEnabled;
    }

    IEnumerator RadialProgress(float time)
    {
        wait = true;
        yield return new WaitForSeconds(time);

        float rate = 1 / time;
        float i = 0;
        while (i < 1)
        {
            i += Time.deltaTime * rate;
            gameObject.renderer.material.SetFloat(""_Progress"", i);

        }
        wait = false;
    }

    void updateMaterials(){
        if (collider.gameObject.tag == ""next"") {
            if (arrayPos == maxMaterials){
                arrayPos = 0;
            }else{

                arrayPos++;
                Sphere.renderer.material = myMaterials [arrayPos];
            }
        }
         if (collider.gameObject.tag == ""previous"") {
            if (arrayPos == 0){
                arrayPos = maxMaterials;
            }else{
                arrayPos--;
                Sphere.renderer.material = myMaterials [arrayPos];

            }
        }


    }


    void Update ()
    {
        RaycastHit hit;
        Collider collider = GetComponent<Collider> ();

        if (collider) {

            bool isLookAt = collider.Raycast (head.Gaze, out hit, Mathf.Infinity);
            if (isLookAt == true) {
                if(wait == false){
                    StartCoroutine (RadialProgress (timeToCompleteLoading));

                    updateMaterials();
                }
            }
        }
    }
}

","
There is something wrong in what you describe. There is 60 materials, so maxMaterial should be 59 (0 to 59 is 60 spots). But you say it shows 61 and 60 and that should be out of bounds. Also, since maxMaterial is 59 (60 -1), when your index is 60 or 61, your if statement does not do.
You could review like so:
 void updateMaterials(){
    if (collider.gameObject.tag == ""next"") {
        if (++arrayPos == myMaterials.Length){
            arrayPos = 0;
        }
    }
    else if (collider.gameObject.tag == ""previous"") {
        if (--arrayPos < 0){
            arrayPos = myMaterials.Length - 1;
        }
    }
    Sphere.renderer.material = myMaterials [arrayPos];
}

P.S.: As I mentioned in a comment, your while loop in the coroutine will not do any delayed process:
    while (i < 1)
    {
        i += Time.deltaTime * rate;
        gameObject.renderer.material.SetFloat(""_Progress"", i);
    }

The while loop runs within a single frame.
    while (i < 1)
    {
        i += Time.deltaTime * rate;
        gameObject.renderer.material.SetFloat(""_Progress"", i);
        yield return null;
    }

Now the while loop runs over several frames.
"
"
In my animate() function I have the following code:
  if (this.controls.forward) {

    // move in direction we look at
    var cameraDirectionVector = new THREE.Vector3(0, 0, -1);
    cameraDirectionVector.applyQuaternion(this.camera.quaternion);

    var angle = cameraDirectionVector.angleTo(this.characterMesh.position);
    this.characterMesh.translateOnAxis(cameraDirectionVector.cross(this.characterMesh.position), moveDistance);

    this.characterMesh.translateZ(-moveDistance);
  }
  if (this.controls.backward) {
    this.characterMesh.translateZ(moveDistance);
  }

The camera is a child of the characterMesh. Moving foreward and backward works perfectly. But I want to move (on a plane) where I am actually looking (just yaw). I found a code example for Unity3D and try to adapt it for three.js which did not work. 
Any help or hints would be very much appreciated!
","
Hmm, I'm not sure where you were going with that calculation, but I think your intention is better implemented with a dot product instead of a cross product. I adapted this Unity3D code and came up with the following solution. See that link for another potentially more efficient method.
var YAXIS = new THREE.Vector3(0, 1, 0);
var ZAXIS = new THREE.Vector3(0, 0, 1);
var direction = ZAXIS.clone();
direction.applyQuaternion(camera.quaternion);
direction.sub(YAXIS.clone().multiplyScalar(direction.dot(YAXIS)));
direction.normalize();
character.quaternion.setFromUnitVectors(ZAXIS, direction);
character.translateZ(-moveDistance);

Full Code:


var renderer = new THREE.WebGLRenderer( { antialias: true } );
renderer.setSize(window.innerWidth, window.innerHeight);
document.body.appendChild(renderer.domElement);

var vrEffect = new THREE.VREffect(renderer, function () {});

var camera = new THREE.PerspectiveCamera(
  75, window.innerWidth / window.innerHeight, 0.1, 1000);
var vrControls = new THREE.VRControls(camera);

var scene = new THREE.Scene();

scene.add(new THREE.PointLight());

var makeCube = function (color) {
  return new THREE.Mesh(
    new THREE.BoxGeometry(1, 1, 1),
    new THREE.MeshLambertMaterial({
      color: color
    })
  );
};

var spacing = 1.5;
for (var i = 0; i < 10; i++) {
  for (var j = 0; j < 10; j++) {
    var color = (
      (i % 2 === 0 || j %2 === 0) ? 'green' : 'red');
    var cube = makeCube(color);
    cube.position.z = -i * spacing;
    cube.position.x = j * spacing;
    cube.position.y = -2;
    scene.add(cube);
  }
}

var character = new THREE.Object3D();
var characterBody = makeCube('blue');
characterBody.position.y = -1.5;
character.add(characterBody);
scene.add(character);

var moving = false;

window.addEventListener('keydown', function () {
  moving = true;
});

window.addEventListener('keyup', function () {
  moving = false;
});

var moveDistance = 0.1;
var YAXIS = new THREE.Vector3(0, 1, 0);
var ZAXIS = new THREE.Vector3(0, 0, 1);

var render = function() {
  requestAnimationFrame(render);
  
  vrControls.update();
  camera.position.copy(character.position);
  
  if (moving) {
    // move in direction we look at
    var direction = ZAXIS.clone();
    direction.applyQuaternion(camera.quaternion);
    direction.sub(YAXIS.clone().multiplyScalar(direction.dot(YAXIS)));
    direction.normalize();
    character.quaternion.setFromUnitVectors(ZAXIS, direction);
    character.translateZ(-moveDistance);
  }
  
  vrEffect.render(scene, camera);
};

render();


window.addEventListener('resize',  function onWindowResize() {
  camera.aspect = window.innerWidth / window.innerHeight;
  camera.updateProjectionMatrix();
  vrEffect.setSize( window.innerWidth, window.innerHeight );
});
<!DOCTYPE html>
<html>
<head>
  <script src=""https://rawgit.com/mrdoob/three.js/dev/build/three.js""></script>
  <script src=""https://rawgit.com/mrdoob/three.js/dev/examples/js/controls/VRControls.js""></script>
  <script src=""https://rawgit.com/mrdoob/three.js/dev/examples/js/effects/VREffect.js""></script>
  <meta charset=""utf-8"">
</head>
<body>
</body>
</html>



"
"
I am making an OpenGL c++ application that tracks the users location in relation to the screen and then updates the rendered scene to the perspective of the user. This is know as ""desktop VR"" or you can think of the screen as a diorama or fish tank. I am rather new to OpenGL and have only defined a very simple scene thus far, just a cube, and it is initially rendered correctly. 
The problem is that when I start moving and want to rerender the cube scene, the projection plane seems translated and I don't see what I think I should. I want this plane fixed. If I were writing a ray tracer, my window would always be fixed, but my eye is allowed to wander. Can someone please explain to me how I can achieve the effect I desire (pinning the viewing window) while having my camera/eye wander at a non-origin coordinate? 
All of the examples I find demand that the camera/eye be at the origin, but this is not conceptually convenient for me. Also, because this is a ""fish tank"", I am setting my d_near to be the xy-plane, where z = 0.
In screen/world space, I assign the center of the screen to (0,0,0) and its 4 corners to:
TL(-44.25, 25, 0)
TR( 44.25, 25, 0)
BR( 44.25,-25, 0)
BL(-44.25,-25, 0)
These values are in cm for a 16x9 display.
I then calculate the user's eye (actually a web cam on my face) using POSIT to be usually somewhere in the range of (+/-40, +/-40, 40-250). My POSIT method is accurate.
I am defining my own matrices for the perspective and viewing transforms and using shaders.
I initialize as follows:
float right = 44.25;
float left = -44.25;
float top = 25.00;
float bottom = -25.00; 

vec3 eye = vec3(0.0, 0.0, 100.0);
vec3 view_dir = vec3(0.0, 0.0, -1.0);
vec3 up = vec3(0.0, 1.0, 0.0);
vec3 n = normalize(-view_dir);
vec3 u = normalize(cross(up, n)); 
vec3 v = normalize(cross(n, u));

float d_x = -(dot(eye, u));
float d_y = -(dot(eye, v));
float d_z = -(dot(eye, n));

float d_near = eye.z;
float d_far = d_near + 50;

// perspective transform matrix
mat4 P = mat4((2.0*d_near)/(right-left ), 0, (right+left)/(right-left), 0, 
            0, (2.0*d_near)/(top-bottom), (top+bottom)/(top-bottom), 0,
            0, 0, -(d_far+d_near)/(d_far-d_near), -(2.0*d_far*d_near)/(d_far-d_near),
            0, 0, -1.0, 0);

// viewing transform matrix
mat4 V = mat4(u.x, u.y, u.z, d_x,
              v.x, v.y, v.z, d_y,
              n.x, n.y, n.z, d_z,
              0.0, 0.0, 0.0, 1.0);

mat4 MV = C * V;
//MV = V;

From what I gather looking on the web, my view_dir and up are to remain fixed. This means that I need only to update d_near and d_far as well as d_x, d_y, and d_y? I do this in my glutIdleFunc( idle );
void idle (void) {  

    hBuffer->getFrame(hFrame);
    if (hFrame->goodH && hFrame->timeStamp != timeStamp) {
        timeStamp = hFrame->timeStamp;
        std::cout << ""("" << hFrame->eye.x << "", "" <<
                    hFrame->eye.y << "", "" <<
                    hFrame->eye.z << "") \n"";

        eye = vec3(hFrame->eye.x, hFrame->eye.y, hFrame->eye.z);

        d_near = eye.z;
        d_far = eye.z + 50;

        P = mat4((2.0*d_near)/(right-left), 0, (right+left)/(right-left), 0, 
                 0, (2.0*d_near)/(top-bottom), (top+bottom)/(top-bottom), 0,
                 0, 0, -(d_far+d_near)/(d_far-d_near), -(2.0*d_far*d_near)/(d_far-d_near),
                 0, 0, -1.0, 0);

        d_x = -(dot(eye, u));
        d_y = -(dot(eye, v));
        d_z = -(dot(eye, n));

        C = mat4(1.0, 0.0, 0.0, eye.x,
                 0.0, 1.0, 0.0, eye.y,
                 0.0, 0.0, 1.0, 0.0,
                 0.0, 0.0, 0.0, 1.0);

        V = mat4(u.x, u.y, u.z, d_x,
                 v.x, v.y, v.z, d_y,
                 n.x, n.y, n.z, d_z,
                 0.0, 0.0, 0.0, 1.0);

        MV = C * V;
        //MV = V;

        glutPostRedisplay();
    }
}

Here is my shader code:
#version 150

uniform mat4 MV;
uniform mat4 P;
in vec4 vPosition;
in vec4 vColor;
out vec4 color;

void 
main() 
{ 
    gl_Position = P * MV * vPosition;
    color = vColor;
}

Ok, I made some changes to my code, but without success. When I use V in place of MV in the vertex shader, everything looks as I want it to, the perspective is correct and the objects are the right size, however, the scene is translated by the displacement of the camera. 
When using C and V to obtain MV, my scene is rendered from the perspective of an observer straight on and the rendered scene fills the window as it should, but the perspective of the eye/camera is lost. 
Really, what I want is to translate the 2D pixels, the projection plane, by the appropriate x and y values of the eye/camera, so as to keep the center of the object (whose xy center is (0,0)) in the center of the rendered image. I am guided by the examples in the textbook ""Interactive Computer Graphics: A Top-Down Approach with Shader-Based OpenGL (6th Edition)"". Using the files paired with the book freely available on the web, I am continuing with the row major approach.
The following images are taken when not using the matrix C to create MV. When I do use C to create MV, all scenes look like the first image below. I desire no translation in z and so I leave that as 0. 
Because the projection plane and my camera plane are parallel, the conversion from one to the other coordinate system is simply a translation and inv(T) ~ -T. 
Here is my image for the eye at (0,0,50):

Here is my image for the eye at (56,-16,50):

","
The solution is to update d_x, d_y, and d_z, accounting for the new eye position, but to never change u, v, or n. In addition, one must update the matrix P with new values for left, right, top and bottom, as they relate to the new position of the camera/eye. 
I initialize with this:
float screen_right = 44.25;
float screen_left = -44.25;
float screen_top = 25.00;
float screen_bottom = -25.00; 
float screen_front = 0.0;
float screen_back = -150;

And now my idle function looks like this, note the calculations for top, bottom, right, and left:
void idle (void) {  

hBuffer->getFrame(&hFrame);
if (hFrame.goodH && hFrame.timeStamp != timeStamp) {
    timeStamp = hFrame.timeStamp;
    //std::cout << ""("" << hFrame.eye.x << "", "" <<
    //                  hFrame.eye.y << "", "" <<
    //                  hFrame.eye.z << "") \n"";

    eye = vec3(hFrame.eye.x, hFrame.eye.y, hFrame.eye.z);

    d_near = eye.z;
    d_far = eye.z + abs(screen_back) + 1;

    float top = screen_top - eye.y;
    float bottom = top - 2*screen_top;
    float right = screen_right - eye.x;
    float left = right - 2*screen_right;

    P = mat4((2.0 * d_near)/(right - left ), 0.0, (right + left)/(right - left), 0.0, 
             0.0, (2.0 * d_near)/(top - bottom), (top + bottom)/(top - bottom), 0.0,
             0.0, 0.0, -(d_far + d_near)/(d_far - d_near), -(2.0 * d_far * d_near)/(d_far - d_near),
             0.0, 0.0, -1.0, 0.0);

    d_x = -(dot(eye, u));
    d_y = -(dot(eye, v));
    d_z = -(dot(eye, n));

    V = mat4(u.x, u.y, u.z, d_x,
             v.x, v.y, v.z, d_y,
             n.x, n.y, n.z, d_z,
             0.0, 0.0, 0.0, 1.0);

    glutPostRedisplay();
}

}
This redefinition of the perspective matrix keeps the rendered image from translating. I still have camera capture and screen grabbing synchronization issues, but I am able to create the images like following updated in real-time for the position of the user:

"
"
I am trying to make a 360 Degree Image GoogleCardoard app in unity3d.
I created a Sphere and made 5 material and applied 360 degree image in material. I am swapping material by Raycast and its working but the problem is when crosshair touches the arrow (applied raycast on arrow and crosshair to swap material) the material is changing very fast. i applied a radial progress bar but its not working. What I want is crosshair loading animation everytime when material change
here is my c# code
using UnityEngine;
using System.Collections;

public class next : MonoBehaviour {

    public CardboardHead head;
    public Material[] myMaterials = new Material[5];
    int maxMaterials;
    int arrayPos = 0;
    public GameObject Sphere;
    public int timeToCompleteLoading = 3;

    void Start ()
    {
        head = Camera.main.GetComponent<StereoController> ().Head;
        maxMaterials = myMaterials.Length-1;

    }

    public void ToggleVRMode() 
    {
        Cardboard.SDK.VRModeEnabled = !Cardboard.SDK.VRModeEnabled;
    }

    IEnumerator RadialProgress(float time)
    {
        yield return new WaitForSeconds(time);
        float rate = 1 / time;
        float i = 0;
        while (i < 1)
        {
            i += Time.deltaTime * rate;
            gameObject.renderer.material.SetFloat(""_Progress"", i);

        }
    }

    void Update ()
    {
                RaycastHit hit;
                Collider collider = GetComponent<Collider> ();

                if (collider) {
                        bool isLookAt = collider.Raycast (head.Gaze, out hit, 6.0f);
                        if (isLookAt == true) {

                            if (collider.gameObject.tag == ""next"") {
                                StartCoroutine (RadialProgress (timeToCompleteLoading));
                                if (arrayPos == maxMaterials){

                                    arrayPos = 0;
                                }else{

                                    arrayPos++;
                                    Sphere.renderer.material = myMaterials [arrayPos];
                            }
                    }
            }
    }
}
}

Please Help. Thanks
","
If you StartCoroutine in Update it will not stop call again StartCoroutine in next frame. You can't delay Update function. But you can use boolean flag to off run part of code:
using UnityEngine;
using System.Collections;

public class next : MonoBehaviour {

    public CardboardHead head;
    public Material[] myMaterials = new Material[5];
    int maxMaterials;
    int arrayPos = 0;
    public GameObject Sphere;
    public int timeToCompleteLoading = 3;
    bool wait = false;

    void Start ()
    {
        head = Camera.main.GetComponent<StereoController> ().Head;
        maxMaterials = myMaterials.Length-1;

    }

    public void ToggleVRMode() 
    {
        Cardboard.SDK.VRModeEnabled = !Cardboard.SDK.VRModeEnabled;
    }

    IEnumerator RadialProgress(float time)
    {
        wait = true;
        yield return new WaitForSeconds(time);

        float rate = 1 / time;
        float i = 0;
        while (i < 1)
        {
            i += Time.deltaTime * rate;
            gameObject.renderer.material.SetFloat(""_Progress"", i);

        }
        wait = false;
    }

    void Update ()
    {
                RaycastHit hit;
                Collider collider = GetComponent<Collider> ();

                if (collider) {
                        bool isLookAt = collider.Raycast (head.Gaze, out hit, 6.0f);
                        if (isLookAt == true) {
                            if(wait == false){
                                if (collider.gameObject.tag == ""next"") {
                                    StartCoroutine (RadialProgress (timeToCompleteLoading));
                                    if (arrayPos == maxMaterials){

                                        arrayPos = 0;
                                    }else{

                                        arrayPos++;
                                        Sphere.renderer.material = myMaterials [arrayPos];
                                }
                        }
                    }
            }
    }
}
}

"
"
I'm using unity 5.2.2 for this project and CardboardMain prefab. I want to rotate and spin around an object such as the mask example from google cardboard app. As far as I know there are two ways of doing this: 
1. Render the CardboardMain (parent of the Head) 
2. Render the main object. 
I'm new to this technology. Would you please help me with C# or JavaScript code and instruction how to do this.
Thank you. 
","
Put the CardboardMain object at the same location as the object you are viewing.  Set the Z component of the Main Camera (child of CardboardHead) to a negative value.  You can do this in the editor.
"
"
i think the title is clear , i want to make the mouse Cursor which is set in GazeInpuModule to be always enabled and in center of the both Left And Right Cameras , can anyone help me ?! 

ive tried changing the GazeInputModule code to place the cursor in center , but got nothing !

","
Here's what worked for me:

Create a Cursor GameObject in the Head hierarchy

Add EventSystem in the project hierarchy
In the EventSystem Gaze Input Module component, ensure ""Show Cursor"" is selected.

In GazeInputModule.cs, PlaceCursor(), change the SetActive() to:
private void PlaceCursor() {
    // ...
    cursor.SetActive(showCursor); 
    // ...
}


"
"
currently working my way through this Cardboard VR tutorial for Unity - https://io2015codelabs.appspot.com/codelabs/cardboard-unity#6
The Main Menu is now currently working in VR, but when I try to load the GameScene the application crashes to Nexus desktop.
Does anybody know why this might be happening? This happens whether I use Application.LoadLevel or SceneManager.LoadScene.
--Rev
In case this might help somebody diagnose the problem, here's the what the adb logcat spat out around the time of the crash. I'm having trouble parsing it, but maybe somebody can spot the problem!
D/Unity   ( 6716): [EGL] Request: ES 3.0 RGBA32 8888 0/0
D/Unity   ( 6716): [EGL] Found: ID[5] ES 3.0 RGBA32 8888 0/0 AFBT
D/Unity   ( 6716): ANativeWindow: (1280/768) RequestedResolution: (0/0) EGLSurface: (1280/768)
D/Unity   ( 6716): Renderer: Adreno (TM) 320
D/Unity   ( 6716): Vendor:   Qualcomm
D/Unity   ( 6716): Version:  OpenGL ES 3.0 [email protected] AU@  (GIT@Id3510ff6dc)
D/Unity   ( 6716): GLES:     3
D/Unity   ( 6716):  GL_AMD_compressed_ATC_texture GL_AMD_performance_monitor GL_AMD_program_binary_Z400 GL_EXT_debug_label GL_EXT_debug_marker GL_EXT_discard_fr
amebuffer GL_EXT_robustness GL_EXT_texture_format_BGRA8888 GL_EXT_texture_type_2_10_10_10_REV GL_NV_fence GL_OES_compressed_ETC1_RGB8_texture GL_OES_depth_textu
re GL_OES_depth24 GL_OES_EGL_image GL_OES_EGL_sync GL_OES_EGL_image_external GL_OES_element_index_uint GL_OES_fbo_render_mipmap GL_OES_fragment_precision_high G
L_OES_get_program_binary GL_OES_packed_depth_stencil GL_OES_depth_texture_cube_map GL_OES_rgb8_rgba8 GL_OES_standard_derivatives GL_OES_texture_3D GL_OES_textur
e_float GL_OES_texture_half_float GL_OES_texture_half_float_linear GL_OES_texture_npot GL_OES_vertex_half_float GL_OES_vertex_type_10_10_10_2 GL_OES_vertex_arra
y_object GL_QCOM_alpha_test GL_QCOM_binning_control GL_QCOM_driver_control GL_QCOM_perfmon_global_mode GL_QCOM_extended_get GL_QCOM_extended_get2 GL_QCOM_tiled_
rendering GL_QCOM_writeonly_rendering GL_EXT_sRGB GL_EXT_sRGB_write_control GL_EXT
D/Unity   ( 6716): _texture_sRGB_decode GL_EXT_multisampled_render_to_texture GL_EXT_color_buffer_float GL_EXT_color_buffer_half_float GL_EXT_disjoint_timer_que
ry
D/Unity   ( 6716): OPENGL LOG: Creating OpenGL ES 3.0 graphics device
D/Unity   ( 6716): InitializeGfxDevice OK
D/Unity   ( 6716): Initialize engine version: 5.3.0f4 (2524e04062b4)
D/Unity   ( 6716): AudioManager: Selecting OpenSL output...
D/Unity   ( 6716): Begin MonoManager ReloadAssembly
D/Unity   ( 6716): Platform assembly: /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/UnityEngine.dll (this message is harmless
)
D/Unity   ( 6716): Loading /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/UnityEngine.dll into Unity Child Domain
D/Unity   ( 6716): Platform assembly: /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/Assembly-CSharp.dll (this message is harm
less)
D/Unity   ( 6716): Loading /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/Assembly-CSharp.dll into Unity Child Domain
D/Unity   ( 6716): Platform assembly: /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/UnityEngine.UI.dll (this message is harml
ess)
D/Unity   ( 6716): Loading /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/UnityEngine.UI.dll into Unity Child Domain
D/Unity   ( 6716): Platform assembly: /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/UnityEngine.Networking.dll (this message
is harmless)
D/Unity   ( 6716): Loading /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/UnityEngine.Networking.dll into Unity Child Domain
D/Unity   ( 6716): - Completed reload, in  0.211 seconds
D/Unity   ( 6716): Platform assembly: /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/System.Core.dll (this message is harmless
)
D/Unity   ( 6716): Platform assembly: /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/System.dll (this message is harmless)
D/Unity   ( 6716): PlayerInitEngineGraphics OK
D/Unity   ( 6716): Found 14 native sensors
D/Unity   ( 6716): Sensor :        Accelerometer ( 1) ; 0.001190 / 0.00s ; LGE Accelerometer Sensor / InvenSense
D/Unity   ( 6716): Sensor :        Accelerometer ( 1) ; 0.001190 / 0.00s ; LGE Accelerometer Sensor / InvenSense
D/Unity   ( 6716): Requested framebuffer: resolution[1280x768], rgba[8/8/8/8], depth+stencil[on], samples[1]
D/Unity   ( 6716): Created framebuffer: resolution[1280x768], rgba[8/8/8/8], depth+stencil[24/8], samples[0]
D/Unity   ( 6716): UnloadTime: 1.771000 ms
D/Unity   ( 6716): Sensor :        Accelerometer ( 1) ; 0.001190 / 0.00s ; LGE Accelerometer Sensor / InvenSense
I/Unity   ( 6716): Creating Cardboard object
I/Unity   ( 6716): UnityEngine.DebugLogHandler:Internal_Log(LogType, String, Object)
I/Unity   ( 6716): UnityEngine.DebugLogHandler:LogFormat(LogType, Object, String, Object[])
I/Unity   ( 6716): UnityEngine.Logger:Log(LogType, Object)
I/Unity   ( 6716): UnityEngine.Debug:Log(Object)
I/Unity   ( 6716): Cardboard:get_SDK() (at J:\Unity\Tutorials\VeryBasicCardBoardTest03\Assets\Cardboard\Scripts\Cardboard.cs:40)
I/Unity   ( 6716): CardboardEye:UpdateStereoValues() (at J:\Unity\Tutorials\VeryBasicCardBoardTest03\Assets\Cardboard\Scripts\CardboardEye.cs:141)
I/Unity   ( 6716): CardboardEye:Start() (at J:\Unity\Tutorials\VeryBasicCardBoardTest03\Assets\Cardboard\Scripts\CardboardEye.cs:96)
I/Unity   ( 6716):
I/Unity   ( 6716): (Filename: J Line: 0)
I/Unity   ( 6716):
I/Unity   ( 6716): Creating new default cardboard screen texture 1474x887.
I/Unity   ( 6716): UnityEngine.DebugLogHandler:Internal_Log(LogType, String, Object)
I/Unity   ( 6716): UnityEngine.DebugLogHandler:LogFormat(LogType, Object, String, Object[])
I/Unity   ( 6716): UnityEngine.Logger:Log(LogType, Object)
I/Unity   ( 6716): UnityEngine.Debug:Log(Object)
I/Unity   ( 6716): BaseVRDevice:CreateStereoScreen() (at J:\Unity\Tutorials\VeryBasicCardBoardTest03\Assets\Cardboard\Scripts\VRDevices\BaseVRDevice.cs:104)
I/Unity   ( 6716): Cardboard:get_StereoScreen() (at J:\Unity\Tutorials\VeryBasicCardBoardTest03\Assets\Cardboard\Scripts\Cardboard.cs:323)
I/Unity   ( 6716): CardboardEye:UpdateStereoValues() (at J:\Unity\Tutorials\VeryBasicCardBoardTest03\Assets\Cardboard\Scripts\CardboardEye.cs:173)
I/Unity   ( 6716): CardboardEye:Start() (at J:\Unity\Tutorials\VeryBasicCardBoardTest03\Assets\Cardboard\Scripts\CardboardEye.cs:96)
I/Unity   ( 6716):
I/Unity   ( 6716): (Filename: J Line: 0)
I/Unity   ( 6716):
D/Unity   ( 6716): Setting up 1 worker threads for Enlighten.
D/Unity   ( 6716):   Thread -> id: b7e6ef38 -> priority: 1
D/Unity   ( 6716): Unloading 7 Unused Serialized files (Serialized files now loaded: 0)
D/Unity   ( 6716): UnloadTime: 114.665001 ms
I/DEBUG   (  185): *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
I/DEBUG   (  185): Build fingerprint: 'google/occam/mako:5.1.1/LMY48T/2237560:user/release-keys'
I/DEBUG   (  185): Revision: '11'
I/DEBUG   (  185): ABI: 'arm'
I/DEBUG   (  185): pid: 6716, tid: 6746, name: UnityMain  >>> com.OgmaDigi.VeryBasicCardBoardTest <<<
I/DEBUG   (  185): signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x9dc2bff0
I/DEBUG   (  185):     r0 9dc2bff0  r1 9dc2c000  r2 fffffe8c  r3 9dc2c000
I/DEBUG   (  185):     r4 9dc2bff0  r5 00000001  r6 9dc2bffc  r7 af6a0000
I/DEBUG   (  185):     r8 9a5625d4  r9 00000000  sl 00000002  fp a33d871c
I/DEBUG   (  185):     ip afb3fed8  sp a33d86f0  lr afa60d7c  pc afa60cec  cpsr 80010010
I/DEBUG   (  185):
I/DEBUG   (  185): backtrace:
I/DEBUG   (  185):     #00 pc 002b2cec  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #01 pc 002b2d78  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so (GC_push_all_stack+72)
I/DEBUG   (  185):     #02 pc 002bb9fc  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #03 pc 002bbacc  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #04 pc 002b7840  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #05 pc 002b4834  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #06 pc 002b1128  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #07 pc 002ab1fc  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #08 pc 002aace0  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #09 pc 002abaa4  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #10 pc 002abb44  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #11 pc 001396d8  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so (mono_gc_collect+40)
I/DEBUG   (  185):     #12 pc 003aea64  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (CleanupAfterLoad()+92)
I/DEBUG   (  185):     #13 pc 00402a04  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (LoadSceneOperation::PlayerLoadSceneFromThread()+340
)
I/DEBUG   (  185):     #14 pc 004027b4  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (LoadSceneOperation::IntegrateMainThread()+184)
I/DEBUG   (  185):     #15 pc 003ffed4  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (PreloadManager::UpdatePreloadingSingleStep(PreloadM
anager::UpdatePreloadingFlags, int)+388)
I/DEBUG   (  185):     #16 pc 004002ec  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (PreloadManager::WaitForAllAsyncOperationsToComplete
()+72)
I/DEBUG   (  185):     #17 pc 004003e8  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (PreloadManager::UpdatePreloading()+164)
I/DEBUG   (  185):     #18 pc 003a7500  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (PlayerLoop(bool, bool, IHookEvent*)+664)
I/DEBUG   (  185):     #19 pc 00513b2c  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (UnityPlayerLoop()+700)
I/DEBUG   (  185):     #20 pc 005192c8  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (nativeRender(_JNIEnv*, _jobject*)+264)
I/DEBUG   (  185):     #21 pc 0000dbe7  /data/dalvik-cache/arm/data@[email protected]@[email protected]
I/DEBUG   (  185):
I/DEBUG   (  185): Tombstone written to: /data/tombstones/tombstone_03
I/ActivityManager(  548): Process com.OgmaDigi.VeryBasicCardBoardTest (pid 6716) has died
W/ActivityManager(  548): Force removing ActivityRecord{3c1c96 u0 com.OgmaDigi.VeryBasicCardBoardTest/com.google.unity.GoogleUnityActivity t83}: app died, no sa
ved state
I/ActivityManager(  548): Config changes=480 {1.0 272mcc5mnc en_GB ?layoutDir sw384dp w384dp h567dp 320dpi nrml port finger -keyb/v/h -nav/h s.11}
V/ActivityManager(  548): Display changed displayId=0
I/ActivityManager(  548): Start proc 7649:com.google.android.gm/u0a75 for service com.google.android.gm/.provider.MailSyncAdapterService
W/ActivityManager(  548): Unable to start service Intent { cmp=com.google.android.gm/com.android.email.service.AttachmentService } U=0: not found
I/ActivityManager(  548): Start proc 7682:com.google.android.gm.exchange/u0a74 for service com.google.android.gm.exchange/com.android.exchange.service.EasServic
e
I/ActivityManager(  548): Killing 5763:com.google.android.videos/u0a81 (adj 15): empty #17
I/ActivityManager(  548): Killing 4610:com.android.defcontainer/u0a5 (adj 15): empty #17
V/ActivityManager(  548): Display changed displayId=0
W/PackageManager(  674): Failure retrieving resources for com.OgmaDigi.VeryBasicCardBoardTest: Resource ID #0x0
V/ActivityManager(  548): Display changed displayId=0
V/ActivityManager(  548): Display changed displayId=0
V/ActivityManager(  548): Display changed displayId=0
I/ActivityManager(  548): START u0 {act=android.intent.action.DIAL flg=0x14000000 cmp=com.google.android.dialer/.extensions.GoogleDialtactsActivity} from uid 10
022 on display 0
I/ActivityManager(  548): Start proc 7883:com.google.android.dialer/u0a10 for activity com.google.android.dialer/.extensions.GoogleDialtactsActivity
V/ActivityManager(  548): Display changed displayId=0
I/ActivityManager(  548): Config changes=480 {1.0 272mcc5mnc en_GB ?layoutDir sw384dp w598dp h359dp 320dpi nrml land finger -keyb/v/h -nav/h s.12}
I/ActivityManager(  548): Displayed com.google.android.dialer/.extensions.GoogleDialtactsActivity: +1s28ms
I/ActivityManager(  548): START u0 {act=android.intent.action.MAIN cat=[android.intent.category.HOME] flg=0x10200000 cmp=com.android.launcher/com.android.launch
er2.Launcher} from uid 1000 on display 0
V/ActivityManager(  548): Display changed displayId=0
I/ActivityManager(  548): Config changes=480 {1.0 272mcc5mnc en_GB ?layoutDir sw384dp w384dp h567dp 320dpi nrml port finger -keyb/v/h -nav/h s.13}
I/ActivityManager(  548): START u0 {act=android.intent.action.MAIN cat=[android.intent.category.LAUNCHER] flg=0x10200000 cmp=com.OgmaDigi.VeryBasicCardBoardTest
/com.google.unity.GoogleUnityActivity bnds=[568,185][728,385] (has extras)} from uid 10014 on display 0
I/ActivityManager(  548): Start proc 8002:com.OgmaDigi.VeryBasicCardBoardTest/u0a122 for activity com.OgmaDigi.VeryBasicCardBoardTest/com.google.unity.GoogleUni
tyActivity
V/ActivityManager(  548): Display changed displayId=0
I/ActivityManager(  548): Config changes=480 {1.0 272mcc5mnc en_GB ?layoutDir sw384dp w598dp h359dp 320dpi nrml land finger -keyb/v/h -nav/h s.14}
I/Unity   ( 8002): splash_mode = 0 (integer)
I/Unity   ( 8002): useObb = False (bool)
D/Unity   ( 8002): surfaces: attach glview
I/Unity   ( 8002): Choreographer available: Enabling VSYNC timing
I/Unity   ( 8002): onResume
D/Unity   ( 8002): [EGL] Attaching window :0xb73d3c20
D/Unity   ( 8002): [EGL] Attaching window :0xb73d3c20
I/Unity   ( 8002): windowFocusChanged: true
D/Unity   ( 8002): SystemInfo CPU = ARMv7 VFPv3 NEON, Cores = 4, Memory = 1835mb
D/Unity   ( 8002): SystemInfo ARM big.LITTLE configuration: 4 big (mask: 15), 0 little (mask: 0)
I/ActivityManager(  548): Displayed com.OgmaDigi.VeryBasicCardBoardTest/com.google.unity.GoogleUnityActivity: +364ms
D/Unity   ( 8002): [SHA1] a0cfe2f5c6cecd88aae2aef0de87a209b8be7076   23066600 bytes ~ lib/armeabi-v7a/libunity.so
D/Unity   ( 8002): [SHA1] 06f01cc28b6f52e2af6894f366dfb8c9e63ee257      45104 bytes ~ lib/armeabi-v7a/libmain.so
D/Unity   ( 8002): [SHA1] 0f1d3f9c24a6219cf7348bf8f8709ee70d89e7ff    3758496 bytes ~ lib/armeabi-v7a/libmono.so
D/Unity   ( 8002): [SHA1] 31e4496152a50bf44f354d4a8ae7f038caa4c577   26329820 bytes ~ lib/x86/libunity.so
D/Unity   ( 8002): [SHA1] 0f2623fd8777a09de55336c57066c5a0c3c29e5d      26776 bytes ~ lib/x86/libmain.so
D/Unity   ( 8002): [SHA1] 151e9995f77f3d5778d93697de3c8bab1204419d    3636444 bytes ~ lib/x86/libmono.so
D/Unity   ( 8002): Mono path[0] = '/data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed'
D/Unity   ( 8002): Mono config path = 'assets/bin/Data/Managed'
D/Unity   ( 8002): PlayerConnection initialized from /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data (debug = 0)
D/Unity   ( 8002): PlayerConnection initialized network socket : 0.0.0.0 55438
D/Unity   ( 8002): PlayerConnection initialized unix socket : Unity-com.OgmaDigi.VeryBasicCardBoardTest
D/Unity   ( 8002): Multi-casting ""[IP] 192.168.1.84 [Port] 55438 [Flags] 3 [Guid] 3257369354 [EditorId] 2106478937 [Version] 1048832 [Id] AndroidPlayer(LGE_Nexu
[email protected]) [Debug] 1"" to [225.0.0.222:54997]...
D/Unity   ( 8002): Waiting for connection from host on [192.168.1.84:55438]...
D/Unity   ( 8002): Timed out. Continuing without host connection.
D/Unity   ( 8002): Using monoOptions --debugger-agent=transport=dt_socket,embedding=1,defer=y,address=0.0.0.0:56354
D/Unity   ( 8002): InitializeScriptEngine OK (af696ee0)
D/Unity   ( 8002): PlayerConnection already initialized - listening to [192.168.1.84:55438]
D/Unity   ( 8002): PlayerInitEngineNoGraphics OK
D/Unity   ( 8002): [EGL] Request: ES 3.1+AEP RGB0 000 0/0
D/Unity   ( 8002): [EGL] Checking ES 3.1 support...
D/Unity   ( 8002): [EGL] ES3.1 not supported
D/Unity   ( 8002): [EGL] Request: ES 3.1 RGB0 000 0/0
D/Unity   ( 8002): [EGL] Request: ES 3.0 RGB0 000 0/0
D/Unity   ( 8002): [EGL] Found: ID[1] ES 3.0 RGB16 565 0/0
D/Unity   ( 8002): [EGL] Request: ES 3.1+AEP RGB0 000 0/0
D/Unity   ( 8002): [EGL] Request: ES 3.1 RGB0 000 0/0
D/Unity   ( 8002): [EGL] Request: ES 3.0 RGB0 000 0/0
D/Unity   ( 8002): [EGL] Found: ID[1] ES 3.0 RGB16 565 0/0
D/Unity   ( 8002): [EGL] Request: ES 3.0 RGBA32 8888 0/0
D/Unity   ( 8002): [EGL] Found: ID[5] ES 3.0 RGBA32 8888 0/0 AFBT
D/Unity   ( 8002): ANativeWindow: (1280/768) RequestedResolution: (0/0) EGLSurface: (1280/768)
D/Unity   ( 8002): Renderer: Adreno (TM) 320
D/Unity   ( 8002): Vendor:   Qualcomm
D/Unity   ( 8002): Version:  OpenGL ES 3.0 [email protected] AU@  (GIT@Id3510ff6dc)
D/Unity   ( 8002): GLES:     3
D/Unity   ( 8002):  GL_AMD_compressed_ATC_texture GL_AMD_performance_monitor GL_AMD_program_binary_Z400 GL_EXT_debug_label GL_EXT_debug_marker GL_EXT_discard_fr
amebuffer GL_EXT_robustness GL_EXT_texture_format_BGRA8888 GL_EXT_texture_type_2_10_10_10_REV GL_NV_fence GL_OES_compressed_ETC1_RGB8_texture GL_OES_depth_textu
re GL_OES_depth24 GL_OES_EGL_image GL_OES_EGL_sync GL_OES_EGL_image_external GL_OES_element_index_uint GL_OES_fbo_render_mipmap GL_OES_fragment_precision_high G
L_OES_get_program_binary GL_OES_packed_depth_stencil GL_OES_depth_texture_cube_map GL_OES_rgb8_rgba8 GL_OES_standard_derivatives GL_OES_texture_3D GL_OES_textur
e_float GL_OES_texture_half_float GL_OES_texture_half_float_linear GL_OES_texture_npot GL_OES_vertex_half_float GL_OES_vertex_type_10_10_10_2 GL_OES_vertex_arra
y_object GL_QCOM_alpha_test GL_QCOM_binning_control GL_QCOM_driver_control GL_QCOM_perfmon_global_mode GL_QCOM_extended_get GL_QCOM_extended_get2 GL_QCOM_tiled_
rendering GL_QCOM_writeonly_rendering GL_EXT_sRGB GL_EXT_sRGB_write_control GL_EXT
D/Unity   ( 8002): _texture_sRGB_decode GL_EXT_multisampled_render_to_texture GL_EXT_color_buffer_float GL_EXT_color_buffer_half_float GL_EXT_disjoint_timer_que
ry
D/Unity   ( 8002): OPENGL LOG: Creating OpenGL ES 3.0 graphics device
D/Unity   ( 8002): InitializeGfxDevice OK
D/Unity   ( 8002): Initialize engine version: 5.3.0f4 (2524e04062b4)
D/Unity   ( 8002): AudioManager: Selecting OpenSL output...
D/Unity   ( 8002): Begin MonoManager ReloadAssembly
D/Unity   ( 8002): Platform assembly: /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/UnityEngine.dll (this message is harmless
)
D/Unity   ( 8002): Loading /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/UnityEngine.dll into Unity Child Domain
D/Unity   ( 8002): Platform assembly: /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/Assembly-CSharp.dll (this message is harm
less)
D/Unity   ( 8002): Loading /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/Assembly-CSharp.dll into Unity Child Domain
D/Unity   ( 8002): Platform assembly: /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/UnityEngine.UI.dll (this message is harml
ess)
D/Unity   ( 8002): Loading /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/UnityEngine.UI.dll into Unity Child Domain
D/Unity   ( 8002): Platform assembly: /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/UnityEngine.Networking.dll (this message
is harmless)
D/Unity   ( 8002): Loading /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/UnityEngine.Networking.dll into Unity Child Domain
D/Unity   ( 8002): - Completed reload, in  0.220 seconds
D/Unity   ( 8002): Platform assembly: /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/System.Core.dll (this message is harmless
)
D/Unity   ( 8002): Platform assembly: /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/base.apk/assets/bin/Data/Managed/System.dll (this message is harmless)
D/Unity   ( 8002): PlayerInitEngineGraphics OK
D/Unity   ( 8002): Found 14 native sensors
D/Unity   ( 8002): Sensor :        Accelerometer ( 1) ; 0.001190 / 0.00s ; LGE Accelerometer Sensor / InvenSense
D/Unity   ( 8002): Sensor :        Accelerometer ( 1) ; 0.001190 / 0.00s ; LGE Accelerometer Sensor / InvenSense
D/Unity   ( 8002): Requested framebuffer: resolution[1280x768], rgba[8/8/8/8], depth+stencil[on], samples[1]
D/Unity   ( 8002): Created framebuffer: resolution[1280x768], rgba[8/8/8/8], depth+stencil[24/8], samples[0]
D/Unity   ( 8002): UnloadTime: 1.495000 ms
D/Unity   ( 8002): Sensor :        Accelerometer ( 1) ; 0.001190 / 0.00s ; LGE Accelerometer Sensor / InvenSense
I/Unity   ( 8002): Creating Cardboard object
I/Unity   ( 8002): UnityEngine.DebugLogHandler:Internal_Log(LogType, String, Object)
I/Unity   ( 8002): UnityEngine.DebugLogHandler:LogFormat(LogType, Object, String, Object[])
I/Unity   ( 8002): UnityEngine.Logger:Log(LogType, Object)
I/Unity   ( 8002): UnityEngine.Debug:Log(Object)
I/Unity   ( 8002): Cardboard:get_SDK() (at J:\Unity\Tutorials\VeryBasicCardBoardTest03\Assets\Cardboard\Scripts\Cardboard.cs:40)
I/Unity   ( 8002): CardboardEye:UpdateStereoValues() (at J:\Unity\Tutorials\VeryBasicCardBoardTest03\Assets\Cardboard\Scripts\CardboardEye.cs:141)
I/Unity   ( 8002): CardboardEye:Start() (at J:\Unity\Tutorials\VeryBasicCardBoardTest03\Assets\Cardboard\Scripts\CardboardEye.cs:96)
I/Unity   ( 8002):
I/Unity   ( 8002): (Filename: J Line: 0)
I/Unity   ( 8002):
I/Unity   ( 8002): Creating new default cardboard screen texture 1474x887.
I/Unity   ( 8002): UnityEngine.DebugLogHandler:Internal_Log(LogType, String, Object)
I/Unity   ( 8002): UnityEngine.DebugLogHandler:LogFormat(LogType, Object, String, Object[])
I/Unity   ( 8002): UnityEngine.Logger:Log(LogType, Object)
I/Unity   ( 8002): UnityEngine.Debug:Log(Object)
I/Unity   ( 8002): BaseVRDevice:CreateStereoScreen() (at J:\Unity\Tutorials\VeryBasicCardBoardTest03\Assets\Cardboard\Scripts\VRDevices\BaseVRDevice.cs:104)
I/Unity   ( 8002): Cardboard:get_StereoScreen() (at J:\Unity\Tutorials\VeryBasicCardBoardTest03\Assets\Cardboard\Scripts\Cardboard.cs:323)
I/Unity   ( 8002): CardboardEye:UpdateStereoValues() (at J:\Unity\Tutorials\VeryBasicCardBoardTest03\Assets\Cardboard\Scripts\CardboardEye.cs:173)
I/Unity   ( 8002): CardboardEye:Start() (at J:\Unity\Tutorials\VeryBasicCardBoardTest03\Assets\Cardboard\Scripts\CardboardEye.cs:96)
I/Unity   ( 8002):
I/Unity   ( 8002): (Filename: J Line: 0)
I/Unity   ( 8002):
D/Unity   ( 8002): Setting up 1 worker threads for Enlighten.
D/Unity   ( 8002):   Thread -> id: b7e71550 -> priority: 1
D/Unity   ( 8002): Unloading 7 Unused Serialized files (Serialized files now loaded: 0)
D/Unity   ( 8002): UnloadTime: 87.258003 ms
I/DEBUG   (  185): *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
I/DEBUG   (  185): Build fingerprint: 'google/occam/mako:5.1.1/LMY48T/2237560:user/release-keys'
I/DEBUG   (  185): Revision: '11'
I/DEBUG   (  185): ABI: 'arm'
I/DEBUG   (  185): pid: 8002, tid: 8031, name: UnityMain  >>> com.OgmaDigi.VeryBasicCardBoardTest <<<
I/DEBUG   (  185): signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x9dc2bff0
I/DEBUG   (  185):     r0 9dc2bff0  r1 9dc2c000  r2 fffffe8c  r3 9dc2c000
I/DEBUG   (  185):     r4 9dc2bff0  r5 00000001  r6 9dc2bffc  r7 af6a0000
I/DEBUG   (  185):     r8 9a5625d4  r9 00000000  sl 00000002  fp a33d871c
I/DEBUG   (  185):     ip afb3fed8  sp a33d86f0  lr afa60d7c  pc afa60cec  cpsr 80010010
I/DEBUG   (  185):
I/DEBUG   (  185): backtrace:
I/DEBUG   (  185):     #00 pc 002b2cec  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #01 pc 002b2d78  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so (GC_push_all_stack+72)
I/DEBUG   (  185):     #02 pc 002bb9fc  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #03 pc 002bbacc  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #04 pc 002b7840  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #05 pc 002b4834  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #06 pc 002b1128  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #07 pc 002ab1fc  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #08 pc 002aace0  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #09 pc 002abaa4  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #10 pc 002abb44  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so
I/DEBUG   (  185):     #11 pc 001396d8  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libmono.so (mono_gc_collect+40)
I/DEBUG   (  185):     #12 pc 003aea64  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (CleanupAfterLoad()+92)
I/DEBUG   (  185):     #13 pc 00402a04  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (LoadSceneOperation::PlayerLoadSceneFromThread()+340
)
I/DEBUG   (  185):     #14 pc 004027b4  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (LoadSceneOperation::IntegrateMainThread()+184)
I/DEBUG   (  185):     #15 pc 003ffed4  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (PreloadManager::UpdatePreloadingSingleStep(PreloadM
anager::UpdatePreloadingFlags, int)+388)
I/DEBUG   (  185):     #16 pc 004002ec  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (PreloadManager::WaitForAllAsyncOperationsToComplete
()+72)
I/DEBUG   (  185):     #17 pc 004003e8  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (PreloadManager::UpdatePreloading()+164)
I/DEBUG   (  185):     #18 pc 003a7500  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (PlayerLoop(bool, bool, IHookEvent*)+664)
I/DEBUG   (  185):     #19 pc 00513b2c  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (UnityPlayerLoop()+700)
I/DEBUG   (  185):     #20 pc 005192c8  /data/app/com.OgmaDigi.VeryBasicCardBoardTest-2/lib/arm/libunity.so (nativeRender(_JNIEnv*, _jobject*)+264)
I/DEBUG   (  185):     #21 pc 0000dbe7  /data/dalvik-cache/arm/data@[email protected]@[email protected]
I/DEBUG   (  185):
I/DEBUG   (  185): Tombstone written to: /data/tombstones/tombstone_04
I/ActivityManager(  548): Process com.OgmaDigi.VeryBasicCardBoardTest (pid 8002) has died
W/ActivityManager(  548): Force removing ActivityRecord{521dc07 u0 com.OgmaDigi.VeryBasicCardBoardTest/com.google.unity.GoogleUnityActivity t85}: app died, no s
aved state
I/ActivityManager(  548): Config changes=480 {1.0 272mcc5mnc en_GB ?layoutDir sw384dp w384dp h567dp 320dpi nrml port finger -keyb/v/h -nav/h s.15}
V/ActivityManager(  548): Display changed displayId=0

","
There is a known crash in cardboard plugin that happens on LoadLevel, workaround is to remove the OnPause method from BaseCardboardDevice.cs
See more details here: https://github.com/googlesamples/cardboard-unity/issues/126
Another suggested solution is to disable cardboard components before loading level, but it seems to be not effective.
"
"
When I use cardboard on a device which has a big screen, 5.5 inch.
And it's resolution is 2560*1440，dpi=640.
The question is that the parallax is too big, so that I can see two non overlapping images through a VR device.
So, what should I to to fix it?
Thanks.
","
It looks like your phone might be reporting an incorrect DPI, which prevents the SDK from drawing the two images at the right place on the screen.
According to the numbers you gave, the phone DPI should be around 534, not 640.
What is your phone model? Even better, could you give us the output of the following: 
adb shell getprop | grep ""model|manufacturer|hardware|product.name|lcd_density""
This way we could add a DPI override in the cardboard SDK for your phone model.
"
"
I'm building an app with Unity and Google Cardboard SDK. 
First prototype involves looking at items coming from the distance towards the user.
The user taps (pulls the trigger on the Cardboard device) and when the Gaze Input Module's cursor is on an item coming towards the user, it's removed.
The problem is, that on Android devices a tap on one item works as it should, but also the next item that's coming towards the user is getting hit as though it was tapped even though I'm only gazing at it without pulling the trigger.
I've tried using different Event Triggers on the oncoming prefabs but that doesn't make a difference.
Switching the Tap is Trigger checkbox on the Cardboard object sort of reverses the problem: then I have to tap every item twice for it to register a hit (instead of hitting two items with one tap). Also quite annoying.
Has anyone dealt with this issue before? I use Unity 5.2.3p1 and Cardboard SDK 0.5.2, both of which are the latest versions.
","
I opened an issue on Google-Cardboard-Unity's Github page and it seems to be a bug that occurs on pre-4.4 Android versions. They don't mention a fix however.
"
"
I'm trying to find out if there are any examples how to play 3D video for cardboard in android. I have a 3D video and need my app to play in cardboard. I'm not sure if the SDK has support and couldn't figure it out as there is very less documentation.
","
Thanks for all the answers. For me only solution that worked is to use that is described at HERE. Had to Use unity with a plugin to play video on movietexture.
Without using the unity plugin Easy Movie Texture plugin I couldn't find any easier solution.
"
"







Closed. This question needs to be more focused. It is not currently accepting answers.
                                
                            











Want to improve this question? Update the question so it focuses on one problem only by editing this post.


Closed 8 years ago.







                        Improve this question
                    



I have four images; each is 350 pixels wide; I have the text North on the first, East on the second, South on the third and West on the fourth. I put them into a scrollView in a single image and I link said scrollView to the direction I am pointing my iPad. Now as I pan around I want the scroll to pan around the images, so if I am looking north and turn east I see north slide of the screen and east onto it.
Now I tried to do this using the compass, and it works reasonably well, but how to code crossing the boundary between 0 degrees and 360 degrees is a challenge. I tried using yaw so that I am looking at two 180 degree circles, but working out how to make that work has evaded me too. I google this and find quaternion and euler angles, but nothing that makes enough sense to me... 
Need some direction/help.
","
You don't really need a scroll view for this, since you are controlling the scroll offset by device heading rather than by letting the user swipe the screen. So you could just use a plain old UIView as the parent of the image views.  You would just update view.bounds.origin instead of scrollView.contentOffset to “scroll”. (Under the hood, the scroll view's contentOffset is the same as its bounds.origin.)
Anyway, lay out your images like this:
+-------+-------+-------+-------+-------+
| North |  East | South |  West | North |
+-------+-------+-------+-------+-------+

Note that the North image is used twice.
When you get a new heading, update the offset of the scroll view like this:
let range = CGFloat(350 * 4)
let offset = range * (CGFloat)direction / 360
scrollView.contentOffset = CGPointMake(offset, 0)

"
"
In a test scene with a locomotion character, when I attach a SmoothFollow script to the character, it works as it should, but when I use Oculus Rift to view the scene in VR, it no longer follows the character as it walks...
I am aware that the camera transform is over-ridden with the head-tracked pose, and that if I want to move the camera, I must attach it as a child to another game object and then move the root game object, but doing so still would not let me follow the character in VR.
Am I missing something, or is it not possible to have this in Oculus Rift where you can just make the character walk and you automatically follow it?
","
It sounds like you are close. Did you attach the SmoothFollow script to the new GameObject (parent of the camera) rather than the camera itself?
Also, you may want to comment out the part of SmoothFollow where it sets the rotation of the camera - this can be very disorienting in VR.
"
"
how can I handle back button single click , long click to pause game , if pressed again destroy game 
","
I think the back button maps to mouse button 1 so you should be able to pick it up with:
Input.GetMouseButtonDown (1);

To detect double clicks or long clicks you would measure the time between each click or how long the button has been pressed, respectively. I have never tried this but it could be something along the lines of setting up a variable to hold the time interval and checking if the second click happens within that time.
-EDIT-
I just found another discussion about this here with some examples which should help, e.g.
if(Input.GetMouseButtonDown(0))
{
 _buttonDownPhaseStart = Time.time;    
}

if (_doubleClickPhaseStart > -1 && (Time.time - _doubleClickPhaseStart) > 0.2f)
{
     Debug.Log (""single click"");
     _doubleClickPhaseStart = -1;
}

if( Input.GetMouseButtonUp(0) )        
{
      if(Time.time - _buttonDownPhaseStart > 1.0f)
 {
     Debug.Log (""long click"");
     _doubleClickPhaseStart = -1;
 }
 else
 {
     if (Time.time - _doubleClickPhaseStart < 0.2f)
      {
           Debug.Log (""double click"");
           _doubleClickPhaseStart = -1;
      }
      else
      {
         _doubleClickPhaseStart = Time.time;
      }    
 }
}

"
"
Is there any way/program(s)/api/etc. which can help to write a program which can automatically convert a spherical jpeg image (for example like http://www.jcwilson.net/craigendarroch.jpg) into a virtual reality movie so that youtube can enable the view-around function?
The photo should just be displayed for maybe 10 seconds.
Thank you for help!
","
So you're looking for a way to change a picture to a movie with one frame that lasts 10 seconds? You want ffmpeg for that: Create a video slideshow from images
.
The question is: why on youtube? There are better ways to display a spherical picture.
Oh and it's 360x180.
"
"
I am trying to extend this Roll-a-Ball tutorial to include a timer and allow the user to try again by tapping the touchpad whether they win or run out of time.
This works as expected if the time runs out (// case A below) but not if the player wins (// case B below), where the tap does not seem to be recognised. The end message appears in both cases so it is definitely reaching those parts but I'm guessing that the program is not reaching the section with the comment // reset on tap but am not certain.
Any ideas appreciated.
My PlayerController script:
void Start ()
{
    timeLeft = 5;
    rb = GetComponent<Rigidbody>();
    count = 0;
    winText.text = """";
    SetCountText ();
}
void Update()
{
    if (!gameOver) {
        timeLeft -= Time.deltaTime;
    }
    timerText.text = timeLeft.ToString (""0.00"");
    if(timeLeft < 0) {
        winner = false;
        GameOver(winner);
    }
}
void GameOver(bool winner)
{
    gameOver = true;
    timerText.text = ""-- --"";
    string tryAgainString = ""Tap the touch pad to try again."";
    if (!winner) { // case A
        winText.text = ""Time's up.\n"" + tryAgainString;
    }
    if (winner) { // case B
        winText.text = ""Well played!\n"" + tryAgainString;
    }
    // reset on tap
    if (Input.GetMouseButtonDown (0)) {
        Application.LoadLevel(0);
    }
} 
void FixedUpdate ()
{
    float moveHorizontal = Input.GetAxis (""Mouse X"");
    float moveVertical = Input.GetAxis (""Mouse Y""); 
    Vector3 movement = new Vector3 (moveHorizontal, 0.0f, moveVertical);    
    rb.AddForce (movement * speed);
}
void OnTriggerEnter(Collider other) 
{
    if (other.gameObject.CompareTag ( ""Pick Up"")){
        other.gameObject.SetActive (false);
        count = count + 1;
        SetCountText ();
        if (!gameOver) {
            timeLeft += 3;
        }
    }
}   
void SetCountText ()
{
    if (!gameOver) {
        countText.text = ""Count: "" + count.ToString ();
    }
    if (count >= 12) {
        winner = true;
        GameOver(winner);
    }
}

","
Put a Debug.Log in SetCountText method, and output the value of count count. You are probably never hitting the 12 point mark.
Make sure all your collectibles have the tag "" Pick Up"".
Update
You should listen for player input in Update method. FixedUpdate and any other functions that execute as part of Fixed Update will miss the player input if it happens between two FixedUpdate calls.
So change your Update and GameOver method as follows:
void Update() {
    if (gameOver) {
        if (Input.GetMouseButtonDown(0)) {
            Application.LoadLevel(0);
        }
    } else {
        timeLeft -= Time.deltaTime;
        timerText.text = timeLeft.ToString(""0.00"");
        if (timeLeft < 0) {
            winner = false;
            GameOver(winner);
        }

    }

}
void GameOver(bool winner) {
    gameOver = true;
    timerText.text = ""-- --"";
    string tryAgainString = ""Tap the touch pad to try again."";
    if (!winner) { // case A
        winText.text = ""Time's up.\n"" + tryAgainString;
    }
    if (winner) { // case B
        winText.text = ""Well played!\n"" + tryAgainString;
    }

}

"
"
I have built a Unity3D + Google Tango based game on the NVidia Dev. device. Everything seems to work fine, but now I would like to play this game in stereoscopic view (For Dive Goggles). I looked at the ExperimentalVirtualReality example (https://github.com/googlesamples/tango-examples-unity/tree/master/UnityExamples/Assets/TangoExamples/ExperimentalVirtualReality) and was successfully able to port all the prefabs into my game, but for some reason the experience is not satisfactory. 
The stereoscopic view of my game tends to over lap with each other when I look through the Dive goggles. The experience is a quite off. 
I noticed that there are some public parameters on the TangoVR Player Object in Unity Project for 'IPD in MM', 'Screen Width in MM', 'Eye Offset in MM', etc. Do I have to play around with any of these. What does these values even represent?
Any help or pointers will be greatly helpful and appreciated. 
","
Thanks for all those who helped answer this. Many of my concepts definitely got cleared but nothing got me close to an actual solution. After researching a lot, I finally found this article (http://www.talkingquickly.co.uk/2014/11/google-cardboard-unity-tutorial/)  super useful. it basically tells me to implement the Durovis SDK (https://www.durovis.com/sdk.html) with its Unity package. 
Everything was pretty straightforward and experience I got from it was so far the best.
"
"
I am working in game project (shooter game) 
I want my player to have a shoot ray  show the point of impact 
The Player ------------> my target

how can I approach this in untiy5 using C#
","
You need to do a raycast forward from the camera and check what did it hit. 
When you have both points (one being the weapon/camera, second the place it hit) connect them with a line using a line renderer.
"
"
I am using Unity 5.1.2p3 with DK2 SDK 0.6.0.1 and I understand from this post that Screen Space - Overlay is not supported in Unity VR. It is recommended to use Screen Space - Camera (which in my case does not work) or World Space (which I am using now) but I need someone to help me understand how I get simple menu with buttons and toggles to show as a still image and how I can make selections and button presses with my mouse cursor.
I have created a menu for my app, with 4 toggles and 1 button. When I check the Virtual Reality Supported option with the Oculus being in Direct Mode and Canvas being in World Space, I can see it in VR, but I cannot see/find my mouse cursor to tick one of the toggles.
When I take off the headset, on my monitor's Game View tab, I can see and even use the mouse and select a toggle. Obviously, I have to keep the headset steady, so in my Game View, things do not shake!
Another thing I notice is that the VR camera is the same as the Main Camera in the Unity Hierarchy, but when I take off the headset and move it around, the position of the camera does not change, only looking up and down and around is reflected.
How do I simply do a static menu like a 2D surface that does not move in VR and a user can use button presses and muse clicks with the headset on? What settings are required for this way of doing UI and canvas stuff? There are 2 attachments, showing my current settings...


","
Are you specifically wanting to use the mouse? If you look through a blog entry I wrote below, it will show you how to use Gaze looking to trigger menu buttons:
http://talesfromtherift.com/vr-gaze-input/
You can achieve this by some code I list there that raycasts from the center of the screen and if it hits any UI, it will trigger the correct events and make it clickable by button (or by time).
"
"
I have an issue with running Oculus DK 2 on Macbook, the latest version Oculus runtime. The issue is that I cannot change the display mode in settings, it is disabled. Therefore, when I run the demo scene, it is only display on the main desktop, nothing on Oculus Rift display. Please help!!! 
Please see this screenshot: https://drive.google.com/file/d/0B8WtbdbX1xQiZGx4dHc0QmttQk0/view
","
Direct to rift is not supported on OSX. Rift works only as a second screen. For a quick howto on how to setup the screen look at http://mozvr.com/downloads/ 
Also, note that oculus on OSX is not supported for some time now, and things like firmware update on windows might break support on OSX.
"
"
I built the application, works fine. But now the boss wants the capability for users to concentrate on a point in the screen and it triggers an event. Call me an old fashioned dev but I haven't got a clue. Any insight would be appreciated.
","
First off, you really need to find out what your boss means by ""... users to concentrate on a point in the screen and it triggers an event"". There is no eye tracking capability (yet) build into any VR HMD. That might end the vague request right there.
OTOH - what most apps do that need some user input without using any buttons is called ""gaze tracking"" or ""gaze input"" but what it really is at this stage of VR development is if the user has centered the screen about some UI trigger, then you can (typically) show some hourglass-like object superimposed on the trigger in an obvious countdown mode. If the countdown goes off, the input is triggered. If the user ""gazes away"" (i.e. moves their head), you abort the countdown and turn off the UI visuals. Works best if there's a reticule that's either always on, or comes into focus when it's near a trigger. While reasonably intuitive, it's not my favorite technique, and it's got serious problems when you do it in a stereo scene, but a lot of apps currently use it. Get a GearVR and visit the Oculus store/apps, they do it as good as anyone, plus they have some nice documentation of best practices.
"
"
Briefly, I'm combining two open source apps into a new VR app, so this only runs on the Note 4 and S6 using the GearVR headset.  My app works on kitkat, but the video is black on lollipop.  The two source apps both work fine on lollipop.
I have a surface created from a gl texture:
glGenTextures( 1, &textureId );
glBindTexture(GL_TEXTURE_EXTERNAL_OES, textureId);
glTexParameterf(GL_TEXTURE_EXTERNAL_OES, GL_TEXTURE_MIN_FILTER, GL_LINEAR);
glTexParameterf(GL_TEXTURE_EXTERNAL_OES, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
glTexParameterf(GL_TEXTURE_EXTERNAL_OES, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
glTexParameterf(GL_TEXTURE_EXTERNAL_OES, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);
glBindTexture(GL_TEXTURE_EXTERNAL_OES, 0);

That gets put into a SurfaceTexture that gets put into a Surface, that gets passed around a bit, then sent to a video decoder:
videoDecoder = MediaCodec.createByCodecName(decoderName);
MediaFormat videoFormat = MediaFormat.createVideoFormat(""video/avc"", width, height);
videoDecoder.configure(videoFormat, ((SurfaceHolder)renderTarget).getSurface(), null, 0);
videoDecoder.setVideoScalingMode(MediaCodec.VIDEO_SCALING_MODE_SCALE_TO_FIT);

// OMX.qcom.video.decoder.avc gets picked for decodername
// Width and height at 1280x720 everywhere

The texture is updated and bound to a surface and rendered.
This works perfectly on kitkat.  On lollipop, the video is completely black.
You can see a diff of the logs (good kitkat red, bad lollipop green) here:
https://www.diffchecker.com/lxxopmhc
Nothing stands out to me as particularly informative.
(The message do not know color format 0x7fa30c04 = 2141391876 from what I can find is just from the thing that turns decoder IDs into names, and shouldn't affect anything.  Another decoder doesn't give this message, but also doesn't work.)
Adding an explicit color format to the MediaFormat, and twiddling with color formats and image sizes everywhere else doesn't have any effect (only black screen, no errors)
I can lock a canvas on the surface and drawARGB(255,0,255,0), screen turns green.
I created a subclass of surface that logged every public method and passed that to the decoder, and none of them got called (other than the initialization when I created it) on kitkat or lollipop, working or not, so that told me nothing at all.
The code for doing the video decoder setup came from Moonlight, which works perfectly fine on lollipop:
github.com/moonlight-stream/moonlight-android
The other half is Oculus Cinema, which displays on the surface using android.media.MediaPlayer and works fine on lollipop.
So my questions are what changed on lollipop or what have I done wrong, and how can I debug this problem any further?
My code is here:
https://github.com/GTMoogle/StreamTheater
Update 6/2:
Tried following suggestion from Q11 of http://bigflake.com/mediacodec/ to set the buffer position and limits, but still no output, though I might have implemented it incorrectly.  Also took care of some depricated calls, also no effect.
Up-Update: The BufferInfo from the dequeueOutputBuffer seems to always have a size of 8, 0 offset, no flags set.  Not sure what a healthy stream looks like yet.
","
Ahhah.  Eventually found it.
For some reason updateTexImage isn't changing the timestamp of the surface texture.  Mediacodec was setting it on kitkat, so either the input buffer doesn't have a timestamp set correctly and mediacodec was cleaning up after it, or mediacodec is no longer setting it when updating the texture?
I can debug it, but I can at least for the moment just always update instead of check whether the timestamp changed.
"
"
i'm trying to develop an Augumented reality app , and i begin by simple exemple an image target and a gameobject (cube) appear on it , the result show succefully in unity , but when i try to build the project to android device , the screen of device change completly and no (cube ) appear . 
this is how it appear in mobile device http://i59.tinypic.com/2ljjqcg.png
but in unity it work good, http://i59.tinypic.com/2nv4cq9.png
","
Just change the ARcamera>Camera>BackgroundPlane video material from custom/videoBackground to custom/renderedVideoBackground.
Also, ensure you've used the correct Vuforia License Keys for the Database being used.
"
"
I am currently creating a VR web app using three.js. As the camera controls I am using the device orientation controls used here in the google cardboard three.js demo.
What I need to do is add keyboard controls to this(e.g Up arrow to go forward etc). I've fiddled around with moving the camera on the two axis (x and z) here:
if (e.keyCode == '38') {
    camera.position.set(0, 10, camera.position.z+4);

    controls.target.set(
        camera.position.x +4,
        camera.position.y,
        camera.position.z
    );
    effect.render(scene, camera);
...

However I want to make the character move relative to where they are looking (e.g You look one way and press the Up arrow and the character moves the way you looking). Like a first person view. 
Does anyone have any ideas on how this is done? Ive tried using the first person controls from three.js but this eliminates the head tracking which is essential for a VR game. 
Any answers would be greatly appreciated. (My source code is practically just the Google cardboard three.js demo code with a function added in too detect key presses)
","
I solved this by different approach. I created an object3d which is moving in scene. Model and camera are child of this object.
I'm rotating object 3d with camera and in the same time rotate model in opposite direction. When i rotate camera object looks keeping direction. when i want to move object, just translateX object with camera and make model rotation to 0. That did the trick.
On long distances (I have millions of units) started to be jerky. Reason is lost precision.
I solved it by keeping position of object at 0,0,0 and move all other things in opposite direction. That makes your model is still on 0,0,0 coords with right rotation and world is moving around.
Most simple example:
you trying something like
scene.add(character_model);
scene.add(camera);
//camera.rotate ....
character_model.translateX(1);
character_model.rotateX(1);
//etc ...

and now you trying to move camera around the pivot (character_model), but this is overcomplicated mathematics.
Try:
var controls_dimension = new THREE.Object3D();
scene.add(controls_dimension);
controls_dimension.add(character_model);
controls_dimension.add(camera);

//use controls to rotate with this object, not with character_model
controls_dimension.rotateX(2);

// in the same rotate model to opposite direction. You can make 
// illusion of rotating camera, not a model.
character_model.rotateX(2*-1);

/* 
when you want to go in camera direction=controls_dimension=controls_dimension.translateX(1) 
and we moving (you most only animate model rotation to controls_dimension.rotation)
*/

"
"
I'm building a VR app launcher,
For this I'm running through the package manager of the installed apps and running the match command for ""vr"" in the packageName, which gives the result I need only if the developer have decided to add the word VR to his app name.
I'm wondering how will this detection can be made 100%.
The Manifest for a VR app doesn't seems to have any particular distinguished tag,
VR apps must use a specific JAR and are running CardboardActivity instead of regular Activity, but I don't know how to search for this features.
Any help would be appreciated,
Thanks. 
","
The comment by random user was correct:
Through the package manager one should read the intent of the application and search for the following intent : com.google.intent.category.CARDBOARD
    Intent intentToResolve = new Intent(Intent.ACTION_MAIN);
    intentToResolve.addCategory(""com.google.intent.category.CARDBOARD"");
    intentToResolve.setPackage(appInfo.packageName);
    ResolveInfo ri = pm.resolveActivity(intentToResolve, 0);

the ResolveInfo is !=null if intent is Cardboard app.
"
"
I am trying to modify the Google Cardboard SDK demo in Unity3d (version 4.6.4f1) for an Android device.
The following description is based on this -picture- I made of what I am trying to accomplish. 

At the start of the game a stationary red cube will be in center of vision.
The user can look around in 360 degrees and for example he tilts his/her head up to look at the clouds. 
When the user double taps the touch screen the red cube will transport to the center of the new field of vision facing the user.

Thanks!
","
You need to use the CardboardHead part of the CardboardMain prefab. In your cube script, leave a public GameObject reference and set it to CardboardHead in the editor.
Next, in your double tap handling function, set your position accordingly to the forward vector of the head.
If you want the cube (or any other 3D object) to face the player, you need to use transform.LookAt()
It will probably look something like this:
public GameObject cardboardHead;
public float distanceFromCamera;

public void doubleTapped()
{
  Vector3 newPosition = cardboardHead.transform.position + (cardboardHead.transform.forward * distanceFromCamera);
  transform.position = newPosition;
  transform.LookAt(cardboardHead.transform);
}

"
"
I am new to VR applications design and development and need your expert opinion. Will Ocusdk work on google cardboard, after specially with a release of oculus mobile sdk ( which is used in samsung vr-gear) or cardboard only works with Cbsdk? Thanks in advance.
","
As per the license, you cannot use the oculus mobile sdk for cardboard apps.

The Oculus Mobile SDK may not be used to interface with unapproved commercial virtual reality mobile or non-mobile products or hardware.

link to the license
"
"
I'm working on Unity, w/ Vuforia.
I've got Virtual buttons that I need to act like the Up/Down Arrows on the keyboard to move an object which is not in its image target, so I'm searching for the basics.
My class starts like this:
public void OnButtonPressed(VirtualButtonAbstractBehaviour vb){
...
}

What do I need to put in this to make it act like the up button?
Without these virtual buttons, my script would move the object like this:
void FixedUpdate(){
        float moveHortizonal = Input.GetAxis(""Horizontal"");
        float moveVertical = Input.GetAxis (""Vertical"");

        Vector3 movement = new Vector3 (moveHortizonal, 0, moveVertical);

        rigidbody.AddForce (movement * speed);
    }

","
Figured out how to do it, and here's a small tutorial on my findings
First of all, you start with registering the buttons:
void Start () {
        VirtualButtonBehaviour[] vbs = transform.GetComponentsInChildren<VirtualButtonBehaviour> ();

        for (int i=0; i < vbs.Length; ++i) {
            vbs[i].RegisterEventHandler(this);
        }

And now you go ahead and start with the function of the button
   public void OnButtonPressed(VirtualButtonAbstractBehaviour vb){
    //specify which button you want to function by using the if statement
    if(vb.name==""ButtonName"") { callButtonfunction();}
    }

Similarly if you want a button to do something on release:
   public void OnButtonReleased(VirtualButtonAbstractBehaviour vb){
    //specify which button you want to function by using the if statement
    if(vb.name==""ButtonName"") { callButtonfunction();}
    }

In case you want your button to control a Gameobject, then go ahead and declare the GameObject as a public variable in the class so that it can be accessed in the Inspector and assigned accordingly.
public GameObject human; 
Where GameObject is the variable type and human is the variable name that we use for reference
It's as simple as that.

The Vuforia logs are very badly documented and so you can almost never
  get an answer from there, so I hope this helps.

"
"
With the current WebVR API, I can get headset position & orientation data relative to the zeroState.
But how can I determine where the headset is relative to the positional tracker camera? I'd like to draw a 3D model of the tracker along with FOV constraints in the virtual world, much like the Oculus Config demo scene.
","
WebVR does not expose the positional tracker camera via API, and they do not have any plans to expose it as other headsets might not have external cameras. Therefore, it's impossible to determine headset position/orientation relative to the tracker, and there is no frustum data.

""At this point no, because it's not obvious if that will be a common element of many HMDs.""

https://twitter.com/Tojiro/status/560614873327869952
"
"
I am coding a small project with Oculus Rfit support, and i use point sprites to render my particles. I calculate the size of the point sprites in pixels, based on their distance from the ""camera"" in the vertex shader. When drawing on the default screen (not on the Rift) the size works perfectly, but when i switch to the Rift i notice these phenomena:

The particles on the Left Eye are small and get reduced in size very rapidly.
  The particles on the Right Eye are huge and do not change in size.

Screenshots:
Rift disabled: https://i.sstatic.net/03l3o.jpg
Rift enabled: https://i.sstatic.net/4tswC.jpg
Here is the vertex shader:
#version 120

attribute vec3 attr_pos;
attribute vec4 attr_col;
attribute float attr_size;

uniform mat4 st_view_matrix;
uniform mat4 st_proj_matrix;
uniform vec2 st_screen_size;

varying vec4 color;

void main()
{
    vec4 local_pos = vec4(attr_pos, 1.0);
    vec4 eye_pos = st_view_matrix * local_pos;
    vec4 proj_vector = st_proj_matrix * vec4(attr_size, 0.0, eye_pos.z, eye_pos.w);
    float proj_size = st_screen_size.x * proj_vector.x / proj_vector.w;

    gl_PointSize = proj_size;
    gl_Position = st_proj_matrix * eye_pos;

    color = attr_col;
}

The st_screen_size uniform is the size of the viewport. Since i am using a single frambuffer when rendering on the Rift (1 half for each eye), the value of st_screen_size should be (frabuffer_width / 2.0, frambuffer_height).
Here is my draw call:
    /*Drawing starts with a call to ovrHmd_BeginFrame.*/
    ovrHmd_BeginFrame(game::engine::ovr_data.hmd, 0);

    /*Start drawing onto our texture render target.*/
    game::engine::ovr_rtarg.bind();
    glClearColor(0, 0, 0, 1);
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);

   //Update the particles.
    game::engine::nuc_manager->update(dt, get_msec());

    /*for each eye... */
    for(unsigned int i = 0 ; i < 2 ; i++){
        ovrEyeType eye = game::engine::ovr_data.hmd->EyeRenderOrder[i];
        /* -- Viewport Transformation --
         * Setup the viewport to draw in the left half of the framebuffer when we're
         * rendering the left eye's view (0, 0, width / 2.0, height), and in the right half
         * of the frambuffer for the right eye's view (width / 2.0, 0, width / 2.0, height)
         */
        int fb_width = game::engine::ovr_rtarg.get_fb_width();
        int fb_height = game::engine::ovr_rtarg.get_fb_height();

        glViewport(eye == ovrEye_Left ? 0 : fb_width / 2, 0, fb_width / 2, fb_height);

      //Send the Viewport size to the shader.
      set_unistate(""st_screen_size"", Vector2(fb_width /2.0 , fb_height));

        /* -- Projection Transformation --
         * We'll just have to use the projection matrix supplied but he oculus SDK for this eye.
         * Note that libovr matrices are the transpose of what OpenGL expects, so we have to
         * send the transposed ovr projection matrix to the shader.*/
        proj = ovrMatrix4f_Projection(game::engine::ovr_data.hmd->DefaultEyeFov[eye], 0.01, 40000.0, true);

      Matrix4x4 proj_mat;
      memcpy(proj_mat[0], proj.M, 16 * sizeof(float));

      //Send the Projection matrix to the shader.
      set_projection_matrix(proj_mat);

        /* --view/camera tranformation --
         * We need to construct a view matrix by combining all the information provided by
         * the oculus SDK, about the position and orientation of the user's head in the world.
         */
         pose[eye] = ovrHmd_GetHmdPosePerEye(game::engine::ovr_data.hmd, eye);

         camera->reset_identity();

         camera->translate(Vector3(game::engine::ovr_data.eye_rdesc[eye].HmdToEyeViewOffset.x,
          game::engine::ovr_data.eye_rdesc[eye].HmdToEyeViewOffset.y,
          game::engine::ovr_data.eye_rdesc[eye].HmdToEyeViewOffset.z));

         /*Construct a quaternion from the data of the oculus SDK and rotate the view matrix*/
         Quaternion q = Quaternion(pose[eye].Orientation.w, pose[eye].Orientation.x,
                                   pose[eye].Orientation.y, pose[eye].Orientation.z);
         camera->rotate(q.inverse().normalized());


         /*Translate the view matrix with the positional tracking*/
         camera->translate(Vector3(-pose[eye].Position.x, -pose[eye].Position.y, -pose[eye].Position.z));

       camera->rotate(Vector3(0, 1, 0), DEG_TO_RAD(theta));

       //Send the View matrix to the shader.
       set_view_matrix(*camera);



         game::engine::active_stage->render(STAGE_RENDER_SKY | STAGE_RENDER_SCENES | STAGE_RENDER_GUNS |
          STAGE_RENDER_ENEMIES | STAGE_RENDER_PROJECTILES, get_msec());
         game::engine::nuc_manager->render(RENDER_PSYS, get_msec());
       game::engine::active_stage->render(STAGE_RENDER_COCKPIT, get_msec());
    }

    /* After drawing both eyes into the texture render target, revert to drawing directly to the display,
     * and we call ovrHmd_EndFrame, to let the Oculus SDK draw both images properly, compensated for lens
     * distortion and chromatic abberation onto the HMD screen.
     */
    game::engine::ovr_rtarg.unbind();

    ovrHmd_EndFrame(game::engine::ovr_data.hmd, pose, &game::engine::ovr_data.fb_ovr_tex[0].Texture);

This problem has troubled me for many days now...and i feel like i have reached a dead end. I could just use billboarded quads.....but i don't want to give up that easily :) Plus point sprites are faster.
Do the math behind Point size attenuation based on distance change when rendering on the Rift?
Am a not taking something into account?
Math is not (,yet at least) my strongest point. :) Any insight will be greatly appreciated!
PS: If any additional information is required about the code i posted, i will gladly provide it.
","

vec4 local_pos = vec4(attr_pos, 1.0);
vec4 eye_pos = st_view_matrix * local_pos;
vec4 proj_voxel = st_proj_matrix * vec4(attr_size, 0.0, eye_pos.z, eye_pos.w);
float proj_size = st_screen_size.x * proj_voxel.x / proj_voxel.w;
gl_PointSize = proj_size;

Basically you are first transforming your point to view space to figure out it's Z coordinate in view space (distance from the viewer) and then you're constructing a vector aligned with the X axis with the desired particle size, and projecting that to see how many pixels it covers when projected and viewport-transformed (sortof).
This is perfectly reasonable, assuming your projection matrix is symmetrical. This assumption is wrong when dealing with the rift. I've drawn a diagram to illustrate the problem better:
https://i.sstatic.net/aLKkx.jpg
As you can see, when the frustum is assymetrical, which is certainly the case with the rift, using the distance of the projected point from the center of the screen will give you wildly different values for each eye, and certainly different from the ""correct"" projection size you're looking for.
What you must do instead, is project two points, say (0, 0, z, 1) AND (attr_size, 0, z, 1), using the same method, and compute their difference in screen space (after projection, perspective divide, and viewport).
"
"
I'm working in a project with the cardboard from Google. I'm asking if there is someone who have the possibility to help with understanding and creating a script for the zoom in and out with the magnet tool on the cardboard. Thanks a lot for your help.
","
 Used this code 
@Override
public void onCardboardTrigger() {
if (xyz) {
 //Create a function 
zoom();
    ...
} else {

}
// Always give user feedback
  mVibrator.vibrate(50);
}

"
"
I'm trying to get Oculus Rift DK2 orientation input working with WebVR - either using Three.js' VRControls (examples/js/controls/VRControls.js) or directly from the PositionSensorVRDevice. 
But the PositionSensorVRDevice's state values orientation and position are always either null (Firefox) or VRPoint3D/4Ds with x,y,z = 0 (Chrome), so the camera orientation never changes.
The Firefox and Chrome VR builds I've been trying are:

Firefox 2014-10-18 version
Chromium_WebVR_OSX_SDK_0.4.2.dmg

It is getting hold of a PositionSensorVRDevice OK, although it looks like it may not be the real one, because Chrome reports it as:

deviceId: debug-0
deviceName: Moculus Rift
hardwareUnitId: debug-0

And FF reports it as:

deviceId: somedevid
deviceName: HMD Position Device
hardwareUnitId: unknownHMDInfo-0x1351d4000

I get the same result with the Leap Motion VR Quickstart demo and the Three.js WebGL Effects VR demo. I also get the same with a demo I did a couple of months back which used to work - it doesn't use VRControls, but deals with the PositionSensorVRDevice directly.
Needless to say the Rift is all hooked up and the Demo Scene from the Oculus Rift Config Util is working fine. I'm on Firmware version 2.12.
The relevant code in VRControls is the update method - I've been checking the values of state.orientation and state.position here:
this.update = function () {

    if ( vrInput === undefined ) return;

    var state = vrInput.getState();

    if ( state.orientation !== null ) {
        object.quaternion.copy( state.orientation );
    }

    if ( state.position !== null ) {
        object.position.copy( state.position );
    }

};

Any suggestions gratefully received!
","
Oops, turns out I wasn't on the latest version of the Oculus runtime! I was on v0.4.1 - I needed to upgrade to the 0.4.3 beta. Working fine now in the latest VR builds of FF and Chrome.
In case it helps anyone else - I found which version I needed from Vlad's post on the web-vr mailing list.
"
"
I'm trying to smooth the data I'm getting from the deviceOrientation API to make a Google Cardboard application in the browser. 
I'm piping the accelerometer data straight into the ThreeJs camera rotation but we're getting a lot of noise on the signal which is causing the view to judder. 
Someone suggested a Kalman filter as the best way to approach smoothing signal processing noise and I found this simple Javascript library on gitHub
https://github.com/itamarwe/kalman
However its really light on the documentation. 
I understand that I need to create a Kalman model by providing a Vector and 3 Matrices as arguments and then update the model, again with a vector and matrices as arguments over a time frame.
I also understand that a Kalman filter equation has several distinct parts: the current estimated position, the Kalman gain value, the current reading from the orientation API and the previous estimated position. 
I can see that a point in 3D space can be described as a Vector so any of the position values, such as an estimated position, or the current reading can be a Vector.
What I don't understand is how these parts could be translated into Matrices to form the arguments for the Javascript library.
","
Well, I wrote the abhorrently documented library a couple of years ago. If there's interest I'm definitely willing to upgrade it, improve the documentation and write tests.
Let me shortly explain what are all the different matrices and vectors and how they should be derived:
x - this is the vector that you try to estimate. In your case, it's probably the 3 angular accelerations.
P - is the covariance matrix of the estimation, meaning the uncertainty of the estimation. It is also estimated in each step of the Kalman filter along with x.
F - describes how X develops according to the model. Generally, the model is x[k] = Fx[k-1]+w[k]. In your case, F might be the identity matrix, if you expect the angular acceleration to be relatively smooth, or the zero matrix, if you expect the angular acceleration to be completely unpredictable. In any case, w would represent how much you expect the acceleration to change from step to step.
w - describes the process noise, meaning, how much does the model diverge from the ""perfect"" model. It is defined as a zero mean multivariate normal distribution with covariance matrix Q.
All the variables above define your model, meaning what you are trying to estimate. In the next part, we talk about the model of the observation - what you measure in order to estimate your model.
z - this is what you measure. In your case, since you are using the accelerometers, you are measuring what you are also estimating. It will be the angular accelerations.
H - describes the relation between your model and the observation. z[k]=H[k]x[k]+v[k]. In your case, it is the identity matrix.
v - is the measurement noise and is assumed to be zero mean Gaussian white noise with covariance R[k]. Here you need to measure how noisy are the accelerometers, and calculate the noise covariance matrix.
To summarize, the steps to use the Kalman filter:

Determine x[0] and P[0] - the initial state of your model, and the initial estimation of how accurately you know x[0].
Determine F based on your model and how it develops from step to step.
Determine Q based on the stochastic nature of your model.
Determine H based on the relation between what you measure and what you want to estimate (between the model and the measurement).
Determine R based on the measurement noise. How noisy is your measurement.

Then, with every new observation, you can update the model state estimation using the Kalman filter, and have an optimal estimation of the state of the model(x[k]), and of the accuracy of that estimation(P[k]).
"
"
I am working in VR field where good calibration of a projected screen is very important, and because of difficult-to-adjust ceiling mounts and other hardware specificities, I am looking for a fullscreen shader method to “correct” the shape of the screen. 
Most of 2D or 3D engines allows to apply a full-screen effect or deformation by redrawing the rendering result on a quad that you can deform or render in a custom way.
The first idea was to use a vertex shader to offset the corners of this screen quad, so the image is deformed as a quadrilateral (like the hardware keystone on a projector), but it won’t be enough for the requirements 
(this approach is described on math.stackexchange with a live fiddle demo).
In my target case:

The image deformation must be non-linear most of the time, so 9 or 16 control points are needed to get a finer adjust.
The borders of the image are not straight (barrel or pillow effect), so even with few control points, the image must be distorted in a curved way in between. Otherwise the deformation would make visible linear seams between at each control points’ limits.
Ideally, knowing the corrected position of each control points of 3x3 or 4x4 grid, the way would be to define a continuous transform for the texture coordinates of the image being drawn on the full screen 
quad:  

u,v => corrected_u, corrected_v
You can find an illustration here. 

I’ve saw some FFD algorithm that works in 2D or 3D that would allow to deform “softly” an image or mesh as if it was made of rubber, but the implementation seems heavy for a real-time shader.
I thought also of a weight-based deformation as we have in squeletal/soft-bodies animation, but seems uncertain to weight properly the control points.
Do you know a method, algorithm or general approach that could help me solve the problem ?
I saw some mesh-based deformation like the new Oculus Rift DK2 requires for its own deformations, but most of the 2D/3D engine use a single quad made of 4 vertices only in standard.

","
If you need non linear deformation Bezier Surfaces are pretty handy and easy to implement.
You can either pre build them in CPU, or use hardware tessellation (example provided here)
"
"
I have set up a window to spread across 2 monitors horizontally. I have 2 viewports. The first is a standard Camera. I have set this to half the width of the window so that it is 1920x1080 (filling the first monitor). The problem I have is that I want to Oculus viewport to take up the 2nd monitor ). However, I cannot find the setting to stop the oculus viewport from taking up the full window.
Can anybody help?
Images for clarification
What it currently looks like:

What I would like it to look like:

I am guessing it is somewhere in the script, but I cannot see where the viewports are drawn.
Any help would be appreciated.
","
In typical form, I have found the answer after a few minutes of posting this.
for anybody who needs this in future, open OVRDistortionCamera.cs and find the lines
float halfWidth = 0.5f * Screen.width;
    GL.Viewport(new Rect(rightEye ? halfWidth : 0f, 0f, halfWidth, Screen.height));

Change them to
float halfWidth = 0.5f * Screen.width/2;
    GL.Viewport(new Rect(rightEye ? halfWidth : 0f, 0f, halfWidth, Screen.height));

"
"
I have a matlab code and wants to convert into appropriate Simulink model.
However i'm new to Simulink, having only Matlab experience. Please Guide me to convert to appropriate model. 
 t=(0:1000)/1000*10*pi; 
    x = (t).*sin(t);
    y = (t).*cos(t);
    z = t;



  plot3(x,y,z,'c') ;
    h=line('Color',[1 0 0],'Marker','.','MarkerSize',40,'EraseMode','xor'); 
    n=length(x);
    i=1;
    while n                          
      set(h,'xdata',x(i),'ydata',y(i),'zdata',z(i)); 
       drawnow;                                             
       pause(0.0005)                                            
       i=i+1;

    end

I followed following steps to design model of refer to signal generation model in Simulink from matlab
t=(0:1000)/1000*10*pi; 
x = (t).*sin(t);
y = (t).*cos(t);
z = t; 

the question is how to I set the step 1 from reference and also, how to give the output to VR SIGNAL EXPANDER in order to move VR Ball in the output generated path. ?
","
You can make your 3 signals in the simulink simply like this:


you can open a scope and go to the settings and then change the number of axis from 1 to 3.

Then add one VR Expander for each signal:

in your case because the translation vector is a 3x1 vector, you need to change the Output width value in the VR Expander to 3 and the input signal indices to [2] because here we care about the second input which is y. It is explained well in this video.
to change the view, there are enough controls and buttons in the VR viewer. you can move right, left, top, down, ....
in the last case you mentioned you need to add a product to the simulink to make a new spiral motion.

"
"
I downloaded the SDK from Oculus Rift website, and I'm trying to run the projects 
on the samples folder. 
When I build the project I'm getting an error that says 
fatal error C1083: Cannot open open include file: 'd3dcompiler.h' : no such file or directory. 

although they add the lib files in the linker. 
anyone else got this errors on their samples projects? 
","
d3dcompiler.h is part of the DirectX development kit. You need to have it installed and either on your project path or your tool's path.
The header will be on the include path but you may need to link against a library, too.
You can add it using:
comment(lib, ""D3dcompiler.lib"")

See this example.
The documentation above says you need D3dcompiler_xx.dll
"
"
I have the Oculus Rift VR and I downloaded the SDK from their website. 
I'm using Visual Studio 2010 Pro, and I did all they mentioned in the WIKI page
Minimal Oculus Application Tutorial.
I added the lib files and all the things they said. But I'm getting 
a lot of errors when I add the line #include ""OVR.h""
it doesn't find all the header files they have in this file. even though I did 
all they mentioned TWICE! 
any help? 
","
OK the problem was that some VS files were missing somehow.. so I installed Visual Studio again.
the missing files were ""math.h"" and etc.. so the problem was from visual studio.. not from the Oculus SDK. 
hope this will help you too..  
"
"
I'm currently working on an X-Plane plugin to add support for the Oculus Rift and I'm having problems with using quaternions for rotation. I've done this kind of thing before with rotation matrices, or Euler angles, but since quaternions seem to be the way to go now, I'm taking the chance to learn them. Quaternions are also what the Oculus SDK seem to use natively so using them is the path of least resistance.
The problem i'm trying to solve is where to position the camera so it is in the location of the pilots head as the plane itself changes its orientation in flight. It seems pretty straightforward to me. I have a vector that represents the relative position of the pilots head to the planes center of gravity, and i want to rotate around the planes z vector by roll degrees.
Here's the code i have right now. I think this is right but running this code yeilds a position that's too high... somewhere above the planes roof and moves around in kind of a circular pattern as the planes orientation changes. Also, if i try rotating the head vector by 0 degrees (by setting phi to 0) I get the same (or similar at least) wierd results, so I must be setting up my quat or something wrong.
    //get the vector to the players head relative to the plane's center of gravity
    float headX = XPLMGetDataf(XPLMFindDataRef(""sim/aircraft/view/acf_peX""));
    float headY = XPLMGetDataf(XPLMFindDataRef(""sim/aircraft/view/acf_peY""));
    float headZ = XPLMGetDataf(XPLMFindDataRef(""sim/aircraft/view/acf_peZ""));

    //the planes orientation in euler angles
    float theta = XPLMGetDataf(XPLMFindDataRef(""sim/flightmodel/position/theta"")) * (M_PI / 180.0f);
    float psi = XPLMGetDataf(XPLMFindDataRef(""sim/flightmodel/position/psi""))  * (M_PI / 180.0f);
    float phi = XPLMGetDataf(XPLMFindDataRef(""sim/flightmodel/position/phi""))  * (M_PI / 180.0f);

    //convert euler angles into a vector for our axis or rotation
    float planeVecX = cos(psi)*cos(theta);
    float planeVecY = sin(psi)*cos(theta);
    float planeVecZ = sin(theta);

    //make some vectors
    Vector3<float> *headVector = new Vector3<float>(headX, headY, headZ);
    headVector->Normalize();
    Vector3<float> *planeVector = new Vector3<float>(planeVecX, planeVecY, planeVecZ);
    planeVector->Normalize();


    //Make a quaternion for our rotation
    Quat<float> *planeQuat = new Quat<float>(*planeVector, phi);

    planeQuat->Normalize();

    //rotate headVector by plane quat
    Vector3<float> rotatedHeadVec = planeQuat->Rotate(*headVector);

    //output our final camera position and orientation
    outCameraPosition->x = planeX + rotatedHeadVec.x;
    outCameraPosition->y = planeY + rotatedHeadVec.y;
    outCameraPosition->z = planeZ + rotatedHeadVec.z;
    outCameraPosition->pitch = theta * (180.0f / M_PI);
    outCameraPosition->heading = psi * (180.0f / M_PI);
    outCameraPosition->roll = phi * (180.0f / M_PI);

TIA for the help.
","
Turns out that x-plane isn't very consistent with it's coordinate system orientations. This code had the right idea, but was using the data it got from X-Plane the wrong way.
"
"
I have the Oculus Rift VR and I downloaded the SDK from their website. 
I'm using Visual Studio 2010 Pro, and I did all they mentioned in the WIKI page
Minimal Oculus Application Tutorial.
I added the lib files and all the things they said. But I'm getting 
a lot of errors when I add the line #include ""OVR.h""
it doesn't find all the header files they have in this file. even though I did 
all they mentioned TWICE! 
any help? 
","
OK the problem was that some VS files were missing somehow.. so I installed Visual Studio again.
the missing files were ""math.h"" and etc.. so the problem was from visual studio.. not from the Oculus SDK. 
hope this will help you too..  
"
"
I'm currently working on an X-Plane plugin to add support for the Oculus Rift and I'm having problems with using quaternions for rotation. I've done this kind of thing before with rotation matrices, or Euler angles, but since quaternions seem to be the way to go now, I'm taking the chance to learn them. Quaternions are also what the Oculus SDK seem to use natively so using them is the path of least resistance.
The problem i'm trying to solve is where to position the camera so it is in the location of the pilots head as the plane itself changes its orientation in flight. It seems pretty straightforward to me. I have a vector that represents the relative position of the pilots head to the planes center of gravity, and i want to rotate around the planes z vector by roll degrees.
Here's the code i have right now. I think this is right but running this code yeilds a position that's too high... somewhere above the planes roof and moves around in kind of a circular pattern as the planes orientation changes. Also, if i try rotating the head vector by 0 degrees (by setting phi to 0) I get the same (or similar at least) wierd results, so I must be setting up my quat or something wrong.
    //get the vector to the players head relative to the plane's center of gravity
    float headX = XPLMGetDataf(XPLMFindDataRef(""sim/aircraft/view/acf_peX""));
    float headY = XPLMGetDataf(XPLMFindDataRef(""sim/aircraft/view/acf_peY""));
    float headZ = XPLMGetDataf(XPLMFindDataRef(""sim/aircraft/view/acf_peZ""));

    //the planes orientation in euler angles
    float theta = XPLMGetDataf(XPLMFindDataRef(""sim/flightmodel/position/theta"")) * (M_PI / 180.0f);
    float psi = XPLMGetDataf(XPLMFindDataRef(""sim/flightmodel/position/psi""))  * (M_PI / 180.0f);
    float phi = XPLMGetDataf(XPLMFindDataRef(""sim/flightmodel/position/phi""))  * (M_PI / 180.0f);

    //convert euler angles into a vector for our axis or rotation
    float planeVecX = cos(psi)*cos(theta);
    float planeVecY = sin(psi)*cos(theta);
    float planeVecZ = sin(theta);

    //make some vectors
    Vector3<float> *headVector = new Vector3<float>(headX, headY, headZ);
    headVector->Normalize();
    Vector3<float> *planeVector = new Vector3<float>(planeVecX, planeVecY, planeVecZ);
    planeVector->Normalize();


    //Make a quaternion for our rotation
    Quat<float> *planeQuat = new Quat<float>(*planeVector, phi);

    planeQuat->Normalize();

    //rotate headVector by plane quat
    Vector3<float> rotatedHeadVec = planeQuat->Rotate(*headVector);

    //output our final camera position and orientation
    outCameraPosition->x = planeX + rotatedHeadVec.x;
    outCameraPosition->y = planeY + rotatedHeadVec.y;
    outCameraPosition->z = planeZ + rotatedHeadVec.z;
    outCameraPosition->pitch = theta * (180.0f / M_PI);
    outCameraPosition->heading = psi * (180.0f / M_PI);
    outCameraPosition->roll = phi * (180.0f / M_PI);

TIA for the help.
","
Turns out that x-plane isn't very consistent with it's coordinate system orientations. This code had the right idea, but was using the data it got from X-Plane the wrong way.
"
"
I was trying to use Apple HTML5 VR in my sample project ...
It looks Fine as this sample code .

threesixty.js

threeSixty = {
    init: function() {
        this._vr = new AC.VR('viewer', 'images/optimized/Seq_v04_640x378_##.jpg', 72, {
            invert: true
        });
    },
    didShow: function() {
        this.init();
    },
    willHide: function() {
        recycleObjectValueForKey(this, ""_vr"");
    },
    shouldCache: function() {
        return false;
    }
}
if (!window.isLoaded) {
    window.addEventListener(""load"", function() {
        threeSixty.init();
    }, false);
}

It load Seq_v04_640x378_01.jpg ~ Seq_v04_640x378_72.jpg
But I was curious , if I want add another picture to display 360
How to change picture name ???
Thanks for any reply or answer : )
Webber
/Edit - Add Projec dictionary screenshot/

And there are 72 pics in ""optimized"" dictionary
","
EDIT You have to edit threesixty.js: inside the new AC.VR( ... ) function call,  it's enough to replace:

images/optimized/Seq_v04_640x378_##.jpg with the name pattern, and
72 with the number of images you have,

as second and third arg, respectively.
Just make sure you placed the files in the right place, that is, a subdirectory of the ""threesixty"" dir in your project.
If,say, your image sequence files are named:

frame0001.jpeg
frame0002.jpeg
...
frame0360.jpeg

Then you'll have to write:

images/optimized/frame0###.jpg as the second arg, and
360 as the third arg.

EDIT Then, add all your files to the threesixty/images/optimized/ dir using XCode menu item ""Project->Add to project"" (CMD+Alt+A).
This way you don't have to rename all your sequence (provided that the files have a name pattern).
"
"
I'm contemplating (if it's possible or) how I would implement the following functionality in the browser:
I'm trying to design a web app that will allow the user to custom design (add images + text) to a 3D surface in the browser.  The design can take place on a (2D) canvas/editor which will allow users can drag and drop images and add text, etc.  Then, preferably in real-time, the design is transformed & applied to the 3D shape/surface.  The user can preview by rotating the 3D surface.
It needs to work in iOS so Flash is out.  Also would it be possible to avoid WebGL for this kind of application - would like the a wide browser coverage (IE8 would need to be supported with the relevant canvas shims).
Is there anything out there that already does this?  Would it be possible to implement this exclusively in the client side (HTML5/Javascript)?
","
Check out http://www.c3dl.org/ for 3d js libraries
http://www.addyosmani.com/resources/googlebox/ for the image and text mapped to box,
And  http://mugtug.com/sketchpad/ for an html5 editor
"
"
Tag this with send-me-the-codez if you will. I've been asked to assist in putting a Quicktime VR thingy (for lack of better term. movie? applet?) in an HTML page. How would I go about this? Pointers to examples online are most appreciated.
Note: I checked out Apple's iPhone 360-degree views, which (I think) are the only examples of QuickTime VR that I've seen on the internets. The Apple specimens are laden with JavaScript and seem like terrible overkill.
","
As far as I'm aware embedding QTVR is just the same as embedding a QT video, eg:
<object classid=""clsid:02BF25D5-8C17-4B23-BC80-D3488ABDDC6B"" width=""..."" height=""..."">
    <param name=""src"" value=""....mov"">
    <param name=""autoplay"" value=""true"">
    <param name=""controller"" value=""true"">

    <embed src=""....mov"" width=""..."" height=""..."" autoplay=""true"" controller=""true""></embed>
</object> 

"
